{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Data Mining on Sentiment Analysis"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Preliminaries"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Import libraries. **You can add other libraries if necessary.**"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-10-16T23:37:03.83871Z","iopub.status.busy":"2022-10-16T23:37:03.837369Z","iopub.status.idle":"2022-10-16T23:37:04.798601Z","shell.execute_reply":"2022-10-16T23:37:04.79744Z","shell.execute_reply.started":"2022-10-16T23:37:03.838618Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\29230\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\29230\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from collections import Counter\n","from collections import defaultdict\n","import nltk\n","from nltk import word_tokenize\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import re  \n","import seaborn as sns\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  \n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score  \n","from sklearn.model_selection import train_test_split  \n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, Dataset, DataLoader\n","from tqdm import tqdm\n","from transformers import BertTokenizer, BertModel\n","import torch.nn.functional as F\n","\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Load the data and add column keys."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:37:04.804857Z","iopub.status.busy":"2022-10-16T23:37:04.804516Z","iopub.status.idle":"2022-10-16T23:37:05.043216Z","shell.execute_reply":"2022-10-16T23:37:05.042013Z","shell.execute_reply.started":"2022-10-16T23:37:04.804825Z"},"trusted":true},"outputs":[],"source":["train_data = pd.read_csv(\"data/sentiment_train.csv\", header=None)\n","test_data = pd.read_csv(\"data/sentiment_test.csv\", header=None)\n","train_data.columns = ['id', 'information', 'type', 'text']\n","test_data.columns = ['id', 'information', 'type', 'text']"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Take a glance at the provided data."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:37:05.044993Z","iopub.status.busy":"2022-10-16T23:37:05.044632Z","iopub.status.idle":"2022-10-16T23:37:05.062473Z","shell.execute_reply":"2022-10-16T23:37:05.06133Z","shell.execute_reply.started":"2022-10-16T23:37:05.044962Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 74682 entries, 0 to 74681\n","Data columns (total 4 columns):\n"," #   Column       Non-Null Count  Dtype \n","---  ------       --------------  ----- \n"," 0   id           74682 non-null  int64 \n"," 1   information  74682 non-null  object\n"," 2   type         74682 non-null  object\n"," 3   text         73996 non-null  object\n","dtypes: int64(1), object(3)\n","memory usage: 2.3+ MB\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>information</th>\n","      <th>type</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im getting on borderlands and i will murder yo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>I am coming to the borders and I will kill you...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im getting on borderlands and i will kill you ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im coming on borderlands and i will murder you...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im getting on borderlands 2 and i will murder ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     id  information      type  \\\n","0  2401  Borderlands  Positive   \n","1  2401  Borderlands  Positive   \n","2  2401  Borderlands  Positive   \n","3  2401  Borderlands  Positive   \n","4  2401  Borderlands  Positive   \n","\n","                                                text  \n","0  im getting on borderlands and i will murder yo...  \n","1  I am coming to the borders and I will kill you...  \n","2  im getting on borderlands and i will kill you ...  \n","3  im coming on borderlands and i will murder you...  \n","4  im getting on borderlands 2 and i will murder ...  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["train_data.info()\n","train_data.head(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Each item consists of 4 columns, where Columns ID and Information are almost task-irrevalent. **Column Type is users' sentiments, which we should predict as our labels based on Column Text.**\n","Now let's take a look at possible values of Column Type."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["Negative      22542\n","Positive      20832\n","Neutral       18318\n","Irrelevant    12990\n","Name: type, dtype: int64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_data[\"type\"].value_counts()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["Neutral       285\n","Positive      277\n","Negative      266\n","Irrelevant    172\n","Name: type, dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["test_data[\"type\"].value_counts()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There are 4 possible values, and ***our goal is to perform the quadruple classification over texts.***"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Data Processing"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["To prepare the data for the text analysis, we should:\n","- Filter texts with only letters and numbers\n","- Turn the texts into lower case\n","> **TODO**\n","\n","Complete the `preprocess` function, which takes a piece of texts as input (not necessary of type `str`), you should convert it to `str`, and outputs the preprocessed string.  \n","Some functions that might be helpful:\n","- `re.sub`\n","- `str.lower`"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def preprocess(text):\n","    # Convert input to str if it's not already\n","    text = str(text)\n","    # Remove any non-alphanumeric characters using regex\n","    text = re.sub(r'\\W+', ' ', text)\n","    # Convert text to lower case\n","    text = text.lower()\n","    return text"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Perform preprocessing, and compare the raw text and the preprocessed tokens."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:37:05.121422Z","iopub.status.busy":"2022-10-16T23:37:05.121009Z","iopub.status.idle":"2022-10-16T23:37:05.59644Z","shell.execute_reply":"2022-10-16T23:37:05.595292Z","shell.execute_reply.started":"2022-10-16T23:37:05.121385Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>information</th>\n","      <th>type</th>\n","      <th>text</th>\n","      <th>clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im getting on borderlands and i will murder yo...</td>\n","      <td>im getting on borderlands and i will murder yo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>I am coming to the borders and I will kill you...</td>\n","      <td>i am coming to the borders and i will kill you...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im getting on borderlands and i will kill you ...</td>\n","      <td>im getting on borderlands and i will kill you ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im coming on borderlands and i will murder you...</td>\n","      <td>im coming on borderlands and i will murder you...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2401</td>\n","      <td>Borderlands</td>\n","      <td>Positive</td>\n","      <td>im getting on borderlands 2 and i will murder ...</td>\n","      <td>im getting on borderlands 2 and i will murder ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     id  information      type  \\\n","0  2401  Borderlands  Positive   \n","1  2401  Borderlands  Positive   \n","2  2401  Borderlands  Positive   \n","3  2401  Borderlands  Positive   \n","4  2401  Borderlands  Positive   \n","\n","                                                text  \\\n","0  im getting on borderlands and i will murder yo...   \n","1  I am coming to the borders and I will kill you...   \n","2  im getting on borderlands and i will kill you ...   \n","3  im coming on borderlands and i will murder you...   \n","4  im getting on borderlands 2 and i will murder ...   \n","\n","                                               clean  \n","0  im getting on borderlands and i will murder yo...  \n","1  i am coming to the borders and i will kill you...  \n","2  im getting on borderlands and i will kill you ...  \n","3  im coming on borderlands and i will murder you...  \n","4  im getting on borderlands 2 and i will murder ...  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train_data[\"clean\"] = train_data.text.apply(preprocess)\n","test_data[\"clean\"] = test_data.text.apply(preprocess)\n","train_data.head(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Feature Engineering\n","\n","The feature engineering with clean texts starts from text tokenization, i.e., split the text into word tokens. Let's see what the tokenization do. It groups all the texts by words stored on a list."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:38:28.152525Z","iopub.status.busy":"2022-10-16T23:38:28.152145Z","iopub.status.idle":"2022-10-16T23:38:38.671652Z","shell.execute_reply":"2022-10-16T23:38:38.670297Z","shell.execute_reply.started":"2022-10-16T23:38:28.15249Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["So I spent a few hours making something for fun. . . If you don't know I am a HUGE @Borderlands fan and Maya is one of my favorite characters. So I decided to make myself a wallpaper for my PC. . Here is the original image versus the creation I made :) Enjoy! pic.twitter.com/mLsI5wf9Jg\n","so i spent a few hours making something for fun if you don t know i am a huge borderlands fan and maya is one of my favorite characters so i decided to make myself a wallpaper for my pc here is the original image versus the creation i made enjoy pic twitter com mlsi5wf9jg\n","['so', 'i', 'spent', 'a', 'few', 'hours', 'making', 'something', 'for', 'fun', 'if', 'you', 'don', 't', 'know', 'i', 'am', 'a', 'huge', 'borderlands', 'fan', 'and', 'maya', 'is', 'one', 'of', 'my', 'favorite', 'characters', 'so', 'i', 'decided', 'to', 'make', 'myself', 'a', 'wallpaper', 'for', 'my', 'pc', 'here', 'is', 'the', 'original', 'image', 'versus', 'the', 'creation', 'i', 'made', 'enjoy', 'pic', 'twitter', 'com', 'mlsi5wf9jg']\n"]}],"source":["print(train_data.text[6])\n","print(train_data.clean[6])\n","print(word_tokenize(train_data.clean[6]))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We can count the total number of tokens in the training data."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:38:38.673317Z","iopub.status.busy":"2022-10-16T23:38:38.672972Z","iopub.status.idle":"2022-10-16T23:38:38.679953Z","shell.execute_reply":"2022-10-16T23:38:38.67885Z","shell.execute_reply.started":"2022-10-16T23:38:38.673286Z"},"trusted":true},"outputs":[{"data":{"text/plain":["31111"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["len(set(token for text in train_data.clean for token in word_tokenize(text)))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Stop words are the words in a stop list which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are insignificant. We can refer to `nltk.corpus.stopwords` to obtain the stop words to use."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:38:38.681569Z","iopub.status.busy":"2022-10-16T23:38:38.681194Z","iopub.status.idle":"2022-10-16T23:38:38.694465Z","shell.execute_reply":"2022-10-16T23:38:38.69331Z","shell.execute_reply.started":"2022-10-16T23:38:38.681537Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["179 ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"]}],"source":["stopwords_nltk = nltk.corpus.stopwords\n","stop_words = stopwords_nltk.words('english')\n","print(len(stop_words), stop_words[:10])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["With text tokenization tools, we can conduct feature engineering on texts with stop words filtered out.\n","\n","For simplification, we mainly consider two possible features:\n","- **Word count**: a vector with the dimension of the number of tokens, the value in each dimension is the number of occurrences of the corresponding word token.\n","- **TF-IDF (Term Frequency-Inverse Document Frequency)**: a \"weighted version\" of word count, each value is the term frequency (normalized by total number of tokens) multiplies the inverse of the frequency of documents consisting this word token. For details please refers to https://en.wikipedia.org/wiki/Tf%E2%80%93idf.\n","\n","> **TODO**\n","Sklearn provides the implementation of the two feature extraction methods, and the interface is in the following. **You should manually re-implement at least one of them in our provided framework.**\n","\n","*Hint: if you are concerned about the too large dimension, which might slow the model inference, there are three possible solutions which you can have a try:*\n","- *Hashing: hash the vector into low dimensions with a function from high-dimension index to lower one. We provide a simple hash function and you can also implement a complex one.*\n","- *Random projection: project the vector into low-dimension space with a fixed random projection. We provide a simple projection method.*\n","- *Sparsity: use sparse matrices instead of dense ones, which is used in sklearn's implementation. Please refer to https://docs.scipy.org/doc/scipy/reference/sparse.html for more details.*"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Here, I manually implement the **Word Count** feature extractor as the following codes show:"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class ManualVectorizer(object):\n","    \"\"\"\n","    Manual vectorizer.\n","    \"\"\"\n","    def __init__(self, tokenizer, stop_words):\n","        \"\"\"\n","        Initialize the vectorizer.\n","        \"\"\"\n","        self.tokenizer = tokenizer\n","        self.stop_words = stop_words\n","        self.vocabulary = defaultdict(int)\n","\n","    def fit_transform(self, texts):\n","        \"\"\"\n","        Fit the dictionary and other attributes (such as IDF) with the texts, and then perform feature extraction.\n","        This method is used on training data.\n","\n","        Parameters:\n","            raw_documents (List[str]): a list of untokenized texts.\n","\n","        Return:\n","            np.array: a 2-D array where each row refers to the feature vector of the corresponding text.\n","        \"\"\"\n","        tokenized_texts = [self.tokenizer(text) for text in texts]\n","        filtered_texts = [[token for token in text if token not in self.stop_words] for text in tokenized_texts]\n","\n","        self.build_vocabulary(filtered_texts)\n","\n","        feature_vectors = np.zeros((len(texts), len(self.vocabulary)))\n","        for i, text in enumerate(filtered_texts):\n","            for token in text:\n","                if token in self.vocabulary:\n","                    feature_vectors[i, self.vocabulary[token]] += 1\n","\n","        return feature_vectors\n","\n","    def transform(self, texts):\n","        \"\"\"\n","        Perform feature extraction with the learned dictionary and other attributes.\n","        This method is used on test data.\n","        Note: if a word token does not appear in training data, it will not be counted as the test feature.\n","\n","        Parameters:\n","            raw_documents (List[str]): a list of untokenized texts.\n","\n","        Return:\n","            np.array or np.matrix: a 2-D array where each row refers to the feature vector of the corresponding text.\n","        \"\"\"\n","        tokenized_texts = [self.tokenizer(text) for text in texts]\n","        filtered_texts = [[token for token in text if token not in self.stop_words] for text in tokenized_texts]\n","\n","        feature_vectors = np.zeros((len(texts), len(self.vocabulary)))\n","        for i, text in enumerate(filtered_texts):\n","            for token in text:\n","                if token in self.vocabulary:\n","                    feature_vectors[i, self.vocabulary[token]] += 1\n","\n","        return feature_vectors\n","\n","    def build_vocabulary(self, filtered_texts):\n","        for text in filtered_texts:\n","            for token in text:\n","                if token not in self.vocabulary:\n","                    self.vocabulary[token] = len(self.vocabulary)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["wordcount_extractor = CountVectorizer(\n","    tokenizer=word_tokenize,\n","    stop_words=stop_words,\n",")\n","\n","tfidf_extractor = TfidfVectorizer(\n","    tokenizer=word_tokenize,\n","    stop_words=stop_words,\n",")\n","\n","manual_extractor = ManualVectorizer(\n","    tokenizer=word_tokenize,\n","    stop_words=stop_words,\n",")\n","\n","extractor = tfidf_extractor # TODO: you can modify here"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["class Identity(object):\n","    \"\"\"\n","    Do nothing.\n","    \"\"\"\n","    def __init__(self):\n","        pass\n","\n","    def __call__(self, xs):\n","        return xs\n","\n","\n","class VectorHasher(object):\n","    \"\"\"\n","    Vector hasher for dimension reduction.\n","    \"\"\"\n","    def __init__(self, target_length=100, hash_func=None):\n","        self.target_length = target_length\n","        if hash_func is None:\n","            self.hash_func = lambda x: x % self.target_length\n","        else:\n","            self.hash_func = hash_func\n","    \n","    def __call__(self, xs):\n","        hashed_xs = np.matrix(xs.shape[:-1] + (self.target_length, ))\n","        for idx in range(xs.shape[-1]):\n","            hashed_idx = self.hash_func(idx)\n","            hashed_xs[:, hashed_idx] += xs[:, idx].flatten()\n","        return hashed_xs\n","\n","\n","class VectorProjector(object):\n","    \"\"\"\n","    Vector projector for dimension reduction.\n","    \"\"\"\n","    def __init__(self, source_length, target_length=100):\n","        self.projector = np.random.normal(size=(source_length, target_length))\n","    \n","    def __call__(self, xs):\n","        return xs @ self.projector\n","    \n","\n","class VectorSVD(object):\n","    def __init__(self, n_components=100, random_state=42):\n","        self.svd_transformer = TruncatedSVD(n_components=n_components, random_state=random_state)\n","        self.is_fit = False\n","\n","    def fit(self, X):\n","        self.svd_transformer.fit(X)\n","        self.is_fit = True\n","\n","    def transform(self, X):\n","        if not self.is_fit:\n","            raise RuntimeError(\"You must fit the transformer before transforming data!\")\n","        return self.svd_transformer.transform(X)\n","    \n","    def __call__(self, X):\n","        return self.transform(X)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In actual training, I found that dimension reduction recommended methods above did not work well. Therefore I use Truncated SVD. \\\n","Truncated SVD works well with sparse data and is often used in NLP for dimensionality reduction. It operates directly on the data matrix, rather than on the covariance matrix like PCA, making it more suitable for sparse data."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\29230\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n","  warnings.warn(\n"]}],"source":["X_train_highdim = extractor.fit_transform(train_data.clean)\n","X_test_highdim = extractor.transform(test_data.clean)\n","\n","identity = Identity()\n","hasher = VectorHasher()\n","projector = VectorProjector(X_train_highdim.shape[1])\n","svd_transformer = VectorSVD()\n","svd_transformer.fit(X_train_highdim)\n","\n","vector_post_process = svd_transformer # TODO: you can modify here"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(74682, 100) (1000, 100)\n"]}],"source":["X_train_raw = vector_post_process(X_train_highdim)\n","X_test = vector_post_process(X_test_highdim)\n","print(X_train_raw.shape, X_test.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For label features, it is natual to assign each label name with an index."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0,\n","        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2,\n","        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0,\n","        0, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64),\n"," (74682,),\n"," (1000,))"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["name_to_index = {\n","    \"Positive\": 0,\n","    \"Negative\": 1,\n","    \"Neutral\": 2,\n","    \"Irrelevant\": 3,\n","}\n","y_train_raw = np.asarray(train_data.type.apply(lambda x: name_to_index[x]))\n","y_test = np.asarray(test_data.type.apply(lambda x: name_to_index[x]))\n","y_train_raw[:120], y_train_raw.shape, y_test.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Model Selection"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In order to train a model and perform model selection, we should split the raw training data into *training data* and *validation data*\n","> **TODO**\n","\n","**Split the data into training data and validation data with proper ratio. You can use `train_test_split` function.**"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-10-16T23:38:38.7041Z","iopub.status.busy":"2022-10-16T23:38:38.703764Z","iopub.status.idle":"2022-10-16T23:38:38.732995Z","shell.execute_reply":"2022-10-16T23:38:38.731632Z","shell.execute_reply.started":"2022-10-16T23:38:38.70407Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(59745, 100) (14937, 100) (59745,) (14937,)\n"]}],"source":["# Split the training data into training and validation sets using feature vectors and labels.\n","X_train, X_val, y_train, y_val = train_test_split(X_train_raw, y_train_raw, test_size=0.2, random_state=42)\n","print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Split the training data into training and validation w/o vectorization for Bert\n","X_train2, X_val2, y_train2, y_val2 = train_test_split(train_data.clean, y_train_raw, test_size=0.2, random_state=42)\n","X_train2 = X_train2.reset_index(drop=True)\n","X_val2 = X_val2.reset_index(drop=True)\n","X_test2 = test_data.clean"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now it's time to train your models and select the best ones.\n","\n","> **TODO**\n","\n","**Train your model on `X_train` and `y_train`, and select your model on `X_val` and `y_val`**\n","\n","We provide an example of `DecisionTreeClassifier`. Now it's time for you to select the model you like to conduct classification.\n","\n","> Requirements\n","- Select at least **three** other Machine Learning classification models and train them on the train split. And among them you should implement at least **one** by yourself with our provided interface."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### MLP"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["class ManualModel(object):\n","    \"\"\"\n","    Manual model with sklearn-style interface, MLP\n","    \"\"\"\n","    def __init__(self, input_dim, hidden_dim=128, output_dim=4, dropout_rate=0, lr=1e-3, weight_decay=1e-5):\n","        \"\"\"\n","        Initialize the model with some hyperparameters.\n","\n","        Parameters:\n","            input_dim: the dimension of input features.\n","            hidden_dim: the dimension of hidden layer.\n","            output_dim: the number of output classes.\n","            dropout_rate: the rate of dropout.\n","            lr: learning rate.\n","            weight_decay: the rate of weight decay.\n","        \"\"\"\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hidden_dim, output_dim),\n","        ).to(self.device)\n","        self.loss_fn = nn.CrossEntropyLoss()\n","        self.lr = lr\n","        self.weight_decay = weight_decay\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n","        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=200, gamma=0.1)\n","\n","    def fit(self, X, y, batch_size=64, num_epochs=400):\n","        \"\"\"\n","        Fit the model on training set.\n","\n","        Parameters:\n","            X: inputs of training data.\n","            y: labels of training data.\n","            num_epochs: the number of epochs to train.\n","        \"\"\"\n","        self.model.train()\n","        X_tensor = torch.tensor(X, dtype=torch.float).to(self.device)\n","        y_tensor = torch.tensor(y, dtype=torch.long).to(self.device)\n","        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n","        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","        # for epoch in tqdm(range(num_epochs), desc=\"Fitting\"):\n","        for epoch in range(num_epochs):\n","            train_loss = 0.0\n","            train_acc = 0.0\n","            for batch_X, batch_y in dataloader:\n","                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n","                # Forward pass\n","                logits = self.model(batch_X)\n","\n","                # Calculate loss\n","                loss = self.loss_fn(logits, batch_y)\n","                train_loss += loss.item()\n","                acc = torch.sum(torch.argmax(logits, dim=1) == batch_y).item() / len(batch_y)\n","                train_acc += acc\n","\n","                # Zero the gradients\n","                self.optimizer.zero_grad()\n","                # Backward pass\n","                loss.backward()\n","                # Update the parameters\n","                self.optimizer.step()\n","                \n","            self.scheduler.step()\n","            # print(\"Epoch: {}, Train Loss: {}, Train Accuracy: {}\".format(epoch+1, train_loss/len(dataloader), train_acc/len(dataloader)))\n","            \n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predict the labels of inputs X with the trained model.\n","\n","        Parameters:\n","            X: inputs of test data\n","        \n","        Return:\n","            The predicted labels of test data.\n","        \"\"\"\n","        self.model.eval()\n","        with torch.no_grad():\n","            X = torch.tensor(X, dtype=torch.float32).to(self.device)\n","            outputs = self.model(X)\n","            _, prds = torch.max(outputs, dim=1)\n","        return prds.cpu().numpy()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Use Bert for classification\n","In this model, I extract the embedded representation from the pre-trained BERT, pool all embeddings to get an effective representation of the sentence, and feed into a fully connected layer to implement text classification."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["class BertClassifier(nn.Module):\n","    def __init__(self, num_classes):\n","        super(BertClassifier, self).__init__()\n","        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n","        self.dropout = nn.Dropout(0.1)\n","        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes) # 768 -> 4\n","\n","    def forward(self, input_ids, attention_mask):\n","        output = self.bert(input_ids=input_ids, attention_mask=attention_mask) # word embedding\n","        pooled_output = output.pooler_output\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.fc(pooled_output)\n","        return logits # we don't need softmax here because we'll use cross-entropy as activation.\n","    \n","class TextDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","        return {'input_ids': encoding['input_ids'].squeeze(),\n","                'attention_mask': encoding['attention_mask'].squeeze(),\n","                'label': torch.tensor(label, dtype=torch.long)}\n","\n","def train_bert_classifier(model, batches, loss_fn, optimizer, device, n_examples):\n","    model.train()\n","    correct_predictions = 0\n","    total_loss = 0\n","\n","    for batch in tqdm(batches, desc=\"Training\"):\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"label\"].to(device)\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","        _, preds = torch.max(outputs, dim=1)\n","        loss = loss_fn(outputs, labels)\n","\n","        correct_predictions += torch.sum(preds == labels)\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","    return total_loss / n_examples, correct_predictions.double() / n_examples\n","\n","def eval_bert_classifier(model, batches, loss_fn, device, n_examples):\n","    model.eval()\n","    correct_predictions = 0\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(batches, desc=\"Evaluating\"):\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"label\"].to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","            _, preds = torch.max(outputs, dim=1)\n","            loss = loss_fn(outputs, labels)\n","\n","            correct_predictions += torch.sum(preds == labels)\n","            total_loss += loss.item()\n","\n","    return total_loss / n_examples, correct_predictions.double() / n_examples"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### TextResNet"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["class ResidualBlock(nn.Module):\n","    def __init__(self, num_channels, kernel_size=9):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv1d(num_channels, num_channels, kernel_size, padding = kernel_size // 2)\n","        self.bn1 = nn.BatchNorm1d(num_channels)\n","        self.dropout = nn.Dropout(0.5)\n","        self.conv2 = nn.Conv1d(num_channels, num_channels, kernel_size, padding = kernel_size // 2)\n","        self.bn2 = nn.BatchNorm1d(num_channels)\n","    \n","    def forward(self, x):\n","        residual = x\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.dropout(out)\n","        out = self.bn2(self.conv2(out))\n","        out += residual\n","        out = F.relu(out)\n","        return out\n","\n","class AttentivePooling(nn.Module):\n","    def __init__(self, num_channels):\n","        super(AttentivePooling, self).__init__()\n","        self.attention = nn.Linear(num_channels, 1) # Linear layer to compute attention scores\n","\n","    def forward(self, x):\n","        # Compute attention scores\n","        scores = self.attention(x.transpose(1, 2))  # Shape: (batch_size, seq_len, channels)\n","        scores = F.softmax(scores, dim=1)  # Apply softmax over the sequence dimension, shape: (batch_size, seq_len, 1)\n","\n","        # Compute weighted sum of features\n","        x = x * scores.transpose(1, 2)  # Shape: (batch_size, num_channels, seq_len)\n","        x = x.sum(dim=2)  # Shape: (batch_size, num_channels)\n","\n","        return x\n","    \n","class TextResNetClassifier(nn.Module):\n","    def __init__(self, input_dim, num_classes, kernel_size=9, num_channels=256):\n","        super(TextResNetClassifier, self).__init__()\n","        self.conv1 = nn.Conv1d(input_dim, num_channels, kernel_size, padding = kernel_size // 2)\n","        self.bn1 = nn.BatchNorm1d(num_channels)\n","        self.resblocks = nn.ModuleList([ResidualBlock(num_channels) for _ in range(8)])\n","        self.pooling = AttentivePooling(num_channels)\n","        self.fc = nn.Linear(num_channels, num_classes)\n","        \n","    def forward(self, x):\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        for resblock in self.resblocks:\n","            x = resblock(x)\n","        x = self.pooling(x)  # Attentive pooling\n","        out = self.fc(x)\n","        return out\n","\n","\n","def train_res_classifier(model, iterator, optimizer, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.train()\n","\n","    for batch in iterator:\n","        inputs, labels = batch\n","        optimizer.zero_grad()\n","        predictions = model(inputs)\n","        loss = criterion(predictions, labels)\n","        acc = (predictions.argmax(1) == labels).float().mean()\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","\n","def evaluate_res_classifier(model, iterator, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for batch in iterator:\n","            inputs, labels = batch\n","            predictions = model(inputs)\n","            loss = criterion(predictions, labels)\n","            acc = (predictions.argmax(1) == labels).float().mean()\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Function to view log "]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def view_line(file_path):\n","    with open(file_path, 'r') as file:\n","        lines = file.readlines()\n","    for line in lines:\n","        print(line, end='')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Training Bert:** \\\n","The training requires high GPU performance and long time. \\\n","**Notice:**  You can **skip** the step and directly view the training records in the log"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# **Notice:** You can skip this cell if you want to save time\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Set up tokenizer, use to convert text to tokens\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  # lowercase text\n","\n","# Hyperparameters\n","max_length = 64 # max length of tokens\n","batch_size = 64 \n","num_classes = 4 \n","num_epochs = 100\n","learning_rate = 2e-5\n","\n","# Create model, optimizer, and loss function\n","model = BertClassifier(num_classes).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Create Dataset\n","train_dataset = TextDataset(X_train2, y_train2, tokenizer, max_length)\n","val_dataset = TextDataset(X_val2, y_val2, tokenizer, max_length)\n","\n","# Create DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  # usually we don't shuffle validation and test sets\n","\n","# Train and evaluate the model\n","for epoch in range(num_epochs):\n","    train_loss, train_acc = train_bert_classifier(model, train_loader, loss_fn, optimizer, device, len(train_dataset))\n","    val_loss, val_acc = eval_bert_classifier(model, val_loader, loss_fn, device, len(val_dataset))\n","    print(f\"Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","    with open (\"bert_2.log\", \"a\") as f:\n","        f.write(f\"Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\\n\")"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1, Train Loss: 0.0140, Train Acc: 0.6379, Val Loss: 0.0101, Val Acc: 0.7567\n","Epoch: 2, Train Loss: 0.0071, Train Acc: 0.8330, Val Loss: 0.0059, Val Acc: 0.8672\n","Epoch: 3, Train Loss: 0.0034, Train Acc: 0.9195, Val Loss: 0.0050, Val Acc: 0.8906\n","Epoch: 4, Train Loss: 0.0022, Train Acc: 0.9451, Val Loss: 0.0046, Val Acc: 0.9063\n","Epoch: 5, Train Loss: 0.0018, Train Acc: 0.9546, Val Loss: 0.0046, Val Acc: 0.9054\n","Epoch: 6, Train Loss: 0.0015, Train Acc: 0.9591, Val Loss: 0.0047, Val Acc: 0.9096\n","Epoch: 7, Train Loss: 0.0014, Train Acc: 0.9609, Val Loss: 0.0047, Val Acc: 0.9134\n","Epoch: 8, Train Loss: 0.0013, Train Acc: 0.9617, Val Loss: 0.0047, Val Acc: 0.9162\n","Epoch: 9, Train Loss: 0.0013, Train Acc: 0.9648, Val Loss: 0.0047, Val Acc: 0.9173\n","Epoch: 10, Train Loss: 0.0013, Train Acc: 0.9642, Val Loss: 0.0045, Val Acc: 0.9164\n","Epoch: 11, Train Loss: 0.0012, Train Acc: 0.9667, Val Loss: 0.0046, Val Acc: 0.9186\n","Epoch: 12, Train Loss: 0.0012, Train Acc: 0.9661, Val Loss: 0.0049, Val Acc: 0.9185\n","Epoch: 13, Train Loss: 0.0011, Train Acc: 0.9673, Val Loss: 0.0051, Val Acc: 0.9162\n","Epoch: 14, Train Loss: 0.0011, Train Acc: 0.9669, Val Loss: 0.0047, Val Acc: 0.9229\n","Epoch: 15, Train Loss: 0.0011, Train Acc: 0.9692, Val Loss: 0.0046, Val Acc: 0.9207\n","Epoch: 16, Train Loss: 0.0011, Train Acc: 0.9683, Val Loss: 0.0046, Val Acc: 0.9229\n","Epoch: 17, Train Loss: 0.0011, Train Acc: 0.9692, Val Loss: 0.0046, Val Acc: 0.9205\n","Epoch: 18, Train Loss: 0.0011, Train Acc: 0.9690, Val Loss: 0.0044, Val Acc: 0.9227\n","Epoch: 19, Train Loss: 0.0010, Train Acc: 0.9701, Val Loss: 0.0048, Val Acc: 0.9205\n","Epoch: 20, Train Loss: 0.0010, Train Acc: 0.9691, Val Loss: 0.0047, Val Acc: 0.9229\n","Epoch: 21, Train Loss: 0.0010, Train Acc: 0.9704, Val Loss: 0.0048, Val Acc: 0.9225\n","Epoch: 22, Train Loss: 0.0010, Train Acc: 0.9695, Val Loss: 0.0045, Val Acc: 0.9252\n","Epoch: 23, Train Loss: 0.0010, Train Acc: 0.9710, Val Loss: 0.0045, Val Acc: 0.9252\n","Epoch: 24, Train Loss: 0.0010, Train Acc: 0.9702, Val Loss: 0.0046, Val Acc: 0.9237\n","Epoch: 25, Train Loss: 0.0010, Train Acc: 0.9707, Val Loss: 0.0046, Val Acc: 0.9239\n","Epoch: 26, Train Loss: 0.0010, Train Acc: 0.9702, Val Loss: 0.0046, Val Acc: 0.9259\n","Epoch: 27, Train Loss: 0.0009, Train Acc: 0.9711, Val Loss: 0.0046, Val Acc: 0.9276\n","Epoch: 28, Train Loss: 0.0010, Train Acc: 0.9705, Val Loss: 0.0045, Val Acc: 0.9258\n","Epoch: 29, Train Loss: 0.0010, Train Acc: 0.9712, Val Loss: 0.0045, Val Acc: 0.9274\n","Epoch: 30, Train Loss: 0.0010, Train Acc: 0.9703, Val Loss: 0.0051, Val Acc: 0.9227\n","Epoch: 31, Train Loss: 0.0010, Train Acc: 0.9712, Val Loss: 0.0051, Val Acc: 0.9226\n","Epoch: 32, Train Loss: 0.0009, Train Acc: 0.9714, Val Loss: 0.0045, Val Acc: 0.9275\n","Epoch: 33, Train Loss: 0.0009, Train Acc: 0.9715, Val Loss: 0.0046, Val Acc: 0.9274\n","Epoch: 34, Train Loss: 0.0010, Train Acc: 0.9715, Val Loss: 0.0044, Val Acc: 0.9287\n","Epoch: 35, Train Loss: 0.0009, Train Acc: 0.9713, Val Loss: 0.0045, Val Acc: 0.9288\n","Epoch: 36, Train Loss: 0.0009, Train Acc: 0.9716, Val Loss: 0.0047, Val Acc: 0.9286\n","Epoch: 37, Train Loss: 0.0009, Train Acc: 0.9717, Val Loss: 0.0046, Val Acc: 0.9291\n","Epoch: 38, Train Loss: 0.0009, Train Acc: 0.9726, Val Loss: 0.0047, Val Acc: 0.9288\n","Epoch: 39, Train Loss: 0.0009, Train Acc: 0.9718, Val Loss: 0.0048, Val Acc: 0.9262\n","Epoch: 40, Train Loss: 0.0009, Train Acc: 0.9716, Val Loss: 0.0047, Val Acc: 0.9258\n","Epoch: 41, Train Loss: 0.0009, Train Acc: 0.9715, Val Loss: 0.0047, Val Acc: 0.9292\n","Epoch: 42, Train Loss: 0.0009, Train Acc: 0.9723, Val Loss: 0.0043, Val Acc: 0.9298\n","Epoch: 43, Train Loss: 0.0009, Train Acc: 0.9724, Val Loss: 0.0046, Val Acc: 0.9278\n","Epoch: 44, Train Loss: 0.0009, Train Acc: 0.9716, Val Loss: 0.0047, Val Acc: 0.9288\n","Epoch: 45, Train Loss: 0.0009, Train Acc: 0.9715, Val Loss: 0.0044, Val Acc: 0.9307\n","Epoch: 46, Train Loss: 0.0009, Train Acc: 0.9721, Val Loss: 0.0046, Val Acc: 0.9277\n","Epoch: 47, Train Loss: 0.0009, Train Acc: 0.9721, Val Loss: 0.0044, Val Acc: 0.9312\n","Epoch: 48, Train Loss: 0.0009, Train Acc: 0.9725, Val Loss: 0.0047, Val Acc: 0.9284\n","Epoch: 49, Train Loss: 0.0009, Train Acc: 0.9726, Val Loss: 0.0052, Val Acc: 0.9273\n","Epoch: 50, Train Loss: 0.0009, Train Acc: 0.9723, Val Loss: 0.0044, Val Acc: 0.9302\n","Epoch: 51, Train Loss: 0.0009, Train Acc: 0.9726, Val Loss: 0.0047, Val Acc: 0.9298\n","Epoch: 52, Train Loss: 0.0009, Train Acc: 0.9727, Val Loss: 0.0044, Val Acc: 0.9320\n","Epoch: 53, Train Loss: 0.0009, Train Acc: 0.9732, Val Loss: 0.0047, Val Acc: 0.9277\n","Epoch: 54, Train Loss: 0.0009, Train Acc: 0.9720, Val Loss: 0.0046, Val Acc: 0.9304\n","Epoch: 55, Train Loss: 0.0009, Train Acc: 0.9725, Val Loss: 0.0048, Val Acc: 0.9294\n","Epoch: 56, Train Loss: 0.0009, Train Acc: 0.9720, Val Loss: 0.0040, Val Acc: 0.9322\n","Epoch: 57, Train Loss: 0.0009, Train Acc: 0.9728, Val Loss: 0.0047, Val Acc: 0.9318\n","Epoch: 58, Train Loss: 0.0009, Train Acc: 0.9721, Val Loss: 0.0046, Val Acc: 0.9310\n","Epoch: 59, Train Loss: 0.0009, Train Acc: 0.9730, Val Loss: 0.0048, Val Acc: 0.9317\n","Epoch: 60, Train Loss: 0.0009, Train Acc: 0.9724, Val Loss: 0.0045, Val Acc: 0.9332\n","Epoch: 61, Train Loss: 0.0009, Train Acc: 0.9721, Val Loss: 0.0046, Val Acc: 0.9304\n","Epoch: 62, Train Loss: 0.0009, Train Acc: 0.9726, Val Loss: 0.0047, Val Acc: 0.9301\n","Epoch: 63, Train Loss: 0.0009, Train Acc: 0.9734, Val Loss: 0.0047, Val Acc: 0.9320\n","Epoch: 64, Train Loss: 0.0009, Train Acc: 0.9722, Val Loss: 0.0047, Val Acc: 0.9308\n","Epoch: 65, Train Loss: 0.0009, Train Acc: 0.9729, Val Loss: 0.0046, Val Acc: 0.9318\n","Epoch: 66, Train Loss: 0.0009, Train Acc: 0.9724, Val Loss: 0.0045, Val Acc: 0.9304\n","Epoch: 67, Train Loss: 0.0009, Train Acc: 0.9727, Val Loss: 0.0047, Val Acc: 0.9316\n","Epoch: 68, Train Loss: 0.0009, Train Acc: 0.9728, Val Loss: 0.0046, Val Acc: 0.9341\n","Epoch: 69, Train Loss: 0.0009, Train Acc: 0.9734, Val Loss: 0.0047, Val Acc: 0.9329\n","Epoch: 70, Train Loss: 0.0009, Train Acc: 0.9725, Val Loss: 0.0043, Val Acc: 0.9324\n","Epoch: 71, Train Loss: 0.0009, Train Acc: 0.9726, Val Loss: 0.0045, Val Acc: 0.9348\n","Epoch: 72, Train Loss: 0.0009, Train Acc: 0.9727, Val Loss: 0.0044, Val Acc: 0.9336\n","Epoch: 73, Train Loss: 0.0009, Train Acc: 0.9734, Val Loss: 0.0044, Val Acc: 0.9324\n","Epoch: 74, Train Loss: 0.0009, Train Acc: 0.9726, Val Loss: 0.0044, Val Acc: 0.9304\n","Epoch: 75, Train Loss: 0.0009, Train Acc: 0.9728, Val Loss: 0.0044, Val Acc: 0.9360\n","Epoch: 76, Train Loss: 0.0009, Train Acc: 0.9729, Val Loss: 0.0046, Val Acc: 0.9271\n","Epoch: 77, Train Loss: 0.0009, Train Acc: 0.9724, Val Loss: 0.0047, Val Acc: 0.9306\n","Epoch: 78, Train Loss: 0.0009, Train Acc: 0.9731, Val Loss: 0.0046, Val Acc: 0.9334\n","Epoch: 79, Train Loss: 0.0009, Train Acc: 0.9732, Val Loss: 0.0047, Val Acc: 0.9314\n","Epoch: 80, Train Loss: 0.0009, Train Acc: 0.9726, Val Loss: 0.0046, Val Acc: 0.9310\n","Epoch: 81, Train Loss: 0.0009, Train Acc: 0.9732, Val Loss: 0.0042, Val Acc: 0.9326\n","Epoch: 82, Train Loss: 0.0009, Train Acc: 0.9734, Val Loss: 0.0045, Val Acc: 0.9343\n","Epoch: 83, Train Loss: 0.0009, Train Acc: 0.9731, Val Loss: 0.0047, Val Acc: 0.9327\n","Epoch: 84, Train Loss: 0.0009, Train Acc: 0.9728, Val Loss: 0.0043, Val Acc: 0.9303\n","Epoch: 85, Train Loss: 0.0009, Train Acc: 0.9731, Val Loss: 0.0044, Val Acc: 0.9326\n","Epoch: 86, Train Loss: 0.0009, Train Acc: 0.9733, Val Loss: 0.0045, Val Acc: 0.9324\n","Epoch: 87, Train Loss: 0.0008, Train Acc: 0.9736, Val Loss: 0.0050, Val Acc: 0.9310\n","Epoch: 88, Train Loss: 0.0009, Train Acc: 0.9727, Val Loss: 0.0044, Val Acc: 0.9316\n","Epoch: 89, Train Loss: 0.0009, Train Acc: 0.9732, Val Loss: 0.0046, Val Acc: 0.9315\n","Epoch: 90, Train Loss: 0.0009, Train Acc: 0.9726, Val Loss: 0.0047, Val Acc: 0.9317\n","Epoch: 91, Train Loss: 0.0009, Train Acc: 0.9735, Val Loss: 0.0045, Val Acc: 0.9322\n","Epoch: 92, Train Loss: 0.0008, Train Acc: 0.9736, Val Loss: 0.0048, Val Acc: 0.9347\n","Epoch: 93, Train Loss: 0.0009, Train Acc: 0.9730, Val Loss: 0.0046, Val Acc: 0.9322\n","Epoch: 94, Train Loss: 0.0009, Train Acc: 0.9730, Val Loss: 0.0050, Val Acc: 0.9310\n","Epoch: 95, Train Loss: 0.0009, Train Acc: 0.9732, Val Loss: 0.0048, Val Acc: 0.9272\n","Epoch: 96, Train Loss: 0.0009, Train Acc: 0.9730, Val Loss: 0.0045, Val Acc: 0.9341\n","Epoch: 97, Train Loss: 0.0009, Train Acc: 0.9734, Val Loss: 0.0047, Val Acc: 0.9311\n","Epoch: 98, Train Loss: 0.0009, Train Acc: 0.9727, Val Loss: 0.0044, Val Acc: 0.9342\n","Epoch: 99, Train Loss: 0.0008, Train Acc: 0.9736, Val Loss: 0.0050, Val Acc: 0.9326\n","Epoch: 100, Train Loss: 0.0009, Train Acc: 0.9727, Val Loss: 0.0045, Val Acc: 0.9326\n"]}],"source":["view_line('logs/bert_2.log')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Training TextResNet:**\\\n","The training requires high GPU performance and long time. \\\n","**Notice:**  You can **skip** the step and directly view the training records in the log"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# **Notice:** You can skip this cell if you want to save time\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Convert data into tensors\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float).unsqueeze(1).to(device)  # add dimension at position 1\n","y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n","X_val_tensor = torch.tensor(X_val, dtype=torch.float).unsqueeze(1).to(device)  # add dimension at position 1\n","y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n","\n","# Create datasets and data loaders\n","batch_size = 32\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n","\n","# Initialize model\n","num_classes = 4\n","input_dim = 1\n","model = TextResNetClassifier(input_dim, num_classes).to(device)\n","\n","# Set up training\n","learning_rate = 0.001\n","num_epochs = 1000\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Train the model\n","for epoch in range(num_epochs):\n","    train_loss, train_acc = train_res_classifier(model, train_loader, optimizer, criterion)\n","    val_loss, val_acc = evaluate_res_classifier(model, val_loader, criterion)\n","\n","    print(f\"Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","    with open (\"res4.log\", \"a\") as f:\n","        f.write(f\"Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\\n\")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1, Train Loss: 1.2841, Train Acc: 0.4204, Val Loss: 1.2050, Val Acc: 0.4691\n","Epoch: 2, Train Loss: 1.1756, Train Acc: 0.4844, Val Loss: 1.1708, Val Acc: 0.4875\n","Epoch: 3, Train Loss: 1.1453, Train Acc: 0.5009, Val Loss: 1.1218, Val Acc: 0.5152\n","Epoch: 4, Train Loss: 1.1226, Train Acc: 0.5114, Val Loss: 1.1100, Val Acc: 0.5169\n","Epoch: 5, Train Loss: 1.0991, Train Acc: 0.5243, Val Loss: 1.0829, Val Acc: 0.5367\n","Epoch: 6, Train Loss: 1.0761, Train Acc: 0.5384, Val Loss: 1.0625, Val Acc: 0.5433\n","Epoch: 7, Train Loss: 1.0529, Train Acc: 0.5455, Val Loss: 1.0519, Val Acc: 0.5508\n","Epoch: 8, Train Loss: 1.0247, Train Acc: 0.5611, Val Loss: 1.0301, Val Acc: 0.5582\n","Epoch: 9, Train Loss: 0.9993, Train Acc: 0.5731, Val Loss: 1.0179, Val Acc: 0.5631\n","Epoch: 10, Train Loss: 0.9682, Train Acc: 0.5877, Val Loss: 0.9712, Val Acc: 0.5888\n","Epoch: 11, Train Loss: 0.9386, Train Acc: 0.6035, Val Loss: 0.9503, Val Acc: 0.6018\n","Epoch: 12, Train Loss: 0.9048, Train Acc: 0.6198, Val Loss: 0.9314, Val Acc: 0.6158\n","Epoch: 13, Train Loss: 0.8722, Train Acc: 0.6364, Val Loss: 0.9066, Val Acc: 0.6269\n","Epoch: 14, Train Loss: 0.8411, Train Acc: 0.6527, Val Loss: 0.8846, Val Acc: 0.6387\n","Epoch: 15, Train Loss: 0.8086, Train Acc: 0.6656, Val Loss: 0.8649, Val Acc: 0.6474\n","Epoch: 16, Train Loss: 0.7738, Train Acc: 0.6794, Val Loss: 0.8308, Val Acc: 0.6641\n","Epoch: 17, Train Loss: 0.7478, Train Acc: 0.6924, Val Loss: 0.8233, Val Acc: 0.6708\n","Epoch: 18, Train Loss: 0.7174, Train Acc: 0.7066, Val Loss: 0.7968, Val Acc: 0.6797\n","Epoch: 19, Train Loss: 0.6909, Train Acc: 0.7191, Val Loss: 0.7839, Val Acc: 0.6882\n","Epoch: 20, Train Loss: 0.6645, Train Acc: 0.7285, Val Loss: 0.7842, Val Acc: 0.6909\n","Epoch: 21, Train Loss: 0.6423, Train Acc: 0.7401, Val Loss: 0.7613, Val Acc: 0.7036\n","Epoch: 22, Train Loss: 0.6186, Train Acc: 0.7505, Val Loss: 0.7617, Val Acc: 0.7105\n","Epoch: 23, Train Loss: 0.6036, Train Acc: 0.7544, Val Loss: 0.7345, Val Acc: 0.7183\n","Epoch: 24, Train Loss: 0.5763, Train Acc: 0.7670, Val Loss: 0.7395, Val Acc: 0.7217\n","Epoch: 25, Train Loss: 0.5590, Train Acc: 0.7762, Val Loss: 0.7235, Val Acc: 0.7322\n","Epoch: 26, Train Loss: 0.5405, Train Acc: 0.7831, Val Loss: 0.7216, Val Acc: 0.7335\n","Epoch: 27, Train Loss: 0.5221, Train Acc: 0.7896, Val Loss: 0.7235, Val Acc: 0.7348\n","Epoch: 28, Train Loss: 0.5038, Train Acc: 0.8005, Val Loss: 0.7115, Val Acc: 0.7400\n","Epoch: 29, Train Loss: 0.4909, Train Acc: 0.8032, Val Loss: 0.7170, Val Acc: 0.7429\n","Epoch: 30, Train Loss: 0.4775, Train Acc: 0.8089, Val Loss: 0.7193, Val Acc: 0.7395\n","Epoch: 31, Train Loss: 0.4608, Train Acc: 0.8175, Val Loss: 0.7128, Val Acc: 0.7504\n","Epoch: 32, Train Loss: 0.4477, Train Acc: 0.8212, Val Loss: 0.6949, Val Acc: 0.7515\n","Epoch: 33, Train Loss: 0.4336, Train Acc: 0.8261, Val Loss: 0.6992, Val Acc: 0.7581\n","Epoch: 34, Train Loss: 0.4241, Train Acc: 0.8313, Val Loss: 0.7028, Val Acc: 0.7573\n","Epoch: 35, Train Loss: 0.4163, Train Acc: 0.8341, Val Loss: 0.6939, Val Acc: 0.7662\n","Epoch: 36, Train Loss: 0.4045, Train Acc: 0.8395, Val Loss: 0.6937, Val Acc: 0.7624\n","Epoch: 37, Train Loss: 0.3949, Train Acc: 0.8425, Val Loss: 0.6898, Val Acc: 0.7611\n","Epoch: 38, Train Loss: 0.3812, Train Acc: 0.8481, Val Loss: 0.6943, Val Acc: 0.7662\n","Epoch: 39, Train Loss: 0.3726, Train Acc: 0.8519, Val Loss: 0.7098, Val Acc: 0.7680\n","Epoch: 40, Train Loss: 0.3653, Train Acc: 0.8542, Val Loss: 0.7297, Val Acc: 0.7666\n","Epoch: 41, Train Loss: 0.3574, Train Acc: 0.8577, Val Loss: 0.7176, Val Acc: 0.7723\n","Epoch: 42, Train Loss: 0.3517, Train Acc: 0.8595, Val Loss: 0.7071, Val Acc: 0.7720\n","Epoch: 43, Train Loss: 0.3419, Train Acc: 0.8642, Val Loss: 0.7278, Val Acc: 0.7706\n","Epoch: 44, Train Loss: 0.3369, Train Acc: 0.8660, Val Loss: 0.6936, Val Acc: 0.7782\n","Epoch: 45, Train Loss: 0.3300, Train Acc: 0.8677, Val Loss: 0.7205, Val Acc: 0.7760\n","Epoch: 46, Train Loss: 0.3217, Train Acc: 0.8710, Val Loss: 0.7479, Val Acc: 0.7748\n","Epoch: 47, Train Loss: 0.3153, Train Acc: 0.8745, Val Loss: 0.7326, Val Acc: 0.7761\n","Epoch: 48, Train Loss: 0.3098, Train Acc: 0.8770, Val Loss: 0.6986, Val Acc: 0.7780\n","Epoch: 49, Train Loss: 0.3043, Train Acc: 0.8790, Val Loss: 0.7076, Val Acc: 0.7771\n","Epoch: 50, Train Loss: 0.2991, Train Acc: 0.8816, Val Loss: 0.7249, Val Acc: 0.7794\n","Epoch: 51, Train Loss: 0.2971, Train Acc: 0.8816, Val Loss: 0.7330, Val Acc: 0.7818\n","Epoch: 52, Train Loss: 0.2896, Train Acc: 0.8853, Val Loss: 0.7129, Val Acc: 0.7834\n","Epoch: 53, Train Loss: 0.2821, Train Acc: 0.8882, Val Loss: 0.7334, Val Acc: 0.7827\n","Epoch: 54, Train Loss: 0.2788, Train Acc: 0.8887, Val Loss: 0.7572, Val Acc: 0.7881\n","Epoch: 55, Train Loss: 0.2794, Train Acc: 0.8888, Val Loss: 0.7353, Val Acc: 0.7884\n","Epoch: 56, Train Loss: 0.2725, Train Acc: 0.8915, Val Loss: 0.7250, Val Acc: 0.7844\n","Epoch: 57, Train Loss: 0.2677, Train Acc: 0.8936, Val Loss: 0.7223, Val Acc: 0.7894\n","Epoch: 58, Train Loss: 0.2666, Train Acc: 0.8956, Val Loss: 0.7620, Val Acc: 0.7855\n","Epoch: 59, Train Loss: 0.2621, Train Acc: 0.8957, Val Loss: 0.7290, Val Acc: 0.7878\n","Epoch: 60, Train Loss: 0.2564, Train Acc: 0.8987, Val Loss: 0.7299, Val Acc: 0.7873\n","Epoch: 61, Train Loss: 0.2517, Train Acc: 0.9003, Val Loss: 0.7784, Val Acc: 0.7834\n","Epoch: 62, Train Loss: 0.2522, Train Acc: 0.9004, Val Loss: 0.7817, Val Acc: 0.7849\n","Epoch: 63, Train Loss: 0.2495, Train Acc: 0.9003, Val Loss: 0.7785, Val Acc: 0.7849\n","Epoch: 64, Train Loss: 0.2472, Train Acc: 0.9011, Val Loss: 0.7564, Val Acc: 0.7880\n","Epoch: 65, Train Loss: 0.2405, Train Acc: 0.9041, Val Loss: 0.7605, Val Acc: 0.7887\n","Epoch: 66, Train Loss: 0.2429, Train Acc: 0.9026, Val Loss: 0.7438, Val Acc: 0.7890\n","Epoch: 67, Train Loss: 0.2348, Train Acc: 0.9078, Val Loss: 0.7730, Val Acc: 0.7911\n","Epoch: 68, Train Loss: 0.2335, Train Acc: 0.9074, Val Loss: 0.7709, Val Acc: 0.7929\n","Epoch: 69, Train Loss: 0.2335, Train Acc: 0.9060, Val Loss: 0.7696, Val Acc: 0.7851\n","Epoch: 70, Train Loss: 0.2283, Train Acc: 0.9082, Val Loss: 0.7704, Val Acc: 0.7868\n","Epoch: 71, Train Loss: 0.2253, Train Acc: 0.9098, Val Loss: 0.7813, Val Acc: 0.7904\n","Epoch: 72, Train Loss: 0.2231, Train Acc: 0.9109, Val Loss: 0.7914, Val Acc: 0.7909\n","Epoch: 73, Train Loss: 0.2210, Train Acc: 0.9114, Val Loss: 0.7724, Val Acc: 0.7847\n","Epoch: 74, Train Loss: 0.2196, Train Acc: 0.9117, Val Loss: 0.7949, Val Acc: 0.7904\n","Epoch: 75, Train Loss: 0.2154, Train Acc: 0.9134, Val Loss: 0.7716, Val Acc: 0.7937\n","Epoch: 76, Train Loss: 0.2156, Train Acc: 0.9136, Val Loss: 0.7695, Val Acc: 0.7965\n","Epoch: 77, Train Loss: 0.2114, Train Acc: 0.9166, Val Loss: 0.7723, Val Acc: 0.7932\n","Epoch: 78, Train Loss: 0.2111, Train Acc: 0.9156, Val Loss: 0.7998, Val Acc: 0.7923\n","Epoch: 79, Train Loss: 0.2081, Train Acc: 0.9170, Val Loss: 0.7989, Val Acc: 0.7963\n","Epoch: 80, Train Loss: 0.2071, Train Acc: 0.9163, Val Loss: 0.8101, Val Acc: 0.7922\n","Epoch: 81, Train Loss: 0.2059, Train Acc: 0.9170, Val Loss: 0.8185, Val Acc: 0.7931\n","Epoch: 82, Train Loss: 0.2056, Train Acc: 0.9177, Val Loss: 0.8061, Val Acc: 0.7968\n","Epoch: 83, Train Loss: 0.2006, Train Acc: 0.9194, Val Loss: 0.8008, Val Acc: 0.7933\n","Epoch: 84, Train Loss: 0.1990, Train Acc: 0.9197, Val Loss: 0.7592, Val Acc: 0.7954\n","Epoch: 85, Train Loss: 0.1978, Train Acc: 0.9207, Val Loss: 0.7989, Val Acc: 0.7943\n","Epoch: 86, Train Loss: 0.1968, Train Acc: 0.9209, Val Loss: 0.8287, Val Acc: 0.7934\n","Epoch: 87, Train Loss: 0.1965, Train Acc: 0.9218, Val Loss: 0.7995, Val Acc: 0.8002\n","Epoch: 88, Train Loss: 0.1930, Train Acc: 0.9218, Val Loss: 0.8088, Val Acc: 0.7951\n","Epoch: 89, Train Loss: 0.1948, Train Acc: 0.9201, Val Loss: 0.8091, Val Acc: 0.7937\n","Epoch: 90, Train Loss: 0.1897, Train Acc: 0.9232, Val Loss: 0.7999, Val Acc: 0.7966\n","Epoch: 91, Train Loss: 0.1906, Train Acc: 0.9229, Val Loss: 0.8257, Val Acc: 0.7930\n","Epoch: 92, Train Loss: 0.1906, Train Acc: 0.9232, Val Loss: 0.8001, Val Acc: 0.7962\n","Epoch: 93, Train Loss: 0.1873, Train Acc: 0.9238, Val Loss: 0.8319, Val Acc: 0.7935\n","Epoch: 94, Train Loss: 0.1879, Train Acc: 0.9231, Val Loss: 0.8169, Val Acc: 0.7945\n","Epoch: 95, Train Loss: 0.1884, Train Acc: 0.9232, Val Loss: 0.8192, Val Acc: 0.7979\n","Epoch: 96, Train Loss: 0.1834, Train Acc: 0.9254, Val Loss: 0.8151, Val Acc: 0.7969\n","Epoch: 97, Train Loss: 0.1826, Train Acc: 0.9262, Val Loss: 0.8523, Val Acc: 0.7975\n","Epoch: 98, Train Loss: 0.1819, Train Acc: 0.9265, Val Loss: 0.8193, Val Acc: 0.7959\n","Epoch: 99, Train Loss: 0.1807, Train Acc: 0.9265, Val Loss: 0.8204, Val Acc: 0.8034\n","Epoch: 100, Train Loss: 0.1783, Train Acc: 0.9271, Val Loss: 0.8435, Val Acc: 0.7971\n","Epoch: 101, Train Loss: 0.1794, Train Acc: 0.9269, Val Loss: 0.8279, Val Acc: 0.7971\n","Epoch: 102, Train Loss: 0.1794, Train Acc: 0.9269, Val Loss: 0.8667, Val Acc: 0.7913\n","Epoch: 103, Train Loss: 0.1773, Train Acc: 0.9275, Val Loss: 0.8227, Val Acc: 0.7987\n","Epoch: 104, Train Loss: 0.1778, Train Acc: 0.9284, Val Loss: 0.8071, Val Acc: 0.7994\n","Epoch: 105, Train Loss: 0.1734, Train Acc: 0.9289, Val Loss: 0.7937, Val Acc: 0.8006\n","Epoch: 106, Train Loss: 0.1720, Train Acc: 0.9295, Val Loss: 0.8469, Val Acc: 0.8002\n","Epoch: 107, Train Loss: 0.1741, Train Acc: 0.9286, Val Loss: 0.8148, Val Acc: 0.7967\n","Epoch: 108, Train Loss: 0.1719, Train Acc: 0.9282, Val Loss: 0.8161, Val Acc: 0.7994\n","Epoch: 109, Train Loss: 0.1704, Train Acc: 0.9307, Val Loss: 0.8237, Val Acc: 0.7996\n","Epoch: 110, Train Loss: 0.1677, Train Acc: 0.9305, Val Loss: 0.8429, Val Acc: 0.8016\n","Epoch: 111, Train Loss: 0.1658, Train Acc: 0.9323, Val Loss: 0.8582, Val Acc: 0.7969\n","Epoch: 112, Train Loss: 0.1667, Train Acc: 0.9322, Val Loss: 0.8702, Val Acc: 0.7959\n","Epoch: 113, Train Loss: 0.1713, Train Acc: 0.9297, Val Loss: 0.8398, Val Acc: 0.7981\n","Epoch: 114, Train Loss: 0.1667, Train Acc: 0.9319, Val Loss: 0.8274, Val Acc: 0.7998\n","Epoch: 115, Train Loss: 0.1663, Train Acc: 0.9314, Val Loss: 0.8421, Val Acc: 0.7981\n","Epoch: 116, Train Loss: 0.1642, Train Acc: 0.9315, Val Loss: 0.8199, Val Acc: 0.7995\n","Epoch: 117, Train Loss: 0.1670, Train Acc: 0.9316, Val Loss: 0.8189, Val Acc: 0.8003\n","Epoch: 118, Train Loss: 0.1617, Train Acc: 0.9333, Val Loss: 0.8497, Val Acc: 0.8019\n","Epoch: 119, Train Loss: 0.1613, Train Acc: 0.9330, Val Loss: 0.8698, Val Acc: 0.7955\n","Epoch: 120, Train Loss: 0.1634, Train Acc: 0.9327, Val Loss: 0.8858, Val Acc: 0.7926\n","Epoch: 121, Train Loss: 0.1613, Train Acc: 0.9324, Val Loss: 0.8758, Val Acc: 0.7965\n","Epoch: 122, Train Loss: 0.1602, Train Acc: 0.9336, Val Loss: 0.8712, Val Acc: 0.8005\n","Epoch: 123, Train Loss: 0.1592, Train Acc: 0.9339, Val Loss: 0.8719, Val Acc: 0.7981\n","Epoch: 124, Train Loss: 0.1611, Train Acc: 0.9333, Val Loss: 0.8573, Val Acc: 0.7990\n","Epoch: 125, Train Loss: 0.1600, Train Acc: 0.9333, Val Loss: 0.8638, Val Acc: 0.8004\n","Epoch: 126, Train Loss: 0.1583, Train Acc: 0.9350, Val Loss: 0.8873, Val Acc: 0.7973\n","Epoch: 127, Train Loss: 0.1577, Train Acc: 0.9351, Val Loss: 0.8715, Val Acc: 0.7982\n","Epoch: 128, Train Loss: 0.1559, Train Acc: 0.9355, Val Loss: 0.8561, Val Acc: 0.8006\n","Epoch: 129, Train Loss: 0.1560, Train Acc: 0.9356, Val Loss: 0.8715, Val Acc: 0.8002\n","Epoch: 130, Train Loss: 0.1554, Train Acc: 0.9356, Val Loss: 0.8664, Val Acc: 0.7997\n","Epoch: 131, Train Loss: 0.1558, Train Acc: 0.9354, Val Loss: 0.8825, Val Acc: 0.7953\n","Epoch: 132, Train Loss: 0.1530, Train Acc: 0.9361, Val Loss: 0.8810, Val Acc: 0.7970\n","Epoch: 133, Train Loss: 0.1525, Train Acc: 0.9370, Val Loss: 0.9226, Val Acc: 0.7969\n","Epoch: 134, Train Loss: 0.1524, Train Acc: 0.9371, Val Loss: 0.8944, Val Acc: 0.7963\n","Epoch: 135, Train Loss: 0.1526, Train Acc: 0.9368, Val Loss: 0.8634, Val Acc: 0.8020\n","Epoch: 136, Train Loss: 0.1535, Train Acc: 0.9368, Val Loss: 0.8632, Val Acc: 0.8018\n","Epoch: 137, Train Loss: 0.1521, Train Acc: 0.9371, Val Loss: 0.9015, Val Acc: 0.8021\n","Epoch: 138, Train Loss: 0.1527, Train Acc: 0.9369, Val Loss: 0.8659, Val Acc: 0.7998\n","Epoch: 139, Train Loss: 0.1498, Train Acc: 0.9370, Val Loss: 0.8402, Val Acc: 0.8040\n","Epoch: 140, Train Loss: 0.1492, Train Acc: 0.9371, Val Loss: 0.8599, Val Acc: 0.8021\n","Epoch: 141, Train Loss: 0.1514, Train Acc: 0.9367, Val Loss: 0.8598, Val Acc: 0.7999\n","Epoch: 142, Train Loss: 0.1498, Train Acc: 0.9378, Val Loss: 0.8735, Val Acc: 0.7994\n","Epoch: 143, Train Loss: 0.1527, Train Acc: 0.9371, Val Loss: 0.8568, Val Acc: 0.8035\n","Epoch: 144, Train Loss: 0.1478, Train Acc: 0.9382, Val Loss: 0.8566, Val Acc: 0.8024\n","Epoch: 145, Train Loss: 0.1480, Train Acc: 0.9384, Val Loss: 0.8687, Val Acc: 0.7998\n","Epoch: 146, Train Loss: 0.1481, Train Acc: 0.9383, Val Loss: 0.8957, Val Acc: 0.8021\n","Epoch: 147, Train Loss: 0.1454, Train Acc: 0.9399, Val Loss: 0.8708, Val Acc: 0.7981\n","Epoch: 148, Train Loss: 0.1460, Train Acc: 0.9385, Val Loss: 0.8747, Val Acc: 0.8006\n","Epoch: 149, Train Loss: 0.1463, Train Acc: 0.9390, Val Loss: 0.8480, Val Acc: 0.8030\n","Epoch: 150, Train Loss: 0.1436, Train Acc: 0.9397, Val Loss: 0.8902, Val Acc: 0.8014\n","Epoch: 151, Train Loss: 0.1452, Train Acc: 0.9388, Val Loss: 0.8868, Val Acc: 0.8031\n","Epoch: 152, Train Loss: 0.1441, Train Acc: 0.9395, Val Loss: 0.8776, Val Acc: 0.8026\n","Epoch: 153, Train Loss: 0.1425, Train Acc: 0.9399, Val Loss: 0.8768, Val Acc: 0.8031\n","Epoch: 154, Train Loss: 0.1433, Train Acc: 0.9406, Val Loss: 0.8692, Val Acc: 0.8016\n","Epoch: 155, Train Loss: 0.1463, Train Acc: 0.9382, Val Loss: 0.8692, Val Acc: 0.8008\n","Epoch: 156, Train Loss: 0.1415, Train Acc: 0.9398, Val Loss: 0.9310, Val Acc: 0.8017\n","Epoch: 157, Train Loss: 0.1418, Train Acc: 0.9405, Val Loss: 0.9235, Val Acc: 0.7986\n","Epoch: 158, Train Loss: 0.1419, Train Acc: 0.9411, Val Loss: 0.9151, Val Acc: 0.7991\n","Epoch: 159, Train Loss: 0.1399, Train Acc: 0.9409, Val Loss: 0.9022, Val Acc: 0.7998\n","Epoch: 160, Train Loss: 0.1403, Train Acc: 0.9411, Val Loss: 0.9291, Val Acc: 0.8004\n","Epoch: 161, Train Loss: 0.1404, Train Acc: 0.9410, Val Loss: 0.9383, Val Acc: 0.8019\n","Epoch: 162, Train Loss: 0.1404, Train Acc: 0.9422, Val Loss: 0.9173, Val Acc: 0.8008\n","Epoch: 163, Train Loss: 0.1412, Train Acc: 0.9415, Val Loss: 0.8669, Val Acc: 0.8024\n","Epoch: 164, Train Loss: 0.1418, Train Acc: 0.9405, Val Loss: 0.8618, Val Acc: 0.8048\n","Epoch: 165, Train Loss: 0.1388, Train Acc: 0.9410, Val Loss: 0.9317, Val Acc: 0.8022\n","Epoch: 166, Train Loss: 0.1413, Train Acc: 0.9405, Val Loss: 0.8748, Val Acc: 0.8020\n","Epoch: 167, Train Loss: 0.1389, Train Acc: 0.9414, Val Loss: 0.8971, Val Acc: 0.8041\n","Epoch: 168, Train Loss: 0.1395, Train Acc: 0.9409, Val Loss: 0.9539, Val Acc: 0.7980\n","Epoch: 169, Train Loss: 0.1375, Train Acc: 0.9411, Val Loss: 0.8885, Val Acc: 0.8039\n","Epoch: 170, Train Loss: 0.1375, Train Acc: 0.9415, Val Loss: 0.9247, Val Acc: 0.8041\n","Epoch: 171, Train Loss: 0.1400, Train Acc: 0.9417, Val Loss: 0.9306, Val Acc: 0.7986\n","Epoch: 172, Train Loss: 0.1370, Train Acc: 0.9427, Val Loss: 0.9310, Val Acc: 0.8023\n","Epoch: 173, Train Loss: 0.1370, Train Acc: 0.9427, Val Loss: 0.8851, Val Acc: 0.8049\n","Epoch: 174, Train Loss: 0.1366, Train Acc: 0.9426, Val Loss: 0.9438, Val Acc: 0.7998\n","Epoch: 175, Train Loss: 0.1346, Train Acc: 0.9430, Val Loss: 0.9142, Val Acc: 0.7997\n","Epoch: 176, Train Loss: 0.1351, Train Acc: 0.9428, Val Loss: 0.9380, Val Acc: 0.8024\n","Epoch: 177, Train Loss: 0.1328, Train Acc: 0.9434, Val Loss: 0.9646, Val Acc: 0.7976\n","Epoch: 178, Train Loss: 0.1341, Train Acc: 0.9440, Val Loss: 0.9342, Val Acc: 0.7993\n","Epoch: 179, Train Loss: 0.1352, Train Acc: 0.9426, Val Loss: 0.9514, Val Acc: 0.8022\n","Epoch: 180, Train Loss: 0.1374, Train Acc: 0.9428, Val Loss: 0.8916, Val Acc: 0.8054\n","Epoch: 181, Train Loss: 0.1330, Train Acc: 0.9436, Val Loss: 0.9229, Val Acc: 0.8036\n","Epoch: 182, Train Loss: 0.1368, Train Acc: 0.9415, Val Loss: 0.9557, Val Acc: 0.8019\n","Epoch: 183, Train Loss: 0.1343, Train Acc: 0.9430, Val Loss: 0.9123, Val Acc: 0.8008\n","Epoch: 184, Train Loss: 0.1324, Train Acc: 0.9434, Val Loss: 0.9100, Val Acc: 0.8014\n","Epoch: 185, Train Loss: 0.1302, Train Acc: 0.9444, Val Loss: 0.9321, Val Acc: 0.8045\n","Epoch: 186, Train Loss: 0.1347, Train Acc: 0.9424, Val Loss: 0.9535, Val Acc: 0.8036\n","Epoch: 187, Train Loss: 0.1301, Train Acc: 0.9449, Val Loss: 0.9159, Val Acc: 0.8014\n","Epoch: 188, Train Loss: 0.1325, Train Acc: 0.9441, Val Loss: 0.9048, Val Acc: 0.8008\n","Epoch: 189, Train Loss: 0.1351, Train Acc: 0.9429, Val Loss: 0.8908, Val Acc: 0.8015\n","Epoch: 190, Train Loss: 0.1325, Train Acc: 0.9438, Val Loss: 0.9066, Val Acc: 0.8032\n","Epoch: 191, Train Loss: 0.1310, Train Acc: 0.9447, Val Loss: 0.9223, Val Acc: 0.8022\n","Epoch: 192, Train Loss: 0.1324, Train Acc: 0.9448, Val Loss: 0.9185, Val Acc: 0.8046\n","Epoch: 193, Train Loss: 0.1320, Train Acc: 0.9445, Val Loss: 0.9400, Val Acc: 0.8002\n","Epoch: 194, Train Loss: 0.1339, Train Acc: 0.9430, Val Loss: 0.9585, Val Acc: 0.8032\n","Epoch: 195, Train Loss: 0.1295, Train Acc: 0.9453, Val Loss: 0.9203, Val Acc: 0.8020\n","Epoch: 196, Train Loss: 0.1297, Train Acc: 0.9442, Val Loss: 0.9298, Val Acc: 0.8027\n","Epoch: 197, Train Loss: 0.1308, Train Acc: 0.9447, Val Loss: 0.9063, Val Acc: 0.8038\n","Epoch: 198, Train Loss: 0.1284, Train Acc: 0.9449, Val Loss: 0.9142, Val Acc: 0.8063\n","Epoch: 199, Train Loss: 0.1304, Train Acc: 0.9447, Val Loss: 0.9259, Val Acc: 0.8053\n","Epoch: 200, Train Loss: 0.1288, Train Acc: 0.9450, Val Loss: 0.9464, Val Acc: 0.8021\n","Epoch: 201, Train Loss: 0.1266, Train Acc: 0.9460, Val Loss: 0.9153, Val Acc: 0.8061\n","Epoch: 202, Train Loss: 0.1300, Train Acc: 0.9444, Val Loss: 0.9295, Val Acc: 0.8031\n","Epoch: 203, Train Loss: 0.1280, Train Acc: 0.9459, Val Loss: 0.9622, Val Acc: 0.8010\n","Epoch: 204, Train Loss: 0.1290, Train Acc: 0.9449, Val Loss: 0.9274, Val Acc: 0.8038\n","Epoch: 205, Train Loss: 0.1288, Train Acc: 0.9450, Val Loss: 0.9286, Val Acc: 0.8039\n","Epoch: 206, Train Loss: 0.1273, Train Acc: 0.9455, Val Loss: 0.9216, Val Acc: 0.8075\n","Epoch: 207, Train Loss: 0.1293, Train Acc: 0.9453, Val Loss: 0.9315, Val Acc: 0.8050\n","Epoch: 208, Train Loss: 0.1270, Train Acc: 0.9451, Val Loss: 0.9364, Val Acc: 0.8040\n","Epoch: 209, Train Loss: 0.1273, Train Acc: 0.9459, Val Loss: 0.9505, Val Acc: 0.8036\n","Epoch: 210, Train Loss: 0.1260, Train Acc: 0.9464, Val Loss: 0.9192, Val Acc: 0.8049\n","Epoch: 211, Train Loss: 0.1286, Train Acc: 0.9443, Val Loss: 0.9241, Val Acc: 0.8081\n","Epoch: 212, Train Loss: 0.1256, Train Acc: 0.9466, Val Loss: 0.9444, Val Acc: 0.8077\n","Epoch: 213, Train Loss: 0.1265, Train Acc: 0.9465, Val Loss: 0.9724, Val Acc: 0.8025\n","Epoch: 214, Train Loss: 0.1250, Train Acc: 0.9460, Val Loss: 0.9198, Val Acc: 0.8038\n","Epoch: 215, Train Loss: 0.1265, Train Acc: 0.9466, Val Loss: 0.9335, Val Acc: 0.7996\n","Epoch: 216, Train Loss: 0.1272, Train Acc: 0.9456, Val Loss: 0.9128, Val Acc: 0.8044\n","Epoch: 217, Train Loss: 0.1263, Train Acc: 0.9466, Val Loss: 0.9540, Val Acc: 0.8055\n","Epoch: 218, Train Loss: 0.1238, Train Acc: 0.9466, Val Loss: 0.9525, Val Acc: 0.8047\n","Epoch: 219, Train Loss: 0.1269, Train Acc: 0.9457, Val Loss: 0.9562, Val Acc: 0.8026\n","Epoch: 220, Train Loss: 0.1262, Train Acc: 0.9456, Val Loss: 0.9388, Val Acc: 0.8040\n","Epoch: 221, Train Loss: 0.1220, Train Acc: 0.9474, Val Loss: 0.9411, Val Acc: 0.8052\n","Epoch: 222, Train Loss: 0.1248, Train Acc: 0.9457, Val Loss: 0.9155, Val Acc: 0.8059\n","Epoch: 223, Train Loss: 0.1249, Train Acc: 0.9458, Val Loss: 0.9402, Val Acc: 0.8048\n","Epoch: 224, Train Loss: 0.1229, Train Acc: 0.9467, Val Loss: 0.9581, Val Acc: 0.8043\n","Epoch: 225, Train Loss: 0.1227, Train Acc: 0.9471, Val Loss: 0.9493, Val Acc: 0.8058\n","Epoch: 226, Train Loss: 0.1228, Train Acc: 0.9475, Val Loss: 0.9315, Val Acc: 0.8040\n","Epoch: 227, Train Loss: 0.1231, Train Acc: 0.9474, Val Loss: 0.9535, Val Acc: 0.8036\n","Epoch: 228, Train Loss: 0.1240, Train Acc: 0.9476, Val Loss: 0.9454, Val Acc: 0.8055\n","Epoch: 229, Train Loss: 0.1232, Train Acc: 0.9471, Val Loss: 0.9368, Val Acc: 0.8030\n","Epoch: 230, Train Loss: 0.1237, Train Acc: 0.9470, Val Loss: 0.9645, Val Acc: 0.8026\n","Epoch: 231, Train Loss: 0.1234, Train Acc: 0.9473, Val Loss: 0.9298, Val Acc: 0.8049\n","Epoch: 232, Train Loss: 0.1224, Train Acc: 0.9470, Val Loss: 0.9252, Val Acc: 0.8057\n","Epoch: 233, Train Loss: 0.1231, Train Acc: 0.9470, Val Loss: 0.9191, Val Acc: 0.8077\n","Epoch: 234, Train Loss: 0.1237, Train Acc: 0.9469, Val Loss: 0.9494, Val Acc: 0.8061\n","Epoch: 235, Train Loss: 0.1208, Train Acc: 0.9476, Val Loss: 0.9451, Val Acc: 0.8045\n","Epoch: 236, Train Loss: 0.1225, Train Acc: 0.9463, Val Loss: 0.9263, Val Acc: 0.8055\n","Epoch: 237, Train Loss: 0.1220, Train Acc: 0.9479, Val Loss: 0.9144, Val Acc: 0.8099\n","Epoch: 238, Train Loss: 0.1215, Train Acc: 0.9474, Val Loss: 0.9412, Val Acc: 0.8063\n","Epoch: 239, Train Loss: 0.1200, Train Acc: 0.9486, Val Loss: 0.8920, Val Acc: 0.8088\n","Epoch: 240, Train Loss: 0.1213, Train Acc: 0.9478, Val Loss: 0.9462, Val Acc: 0.8064\n","Epoch: 241, Train Loss: 0.1206, Train Acc: 0.9488, Val Loss: 0.9392, Val Acc: 0.8058\n","Epoch: 242, Train Loss: 0.1229, Train Acc: 0.9469, Val Loss: 0.9560, Val Acc: 0.8063\n","Epoch: 243, Train Loss: 0.1212, Train Acc: 0.9478, Val Loss: 0.9447, Val Acc: 0.8062\n","Epoch: 244, Train Loss: 0.1200, Train Acc: 0.9483, Val Loss: 0.9370, Val Acc: 0.8055\n","Epoch: 245, Train Loss: 0.1225, Train Acc: 0.9469, Val Loss: 0.9410, Val Acc: 0.8073\n","Epoch: 246, Train Loss: 0.1207, Train Acc: 0.9470, Val Loss: 0.9341, Val Acc: 0.8038\n","Epoch: 247, Train Loss: 0.1199, Train Acc: 0.9489, Val Loss: 0.9440, Val Acc: 0.8060\n","Epoch: 248, Train Loss: 0.1220, Train Acc: 0.9473, Val Loss: 0.9484, Val Acc: 0.8048\n","Epoch: 249, Train Loss: 0.1199, Train Acc: 0.9489, Val Loss: 0.9237, Val Acc: 0.8064\n","Epoch: 250, Train Loss: 0.1205, Train Acc: 0.9477, Val Loss: 0.9579, Val Acc: 0.8038\n","Epoch: 251, Train Loss: 0.1205, Train Acc: 0.9482, Val Loss: 0.9585, Val Acc: 0.8056\n","Epoch: 252, Train Loss: 0.1172, Train Acc: 0.9485, Val Loss: 0.9757, Val Acc: 0.8065\n","Epoch: 253, Train Loss: 0.1191, Train Acc: 0.9479, Val Loss: 1.0040, Val Acc: 0.8047\n","Epoch: 254, Train Loss: 0.1204, Train Acc: 0.9484, Val Loss: 0.9568, Val Acc: 0.8059\n","Epoch: 255, Train Loss: 0.1200, Train Acc: 0.9487, Val Loss: 0.9721, Val Acc: 0.8021\n","Epoch: 256, Train Loss: 0.1177, Train Acc: 0.9492, Val Loss: 0.9546, Val Acc: 0.8067\n","Epoch: 257, Train Loss: 0.1194, Train Acc: 0.9479, Val Loss: 0.9767, Val Acc: 0.8064\n","Epoch: 258, Train Loss: 0.1196, Train Acc: 0.9489, Val Loss: 0.9525, Val Acc: 0.8069\n","Epoch: 259, Train Loss: 0.1170, Train Acc: 0.9495, Val Loss: 0.9661, Val Acc: 0.8089\n","Epoch: 260, Train Loss: 0.1187, Train Acc: 0.9490, Val Loss: 0.9966, Val Acc: 0.8055\n","Epoch: 261, Train Loss: 0.1192, Train Acc: 0.9484, Val Loss: 0.9861, Val Acc: 0.7998\n","Epoch: 262, Train Loss: 0.1208, Train Acc: 0.9473, Val Loss: 0.9391, Val Acc: 0.8057\n","Epoch: 263, Train Loss: 0.1167, Train Acc: 0.9491, Val Loss: 1.0046, Val Acc: 0.8033\n","Epoch: 264, Train Loss: 0.1152, Train Acc: 0.9497, Val Loss: 0.9556, Val Acc: 0.8041\n","Epoch: 265, Train Loss: 0.1177, Train Acc: 0.9483, Val Loss: 1.0284, Val Acc: 0.8059\n","Epoch: 266, Train Loss: 0.1164, Train Acc: 0.9494, Val Loss: 0.9949, Val Acc: 0.8069\n","Epoch: 267, Train Loss: 0.1176, Train Acc: 0.9487, Val Loss: 0.9558, Val Acc: 0.8059\n","Epoch: 268, Train Loss: 0.1180, Train Acc: 0.9486, Val Loss: 0.9526, Val Acc: 0.8065\n","Epoch: 269, Train Loss: 0.1173, Train Acc: 0.9494, Val Loss: 0.9289, Val Acc: 0.8067\n","Epoch: 270, Train Loss: 0.1158, Train Acc: 0.9491, Val Loss: 0.9559, Val Acc: 0.8077\n","Epoch: 271, Train Loss: 0.1176, Train Acc: 0.9486, Val Loss: 0.9729, Val Acc: 0.8064\n","Epoch: 272, Train Loss: 0.1162, Train Acc: 0.9488, Val Loss: 0.9982, Val Acc: 0.8037\n","Epoch: 273, Train Loss: 0.1151, Train Acc: 0.9496, Val Loss: 0.9988, Val Acc: 0.8074\n","Epoch: 274, Train Loss: 0.1165, Train Acc: 0.9496, Val Loss: 0.9211, Val Acc: 0.8038\n","Epoch: 275, Train Loss: 0.1155, Train Acc: 0.9498, Val Loss: 0.9945, Val Acc: 0.8043\n","Epoch: 276, Train Loss: 0.1177, Train Acc: 0.9488, Val Loss: 0.9990, Val Acc: 0.8060\n","Epoch: 277, Train Loss: 0.1156, Train Acc: 0.9498, Val Loss: 0.9932, Val Acc: 0.8036\n","Epoch: 278, Train Loss: 0.1154, Train Acc: 0.9496, Val Loss: 0.9837, Val Acc: 0.8023\n","Epoch: 279, Train Loss: 0.1175, Train Acc: 0.9489, Val Loss: 0.9766, Val Acc: 0.8028\n","Epoch: 280, Train Loss: 0.1149, Train Acc: 0.9497, Val Loss: 0.9859, Val Acc: 0.8050\n","Epoch: 281, Train Loss: 0.1155, Train Acc: 0.9505, Val Loss: 0.9525, Val Acc: 0.8056\n","Epoch: 282, Train Loss: 0.1149, Train Acc: 0.9504, Val Loss: 0.9450, Val Acc: 0.8038\n","Epoch: 283, Train Loss: 0.1144, Train Acc: 0.9503, Val Loss: 1.0081, Val Acc: 0.8043\n","Epoch: 284, Train Loss: 0.1153, Train Acc: 0.9496, Val Loss: 0.9974, Val Acc: 0.8050\n","Epoch: 285, Train Loss: 0.1142, Train Acc: 0.9499, Val Loss: 0.9722, Val Acc: 0.8061\n","Epoch: 286, Train Loss: 0.1155, Train Acc: 0.9504, Val Loss: 0.9436, Val Acc: 0.8041\n","Epoch: 287, Train Loss: 0.1125, Train Acc: 0.9513, Val Loss: 0.9864, Val Acc: 0.8047\n","Epoch: 288, Train Loss: 0.1173, Train Acc: 0.9494, Val Loss: 0.9525, Val Acc: 0.8056\n","Epoch: 289, Train Loss: 0.1134, Train Acc: 0.9512, Val Loss: 0.9448, Val Acc: 0.8028\n","Epoch: 290, Train Loss: 0.1140, Train Acc: 0.9500, Val Loss: 0.9615, Val Acc: 0.8097\n","Epoch: 291, Train Loss: 0.1146, Train Acc: 0.9504, Val Loss: 0.9469, Val Acc: 0.8047\n","Epoch: 292, Train Loss: 0.1136, Train Acc: 0.9509, Val Loss: 0.9539, Val Acc: 0.8085\n","Epoch: 293, Train Loss: 0.1129, Train Acc: 0.9509, Val Loss: 0.9563, Val Acc: 0.8061\n","Epoch: 294, Train Loss: 0.1139, Train Acc: 0.9501, Val Loss: 0.9964, Val Acc: 0.8046\n","Epoch: 295, Train Loss: 0.1142, Train Acc: 0.9510, Val Loss: 0.9388, Val Acc: 0.8055\n","Epoch: 296, Train Loss: 0.1144, Train Acc: 0.9501, Val Loss: 0.9718, Val Acc: 0.8049\n","Epoch: 297, Train Loss: 0.1133, Train Acc: 0.9508, Val Loss: 0.9669, Val Acc: 0.8047\n","Epoch: 298, Train Loss: 0.1133, Train Acc: 0.9505, Val Loss: 0.9879, Val Acc: 0.8071\n","Epoch: 299, Train Loss: 0.1129, Train Acc: 0.9507, Val Loss: 0.9957, Val Acc: 0.8075\n","Epoch: 300, Train Loss: 0.1138, Train Acc: 0.9503, Val Loss: 0.9688, Val Acc: 0.8051\n","Epoch: 301, Train Loss: 0.1140, Train Acc: 0.9503, Val Loss: 0.9549, Val Acc: 0.8055\n","Epoch: 302, Train Loss: 0.1127, Train Acc: 0.9508, Val Loss: 0.9816, Val Acc: 0.8047\n","Epoch: 303, Train Loss: 0.1133, Train Acc: 0.9508, Val Loss: 0.9759, Val Acc: 0.8065\n","Epoch: 304, Train Loss: 0.1141, Train Acc: 0.9503, Val Loss: 0.9403, Val Acc: 0.8072\n","Epoch: 305, Train Loss: 0.1119, Train Acc: 0.9510, Val Loss: 0.9550, Val Acc: 0.8087\n","Epoch: 306, Train Loss: 0.1137, Train Acc: 0.9507, Val Loss: 0.9391, Val Acc: 0.8079\n","Epoch: 307, Train Loss: 0.1112, Train Acc: 0.9516, Val Loss: 0.9934, Val Acc: 0.8034\n","Epoch: 308, Train Loss: 0.1106, Train Acc: 0.9520, Val Loss: 0.9803, Val Acc: 0.8087\n","Epoch: 309, Train Loss: 0.1122, Train Acc: 0.9506, Val Loss: 0.9782, Val Acc: 0.8034\n","Epoch: 310, Train Loss: 0.1138, Train Acc: 0.9508, Val Loss: 0.9735, Val Acc: 0.8057\n","Epoch: 311, Train Loss: 0.1123, Train Acc: 0.9505, Val Loss: 0.9763, Val Acc: 0.8054\n","Epoch: 312, Train Loss: 0.1106, Train Acc: 0.9515, Val Loss: 0.9776, Val Acc: 0.8046\n","Epoch: 313, Train Loss: 0.1115, Train Acc: 0.9508, Val Loss: 1.0046, Val Acc: 0.8028\n","Epoch: 314, Train Loss: 0.1138, Train Acc: 0.9501, Val Loss: 0.9614, Val Acc: 0.8047\n","Epoch: 315, Train Loss: 0.1086, Train Acc: 0.9515, Val Loss: 0.9546, Val Acc: 0.8067\n","Epoch: 316, Train Loss: 0.1109, Train Acc: 0.9510, Val Loss: 0.9598, Val Acc: 0.8025\n","Epoch: 317, Train Loss: 0.1111, Train Acc: 0.9515, Val Loss: 1.0303, Val Acc: 0.8036\n","Epoch: 318, Train Loss: 0.1111, Train Acc: 0.9514, Val Loss: 0.9757, Val Acc: 0.8063\n","Epoch: 319, Train Loss: 0.1128, Train Acc: 0.9502, Val Loss: 0.9967, Val Acc: 0.8038\n","Epoch: 320, Train Loss: 0.1104, Train Acc: 0.9512, Val Loss: 0.9576, Val Acc: 0.8073\n","Epoch: 321, Train Loss: 0.1147, Train Acc: 0.9501, Val Loss: 0.9378, Val Acc: 0.8077\n","Epoch: 322, Train Loss: 0.1094, Train Acc: 0.9518, Val Loss: 0.9813, Val Acc: 0.8078\n","Epoch: 323, Train Loss: 0.1129, Train Acc: 0.9504, Val Loss: 0.9514, Val Acc: 0.8055\n","Epoch: 324, Train Loss: 0.1108, Train Acc: 0.9512, Val Loss: 0.9630, Val Acc: 0.8079\n","Epoch: 325, Train Loss: 0.1084, Train Acc: 0.9517, Val Loss: 0.9771, Val Acc: 0.8081\n","Epoch: 326, Train Loss: 0.1106, Train Acc: 0.9514, Val Loss: 0.9604, Val Acc: 0.8045\n","Epoch: 327, Train Loss: 0.1099, Train Acc: 0.9526, Val Loss: 0.9615, Val Acc: 0.8055\n","Epoch: 328, Train Loss: 0.1100, Train Acc: 0.9512, Val Loss: 0.9741, Val Acc: 0.8065\n","Epoch: 329, Train Loss: 0.1093, Train Acc: 0.9522, Val Loss: 0.9778, Val Acc: 0.8071\n","Epoch: 330, Train Loss: 0.1117, Train Acc: 0.9511, Val Loss: 0.9753, Val Acc: 0.8077\n","Epoch: 331, Train Loss: 0.1105, Train Acc: 0.9516, Val Loss: 0.9972, Val Acc: 0.8066\n","Epoch: 332, Train Loss: 0.1105, Train Acc: 0.9513, Val Loss: 0.9395, Val Acc: 0.8043\n","Epoch: 333, Train Loss: 0.1107, Train Acc: 0.9507, Val Loss: 0.9946, Val Acc: 0.8084\n","Epoch: 334, Train Loss: 0.1113, Train Acc: 0.9507, Val Loss: 0.9362, Val Acc: 0.8074\n","Epoch: 335, Train Loss: 0.1080, Train Acc: 0.9521, Val Loss: 0.9657, Val Acc: 0.8076\n","Epoch: 336, Train Loss: 0.1083, Train Acc: 0.9515, Val Loss: 0.9797, Val Acc: 0.8079\n","Epoch: 337, Train Loss: 0.1088, Train Acc: 0.9518, Val Loss: 0.9775, Val Acc: 0.8057\n","Epoch: 338, Train Loss: 0.1092, Train Acc: 0.9519, Val Loss: 0.9766, Val Acc: 0.8073\n","Epoch: 339, Train Loss: 0.1094, Train Acc: 0.9521, Val Loss: 0.9496, Val Acc: 0.8075\n","Epoch: 340, Train Loss: 0.1092, Train Acc: 0.9517, Val Loss: 0.9942, Val Acc: 0.8093\n","Epoch: 341, Train Loss: 0.1084, Train Acc: 0.9522, Val Loss: 1.0090, Val Acc: 0.8092\n","Epoch: 342, Train Loss: 0.1096, Train Acc: 0.9511, Val Loss: 0.9832, Val Acc: 0.8087\n","Epoch: 343, Train Loss: 0.1091, Train Acc: 0.9514, Val Loss: 0.9699, Val Acc: 0.8061\n","Epoch: 344, Train Loss: 0.1085, Train Acc: 0.9515, Val Loss: 0.9924, Val Acc: 0.8083\n","Epoch: 345, Train Loss: 0.1094, Train Acc: 0.9521, Val Loss: 0.9886, Val Acc: 0.8072\n","Epoch: 346, Train Loss: 0.1079, Train Acc: 0.9530, Val Loss: 1.0329, Val Acc: 0.8066\n","Epoch: 347, Train Loss: 0.1085, Train Acc: 0.9513, Val Loss: 0.9939, Val Acc: 0.8084\n","Epoch: 348, Train Loss: 0.1072, Train Acc: 0.9526, Val Loss: 1.0061, Val Acc: 0.8077\n","Epoch: 349, Train Loss: 0.1084, Train Acc: 0.9521, Val Loss: 0.9769, Val Acc: 0.8090\n","Epoch: 350, Train Loss: 0.1097, Train Acc: 0.9520, Val Loss: 1.0097, Val Acc: 0.8064\n","Epoch: 351, Train Loss: 0.1070, Train Acc: 0.9528, Val Loss: 1.0218, Val Acc: 0.8010\n","Epoch: 352, Train Loss: 0.1088, Train Acc: 0.9525, Val Loss: 0.9860, Val Acc: 0.8073\n","Epoch: 353, Train Loss: 0.1101, Train Acc: 0.9515, Val Loss: 0.9776, Val Acc: 0.8065\n","Epoch: 354, Train Loss: 0.1077, Train Acc: 0.9524, Val Loss: 0.9928, Val Acc: 0.8075\n","Epoch: 355, Train Loss: 0.1071, Train Acc: 0.9525, Val Loss: 0.9409, Val Acc: 0.8101\n","Epoch: 356, Train Loss: 0.1086, Train Acc: 0.9519, Val Loss: 0.9598, Val Acc: 0.8086\n","Epoch: 357, Train Loss: 0.1059, Train Acc: 0.9531, Val Loss: 1.0233, Val Acc: 0.8100\n","Epoch: 358, Train Loss: 0.1114, Train Acc: 0.9519, Val Loss: 0.9513, Val Acc: 0.8102\n","Epoch: 359, Train Loss: 0.1064, Train Acc: 0.9529, Val Loss: 1.0219, Val Acc: 0.8038\n","Epoch: 360, Train Loss: 0.1065, Train Acc: 0.9530, Val Loss: 1.0001, Val Acc: 0.8087\n","Epoch: 361, Train Loss: 0.1067, Train Acc: 0.9527, Val Loss: 1.0250, Val Acc: 0.8060\n","Epoch: 362, Train Loss: 0.1075, Train Acc: 0.9523, Val Loss: 1.0405, Val Acc: 0.8072\n","Epoch: 363, Train Loss: 0.1068, Train Acc: 0.9515, Val Loss: 1.0284, Val Acc: 0.8053\n","Epoch: 364, Train Loss: 0.1074, Train Acc: 0.9520, Val Loss: 0.9350, Val Acc: 0.8102\n","Epoch: 365, Train Loss: 0.1064, Train Acc: 0.9525, Val Loss: 1.0091, Val Acc: 0.8082\n","Epoch: 366, Train Loss: 0.1063, Train Acc: 0.9532, Val Loss: 0.9549, Val Acc: 0.8069\n","Epoch: 367, Train Loss: 0.1073, Train Acc: 0.9529, Val Loss: 0.9428, Val Acc: 0.8087\n","Epoch: 368, Train Loss: 0.1087, Train Acc: 0.9525, Val Loss: 0.9442, Val Acc: 0.8123\n","Epoch: 369, Train Loss: 0.1061, Train Acc: 0.9532, Val Loss: 0.9695, Val Acc: 0.8093\n","Epoch: 370, Train Loss: 0.1075, Train Acc: 0.9527, Val Loss: 0.9645, Val Acc: 0.8093\n","Epoch: 371, Train Loss: 0.1073, Train Acc: 0.9522, Val Loss: 1.0260, Val Acc: 0.8049\n","Epoch: 372, Train Loss: 0.1073, Train Acc: 0.9529, Val Loss: 0.9813, Val Acc: 0.8099\n","Epoch: 373, Train Loss: 0.1064, Train Acc: 0.9524, Val Loss: 0.9890, Val Acc: 0.8075\n","Epoch: 374, Train Loss: 0.1054, Train Acc: 0.9533, Val Loss: 1.0184, Val Acc: 0.8076\n","Epoch: 375, Train Loss: 0.1074, Train Acc: 0.9523, Val Loss: 0.9598, Val Acc: 0.8088\n","Epoch: 376, Train Loss: 0.1065, Train Acc: 0.9522, Val Loss: 0.9731, Val Acc: 0.8093\n","Epoch: 377, Train Loss: 0.1055, Train Acc: 0.9534, Val Loss: 0.9808, Val Acc: 0.8085\n","Epoch: 378, Train Loss: 0.1053, Train Acc: 0.9528, Val Loss: 1.0600, Val Acc: 0.8061\n","Epoch: 379, Train Loss: 0.1089, Train Acc: 0.9526, Val Loss: 1.0385, Val Acc: 0.8047\n","Epoch: 380, Train Loss: 0.1055, Train Acc: 0.9534, Val Loss: 0.9811, Val Acc: 0.8072\n","Epoch: 381, Train Loss: 0.1065, Train Acc: 0.9536, Val Loss: 0.9894, Val Acc: 0.8068\n","Epoch: 382, Train Loss: 0.1060, Train Acc: 0.9524, Val Loss: 0.9840, Val Acc: 0.8067\n","Epoch: 383, Train Loss: 0.1066, Train Acc: 0.9528, Val Loss: 1.0494, Val Acc: 0.8024\n","Epoch: 384, Train Loss: 0.1043, Train Acc: 0.9539, Val Loss: 0.9888, Val Acc: 0.8109\n","Epoch: 385, Train Loss: 0.1070, Train Acc: 0.9527, Val Loss: 1.0122, Val Acc: 0.8034\n","Epoch: 386, Train Loss: 0.1053, Train Acc: 0.9537, Val Loss: 1.0210, Val Acc: 0.8059\n","Epoch: 387, Train Loss: 0.1049, Train Acc: 0.9533, Val Loss: 0.9425, Val Acc: 0.8104\n","Epoch: 388, Train Loss: 0.1047, Train Acc: 0.9534, Val Loss: 1.0568, Val Acc: 0.8067\n","Epoch: 389, Train Loss: 0.1050, Train Acc: 0.9535, Val Loss: 0.9883, Val Acc: 0.8098\n","Epoch: 390, Train Loss: 0.1053, Train Acc: 0.9535, Val Loss: 1.0113, Val Acc: 0.8085\n","Epoch: 391, Train Loss: 0.1041, Train Acc: 0.9542, Val Loss: 0.9849, Val Acc: 0.8081\n","Epoch: 392, Train Loss: 0.1052, Train Acc: 0.9530, Val Loss: 0.9818, Val Acc: 0.8073\n","Epoch: 393, Train Loss: 0.1065, Train Acc: 0.9527, Val Loss: 1.0183, Val Acc: 0.8045\n","Epoch: 394, Train Loss: 0.1052, Train Acc: 0.9533, Val Loss: 1.0151, Val Acc: 0.8088\n","Epoch: 395, Train Loss: 0.1047, Train Acc: 0.9531, Val Loss: 0.9802, Val Acc: 0.8081\n","Epoch: 396, Train Loss: 0.1055, Train Acc: 0.9538, Val Loss: 0.9904, Val Acc: 0.8121\n","Epoch: 397, Train Loss: 0.1062, Train Acc: 0.9528, Val Loss: 0.9943, Val Acc: 0.8102\n","Epoch: 398, Train Loss: 0.1021, Train Acc: 0.9548, Val Loss: 0.9964, Val Acc: 0.8072\n","Epoch: 399, Train Loss: 0.1038, Train Acc: 0.9536, Val Loss: 1.0027, Val Acc: 0.8111\n","Epoch: 400, Train Loss: 0.1038, Train Acc: 0.9539, Val Loss: 0.9981, Val Acc: 0.8091\n","Epoch: 401, Train Loss: 0.1018, Train Acc: 0.9546, Val Loss: 1.0132, Val Acc: 0.8082\n","Epoch: 402, Train Loss: 0.1030, Train Acc: 0.9541, Val Loss: 0.9888, Val Acc: 0.8075\n","Epoch: 403, Train Loss: 0.1047, Train Acc: 0.9531, Val Loss: 1.0242, Val Acc: 0.8083\n","Epoch: 404, Train Loss: 0.1044, Train Acc: 0.9533, Val Loss: 1.0131, Val Acc: 0.8053\n","Epoch: 405, Train Loss: 0.1062, Train Acc: 0.9529, Val Loss: 1.0181, Val Acc: 0.8067\n","Epoch: 406, Train Loss: 0.1037, Train Acc: 0.9545, Val Loss: 1.0001, Val Acc: 0.8074\n","Epoch: 407, Train Loss: 0.1030, Train Acc: 0.9535, Val Loss: 1.0120, Val Acc: 0.8057\n","Epoch: 408, Train Loss: 0.1039, Train Acc: 0.9539, Val Loss: 0.9977, Val Acc: 0.8069\n","Epoch: 409, Train Loss: 0.1069, Train Acc: 0.9527, Val Loss: 1.0285, Val Acc: 0.8065\n","Epoch: 410, Train Loss: 0.1043, Train Acc: 0.9538, Val Loss: 1.0105, Val Acc: 0.8074\n","Epoch: 411, Train Loss: 0.1025, Train Acc: 0.9541, Val Loss: 0.9843, Val Acc: 0.8077\n","Epoch: 412, Train Loss: 0.1044, Train Acc: 0.9532, Val Loss: 1.0017, Val Acc: 0.8072\n","Epoch: 413, Train Loss: 0.1031, Train Acc: 0.9538, Val Loss: 1.0262, Val Acc: 0.8073\n","Epoch: 414, Train Loss: 0.1032, Train Acc: 0.9541, Val Loss: 0.9997, Val Acc: 0.8078\n","Epoch: 415, Train Loss: 0.1038, Train Acc: 0.9538, Val Loss: 1.0152, Val Acc: 0.8059\n","Epoch: 416, Train Loss: 0.1056, Train Acc: 0.9533, Val Loss: 1.0283, Val Acc: 0.8078\n","Epoch: 417, Train Loss: 0.1042, Train Acc: 0.9534, Val Loss: 0.9952, Val Acc: 0.8065\n","Epoch: 418, Train Loss: 0.1028, Train Acc: 0.9537, Val Loss: 1.0566, Val Acc: 0.8066\n","Epoch: 419, Train Loss: 0.1039, Train Acc: 0.9538, Val Loss: 1.0095, Val Acc: 0.8050\n","Epoch: 420, Train Loss: 0.1031, Train Acc: 0.9544, Val Loss: 1.0289, Val Acc: 0.8089\n","Epoch: 421, Train Loss: 0.1036, Train Acc: 0.9541, Val Loss: 1.0365, Val Acc: 0.8073\n","Epoch: 422, Train Loss: 0.1051, Train Acc: 0.9530, Val Loss: 0.9861, Val Acc: 0.8081\n","Epoch: 423, Train Loss: 0.1035, Train Acc: 0.9538, Val Loss: 0.9854, Val Acc: 0.8070\n","Epoch: 424, Train Loss: 0.1017, Train Acc: 0.9549, Val Loss: 1.0236, Val Acc: 0.8071\n","Epoch: 425, Train Loss: 0.1041, Train Acc: 0.9536, Val Loss: 0.9855, Val Acc: 0.8072\n","Epoch: 426, Train Loss: 0.1035, Train Acc: 0.9541, Val Loss: 1.0266, Val Acc: 0.8081\n","Epoch: 427, Train Loss: 0.0998, Train Acc: 0.9549, Val Loss: 1.0337, Val Acc: 0.8079\n","Epoch: 428, Train Loss: 0.1052, Train Acc: 0.9533, Val Loss: 1.0287, Val Acc: 0.8074\n","Epoch: 429, Train Loss: 0.1028, Train Acc: 0.9546, Val Loss: 1.0295, Val Acc: 0.8063\n","Epoch: 430, Train Loss: 0.1033, Train Acc: 0.9541, Val Loss: 1.0115, Val Acc: 0.8051\n","Epoch: 431, Train Loss: 0.1026, Train Acc: 0.9547, Val Loss: 1.0366, Val Acc: 0.8092\n","Epoch: 432, Train Loss: 0.1038, Train Acc: 0.9539, Val Loss: 1.0471, Val Acc: 0.8060\n","Epoch: 433, Train Loss: 0.1027, Train Acc: 0.9541, Val Loss: 1.0102, Val Acc: 0.8068\n","Epoch: 434, Train Loss: 0.1029, Train Acc: 0.9539, Val Loss: 1.0141, Val Acc: 0.8051\n","Epoch: 435, Train Loss: 0.1014, Train Acc: 0.9546, Val Loss: 0.9782, Val Acc: 0.8068\n","Epoch: 436, Train Loss: 0.1030, Train Acc: 0.9541, Val Loss: 1.0356, Val Acc: 0.8089\n","Epoch: 437, Train Loss: 0.1005, Train Acc: 0.9549, Val Loss: 1.0091, Val Acc: 0.8092\n","Epoch: 438, Train Loss: 0.1029, Train Acc: 0.9541, Val Loss: 1.0040, Val Acc: 0.8076\n","Epoch: 439, Train Loss: 0.1026, Train Acc: 0.9542, Val Loss: 1.0011, Val Acc: 0.8078\n","Epoch: 440, Train Loss: 0.1029, Train Acc: 0.9543, Val Loss: 1.0292, Val Acc: 0.8060\n","Epoch: 441, Train Loss: 0.1025, Train Acc: 0.9542, Val Loss: 1.0293, Val Acc: 0.8049\n","Epoch: 442, Train Loss: 0.1022, Train Acc: 0.9545, Val Loss: 1.0043, Val Acc: 0.8073\n","Epoch: 443, Train Loss: 0.1031, Train Acc: 0.9541, Val Loss: 1.0273, Val Acc: 0.8059\n","Epoch: 444, Train Loss: 0.1023, Train Acc: 0.9538, Val Loss: 0.9875, Val Acc: 0.8045\n","Epoch: 445, Train Loss: 0.1001, Train Acc: 0.9544, Val Loss: 1.0032, Val Acc: 0.8024\n","Epoch: 446, Train Loss: 0.1009, Train Acc: 0.9544, Val Loss: 1.0150, Val Acc: 0.8085\n","Epoch: 447, Train Loss: 0.1038, Train Acc: 0.9538, Val Loss: 0.9748, Val Acc: 0.8083\n","Epoch: 448, Train Loss: 0.1022, Train Acc: 0.9539, Val Loss: 1.0611, Val Acc: 0.8062\n","Epoch: 449, Train Loss: 0.1027, Train Acc: 0.9537, Val Loss: 0.9879, Val Acc: 0.8063\n","Epoch: 450, Train Loss: 0.1009, Train Acc: 0.9545, Val Loss: 1.0394, Val Acc: 0.8051\n","Epoch: 451, Train Loss: 0.1022, Train Acc: 0.9542, Val Loss: 1.0210, Val Acc: 0.8055\n","Epoch: 452, Train Loss: 0.1014, Train Acc: 0.9544, Val Loss: 0.9927, Val Acc: 0.8077\n","Epoch: 453, Train Loss: 0.1022, Train Acc: 0.9541, Val Loss: 1.0021, Val Acc: 0.8053\n","Epoch: 454, Train Loss: 0.1015, Train Acc: 0.9553, Val Loss: 0.9871, Val Acc: 0.8089\n","Epoch: 455, Train Loss: 0.1018, Train Acc: 0.9537, Val Loss: 0.9963, Val Acc: 0.8073\n","Epoch: 456, Train Loss: 0.1006, Train Acc: 0.9550, Val Loss: 1.0305, Val Acc: 0.8075\n","Epoch: 457, Train Loss: 0.1020, Train Acc: 0.9548, Val Loss: 1.0642, Val Acc: 0.8054\n","Epoch: 458, Train Loss: 0.1013, Train Acc: 0.9545, Val Loss: 1.0122, Val Acc: 0.8071\n","Epoch: 459, Train Loss: 0.1014, Train Acc: 0.9546, Val Loss: 0.9984, Val Acc: 0.8085\n","Epoch: 460, Train Loss: 0.1013, Train Acc: 0.9546, Val Loss: 1.0248, Val Acc: 0.8086\n","Epoch: 461, Train Loss: 0.1014, Train Acc: 0.9542, Val Loss: 1.0374, Val Acc: 0.8070\n","Epoch: 462, Train Loss: 0.0994, Train Acc: 0.9552, Val Loss: 1.0306, Val Acc: 0.8043\n","Epoch: 463, Train Loss: 0.1003, Train Acc: 0.9547, Val Loss: 1.0060, Val Acc: 0.8084\n","Epoch: 464, Train Loss: 0.1005, Train Acc: 0.9545, Val Loss: 1.0384, Val Acc: 0.8093\n","Epoch: 465, Train Loss: 0.1024, Train Acc: 0.9536, Val Loss: 1.0040, Val Acc: 0.8081\n","Epoch: 466, Train Loss: 0.1016, Train Acc: 0.9546, Val Loss: 1.0214, Val Acc: 0.8079\n","Epoch: 467, Train Loss: 0.1011, Train Acc: 0.9544, Val Loss: 0.9880, Val Acc: 0.8075\n","Epoch: 468, Train Loss: 0.1020, Train Acc: 0.9544, Val Loss: 1.0559, Val Acc: 0.8057\n","Epoch: 469, Train Loss: 0.1020, Train Acc: 0.9548, Val Loss: 1.0534, Val Acc: 0.8038\n","Epoch: 470, Train Loss: 0.1002, Train Acc: 0.9547, Val Loss: 1.0389, Val Acc: 0.8085\n","Epoch: 471, Train Loss: 0.1007, Train Acc: 0.9544, Val Loss: 1.0275, Val Acc: 0.8046\n","Epoch: 472, Train Loss: 0.1023, Train Acc: 0.9544, Val Loss: 1.0070, Val Acc: 0.8086\n","Epoch: 473, Train Loss: 0.0986, Train Acc: 0.9554, Val Loss: 1.0280, Val Acc: 0.8077\n","Epoch: 474, Train Loss: 0.0993, Train Acc: 0.9552, Val Loss: 0.9726, Val Acc: 0.8073\n","Epoch: 475, Train Loss: 0.1008, Train Acc: 0.9550, Val Loss: 1.0025, Val Acc: 0.8086\n","Epoch: 476, Train Loss: 0.1020, Train Acc: 0.9547, Val Loss: 1.0300, Val Acc: 0.8079\n","Epoch: 477, Train Loss: 0.1001, Train Acc: 0.9550, Val Loss: 1.0483, Val Acc: 0.8077\n","Epoch: 478, Train Loss: 0.1009, Train Acc: 0.9542, Val Loss: 1.0025, Val Acc: 0.8083\n","Epoch: 479, Train Loss: 0.1003, Train Acc: 0.9551, Val Loss: 0.9574, Val Acc: 0.8086\n","Epoch: 480, Train Loss: 0.1016, Train Acc: 0.9547, Val Loss: 1.0314, Val Acc: 0.8085\n","Epoch: 481, Train Loss: 0.1002, Train Acc: 0.9549, Val Loss: 1.0453, Val Acc: 0.8061\n","Epoch: 482, Train Loss: 0.1008, Train Acc: 0.9541, Val Loss: 1.0416, Val Acc: 0.8074\n","Epoch: 483, Train Loss: 0.1010, Train Acc: 0.9552, Val Loss: 1.0325, Val Acc: 0.8068\n","Epoch: 484, Train Loss: 0.1004, Train Acc: 0.9553, Val Loss: 1.0415, Val Acc: 0.8113\n","Epoch: 485, Train Loss: 0.1008, Train Acc: 0.9542, Val Loss: 1.0499, Val Acc: 0.8083\n","Epoch: 486, Train Loss: 0.0999, Train Acc: 0.9543, Val Loss: 1.0168, Val Acc: 0.8071\n","Epoch: 487, Train Loss: 0.0996, Train Acc: 0.9553, Val Loss: 1.0123, Val Acc: 0.8085\n","Epoch: 488, Train Loss: 0.1000, Train Acc: 0.9550, Val Loss: 1.0510, Val Acc: 0.8073\n","Epoch: 489, Train Loss: 0.1013, Train Acc: 0.9551, Val Loss: 1.0458, Val Acc: 0.8083\n","Epoch: 490, Train Loss: 0.1008, Train Acc: 0.9545, Val Loss: 0.9961, Val Acc: 0.8067\n","Epoch: 491, Train Loss: 0.0995, Train Acc: 0.9556, Val Loss: 1.0189, Val Acc: 0.8075\n","Epoch: 492, Train Loss: 0.0994, Train Acc: 0.9555, Val Loss: 1.0450, Val Acc: 0.8095\n","Epoch: 493, Train Loss: 0.0994, Train Acc: 0.9556, Val Loss: 1.0269, Val Acc: 0.8092\n","Epoch: 494, Train Loss: 0.1001, Train Acc: 0.9550, Val Loss: 0.9892, Val Acc: 0.8087\n","Epoch: 495, Train Loss: 0.0987, Train Acc: 0.9553, Val Loss: 1.0417, Val Acc: 0.8089\n","Epoch: 496, Train Loss: 0.1023, Train Acc: 0.9540, Val Loss: 1.0082, Val Acc: 0.8097\n","Epoch: 497, Train Loss: 0.1010, Train Acc: 0.9545, Val Loss: 1.0274, Val Acc: 0.8100\n","Epoch: 498, Train Loss: 0.1005, Train Acc: 0.9545, Val Loss: 1.0041, Val Acc: 0.8078\n","Epoch: 499, Train Loss: 0.1007, Train Acc: 0.9552, Val Loss: 1.0153, Val Acc: 0.8093\n","Epoch: 500, Train Loss: 0.0976, Train Acc: 0.9555, Val Loss: 1.0220, Val Acc: 0.8093\n","Epoch: 501, Train Loss: 0.0995, Train Acc: 0.9554, Val Loss: 1.0002, Val Acc: 0.8072\n","Epoch: 502, Train Loss: 0.1001, Train Acc: 0.9550, Val Loss: 0.9788, Val Acc: 0.8098\n","Epoch: 503, Train Loss: 0.1001, Train Acc: 0.9543, Val Loss: 0.9381, Val Acc: 0.8104\n","Epoch: 504, Train Loss: 0.0979, Train Acc: 0.9559, Val Loss: 1.0287, Val Acc: 0.8110\n","Epoch: 505, Train Loss: 0.0980, Train Acc: 0.9556, Val Loss: 1.0305, Val Acc: 0.8092\n","Epoch: 506, Train Loss: 0.0996, Train Acc: 0.9547, Val Loss: 1.0144, Val Acc: 0.8085\n","Epoch: 507, Train Loss: 0.0983, Train Acc: 0.9556, Val Loss: 1.0119, Val Acc: 0.8057\n","Epoch: 508, Train Loss: 0.0999, Train Acc: 0.9545, Val Loss: 1.0062, Val Acc: 0.8076\n","Epoch: 509, Train Loss: 0.0983, Train Acc: 0.9560, Val Loss: 1.0443, Val Acc: 0.8085\n","Epoch: 510, Train Loss: 0.0991, Train Acc: 0.9555, Val Loss: 1.0495, Val Acc: 0.8053\n","Epoch: 511, Train Loss: 0.0998, Train Acc: 0.9546, Val Loss: 1.0188, Val Acc: 0.8092\n","Epoch: 512, Train Loss: 0.0998, Train Acc: 0.9547, Val Loss: 1.0029, Val Acc: 0.8055\n","Epoch: 513, Train Loss: 0.0990, Train Acc: 0.9552, Val Loss: 1.0232, Val Acc: 0.8077\n","Epoch: 514, Train Loss: 0.0980, Train Acc: 0.9564, Val Loss: 1.0366, Val Acc: 0.8109\n","Epoch: 515, Train Loss: 0.0979, Train Acc: 0.9558, Val Loss: 0.9904, Val Acc: 0.8064\n","Epoch: 516, Train Loss: 0.0987, Train Acc: 0.9559, Val Loss: 1.0318, Val Acc: 0.8054\n","Epoch: 517, Train Loss: 0.0973, Train Acc: 0.9558, Val Loss: 1.0568, Val Acc: 0.8050\n","Epoch: 518, Train Loss: 0.1004, Train Acc: 0.9548, Val Loss: 1.0223, Val Acc: 0.8080\n","Epoch: 519, Train Loss: 0.0986, Train Acc: 0.9558, Val Loss: 0.9891, Val Acc: 0.8102\n","Epoch: 520, Train Loss: 0.0966, Train Acc: 0.9556, Val Loss: 1.0583, Val Acc: 0.8066\n","Epoch: 521, Train Loss: 0.0989, Train Acc: 0.9556, Val Loss: 1.0130, Val Acc: 0.8067\n","Epoch: 522, Train Loss: 0.1004, Train Acc: 0.9550, Val Loss: 1.0190, Val Acc: 0.8060\n","Epoch: 523, Train Loss: 0.0976, Train Acc: 0.9557, Val Loss: 1.0141, Val Acc: 0.8066\n","Epoch: 524, Train Loss: 0.0975, Train Acc: 0.9557, Val Loss: 1.0800, Val Acc: 0.8071\n","Epoch: 525, Train Loss: 0.0988, Train Acc: 0.9554, Val Loss: 1.0146, Val Acc: 0.8101\n","Epoch: 526, Train Loss: 0.0983, Train Acc: 0.9558, Val Loss: 1.0503, Val Acc: 0.8057\n","Epoch: 527, Train Loss: 0.0979, Train Acc: 0.9557, Val Loss: 1.0313, Val Acc: 0.8056\n","Epoch: 528, Train Loss: 0.0971, Train Acc: 0.9559, Val Loss: 1.0322, Val Acc: 0.8075\n","Epoch: 529, Train Loss: 0.0964, Train Acc: 0.9558, Val Loss: 1.0550, Val Acc: 0.8079\n","Epoch: 530, Train Loss: 0.0997, Train Acc: 0.9558, Val Loss: 1.0132, Val Acc: 0.8073\n","Epoch: 531, Train Loss: 0.1001, Train Acc: 0.9552, Val Loss: 1.0098, Val Acc: 0.8084\n","Epoch: 532, Train Loss: 0.0964, Train Acc: 0.9564, Val Loss: 1.0149, Val Acc: 0.8099\n","Epoch: 533, Train Loss: 0.0975, Train Acc: 0.9554, Val Loss: 1.0090, Val Acc: 0.8109\n","Epoch: 534, Train Loss: 0.1000, Train Acc: 0.9547, Val Loss: 1.0049, Val Acc: 0.8049\n","Epoch: 535, Train Loss: 0.0974, Train Acc: 0.9560, Val Loss: 0.9761, Val Acc: 0.8087\n","Epoch: 536, Train Loss: 0.0968, Train Acc: 0.9564, Val Loss: 1.0107, Val Acc: 0.8094\n","Epoch: 537, Train Loss: 0.0994, Train Acc: 0.9551, Val Loss: 0.9892, Val Acc: 0.8093\n","Epoch: 538, Train Loss: 0.0986, Train Acc: 0.9556, Val Loss: 1.0219, Val Acc: 0.8077\n","Epoch: 539, Train Loss: 0.0965, Train Acc: 0.9559, Val Loss: 1.0033, Val Acc: 0.8093\n","Epoch: 540, Train Loss: 0.0981, Train Acc: 0.9552, Val Loss: 1.0121, Val Acc: 0.8085\n","Epoch: 541, Train Loss: 0.0977, Train Acc: 0.9561, Val Loss: 1.0303, Val Acc: 0.8081\n","Epoch: 542, Train Loss: 0.0967, Train Acc: 0.9559, Val Loss: 1.0259, Val Acc: 0.8111\n","Epoch: 543, Train Loss: 0.0956, Train Acc: 0.9562, Val Loss: 1.0641, Val Acc: 0.8100\n","Epoch: 544, Train Loss: 0.0976, Train Acc: 0.9555, Val Loss: 1.0319, Val Acc: 0.8092\n","Epoch: 545, Train Loss: 0.0961, Train Acc: 0.9566, Val Loss: 1.0774, Val Acc: 0.8095\n","Epoch: 546, Train Loss: 0.0994, Train Acc: 0.9546, Val Loss: 1.0192, Val Acc: 0.8067\n","Epoch: 547, Train Loss: 0.0961, Train Acc: 0.9564, Val Loss: 1.0112, Val Acc: 0.8083\n","Epoch: 548, Train Loss: 0.1000, Train Acc: 0.9558, Val Loss: 1.0337, Val Acc: 0.8040\n","Epoch: 549, Train Loss: 0.0968, Train Acc: 0.9551, Val Loss: 1.0208, Val Acc: 0.8081\n","Epoch: 550, Train Loss: 0.0980, Train Acc: 0.9565, Val Loss: 1.0317, Val Acc: 0.8078\n","Epoch: 551, Train Loss: 0.0974, Train Acc: 0.9556, Val Loss: 1.0201, Val Acc: 0.8087\n","Epoch: 552, Train Loss: 0.0978, Train Acc: 0.9560, Val Loss: 1.0465, Val Acc: 0.8055\n","Epoch: 553, Train Loss: 0.0966, Train Acc: 0.9562, Val Loss: 1.0436, Val Acc: 0.8076\n","Epoch: 554, Train Loss: 0.0985, Train Acc: 0.9559, Val Loss: 1.0386, Val Acc: 0.8069\n","Epoch: 555, Train Loss: 0.0962, Train Acc: 0.9565, Val Loss: 1.0118, Val Acc: 0.8091\n","Epoch: 556, Train Loss: 0.0990, Train Acc: 0.9557, Val Loss: 0.9815, Val Acc: 0.8098\n","Epoch: 557, Train Loss: 0.0968, Train Acc: 0.9555, Val Loss: 1.0451, Val Acc: 0.8082\n","Epoch: 558, Train Loss: 0.0990, Train Acc: 0.9551, Val Loss: 1.0579, Val Acc: 0.8081\n","Epoch: 559, Train Loss: 0.0965, Train Acc: 0.9561, Val Loss: 1.0885, Val Acc: 0.8057\n","Epoch: 560, Train Loss: 0.0994, Train Acc: 0.9550, Val Loss: 1.0179, Val Acc: 0.8072\n","Epoch: 561, Train Loss: 0.0958, Train Acc: 0.9565, Val Loss: 1.0441, Val Acc: 0.8067\n","Epoch: 562, Train Loss: 0.0960, Train Acc: 0.9564, Val Loss: 1.0149, Val Acc: 0.8095\n","Epoch: 563, Train Loss: 0.0969, Train Acc: 0.9569, Val Loss: 1.0633, Val Acc: 0.8060\n","Epoch: 564, Train Loss: 0.0962, Train Acc: 0.9564, Val Loss: 1.0238, Val Acc: 0.8073\n","Epoch: 565, Train Loss: 0.0973, Train Acc: 0.9559, Val Loss: 1.0134, Val Acc: 0.8072\n","Epoch: 566, Train Loss: 0.0968, Train Acc: 0.9558, Val Loss: 1.0226, Val Acc: 0.8069\n","Epoch: 567, Train Loss: 0.0961, Train Acc: 0.9562, Val Loss: 1.0660, Val Acc: 0.8069\n","Epoch: 568, Train Loss: 0.0970, Train Acc: 0.9565, Val Loss: 1.0225, Val Acc: 0.8096\n","Epoch: 569, Train Loss: 0.0979, Train Acc: 0.9556, Val Loss: 1.0299, Val Acc: 0.8071\n","Epoch: 570, Train Loss: 0.0963, Train Acc: 0.9569, Val Loss: 1.0996, Val Acc: 0.8072\n","Epoch: 571, Train Loss: 0.0958, Train Acc: 0.9565, Val Loss: 1.0418, Val Acc: 0.8081\n","Epoch: 572, Train Loss: 0.0974, Train Acc: 0.9562, Val Loss: 1.0546, Val Acc: 0.8094\n","Epoch: 573, Train Loss: 0.0970, Train Acc: 0.9559, Val Loss: 1.0350, Val Acc: 0.8060\n","Epoch: 574, Train Loss: 0.0971, Train Acc: 0.9560, Val Loss: 0.9951, Val Acc: 0.8127\n","Epoch: 575, Train Loss: 0.0957, Train Acc: 0.9566, Val Loss: 1.0183, Val Acc: 0.8088\n","Epoch: 576, Train Loss: 0.0967, Train Acc: 0.9570, Val Loss: 1.0322, Val Acc: 0.8086\n","Epoch: 577, Train Loss: 0.0968, Train Acc: 0.9556, Val Loss: 1.0488, Val Acc: 0.8112\n","Epoch: 578, Train Loss: 0.0955, Train Acc: 0.9567, Val Loss: 1.0109, Val Acc: 0.8087\n","Epoch: 579, Train Loss: 0.0975, Train Acc: 0.9553, Val Loss: 1.0586, Val Acc: 0.8073\n","Epoch: 580, Train Loss: 0.0970, Train Acc: 0.9560, Val Loss: 1.1129, Val Acc: 0.8066\n","Epoch: 581, Train Loss: 0.0968, Train Acc: 0.9567, Val Loss: 1.0991, Val Acc: 0.8092\n","Epoch: 582, Train Loss: 0.0953, Train Acc: 0.9562, Val Loss: 1.0610, Val Acc: 0.8062\n","Epoch: 583, Train Loss: 0.0970, Train Acc: 0.9565, Val Loss: 1.0628, Val Acc: 0.8077\n","Epoch: 584, Train Loss: 0.0969, Train Acc: 0.9559, Val Loss: 1.0616, Val Acc: 0.8070\n","Epoch: 585, Train Loss: 0.0967, Train Acc: 0.9560, Val Loss: 1.0788, Val Acc: 0.8085\n","Epoch: 586, Train Loss: 0.0949, Train Acc: 0.9568, Val Loss: 1.0455, Val Acc: 0.8070\n","Epoch: 587, Train Loss: 0.0966, Train Acc: 0.9562, Val Loss: 1.0563, Val Acc: 0.8091\n","Epoch: 588, Train Loss: 0.0958, Train Acc: 0.9561, Val Loss: 1.0712, Val Acc: 0.8087\n","Epoch: 589, Train Loss: 0.0974, Train Acc: 0.9558, Val Loss: 1.0470, Val Acc: 0.8091\n","Epoch: 590, Train Loss: 0.0953, Train Acc: 0.9566, Val Loss: 1.0538, Val Acc: 0.8076\n","Epoch: 591, Train Loss: 0.0949, Train Acc: 0.9572, Val Loss: 1.0247, Val Acc: 0.8062\n","Epoch: 592, Train Loss: 0.0966, Train Acc: 0.9567, Val Loss: 1.0254, Val Acc: 0.8081\n","Epoch: 593, Train Loss: 0.0958, Train Acc: 0.9565, Val Loss: 1.0431, Val Acc: 0.8069\n","Epoch: 594, Train Loss: 0.0973, Train Acc: 0.9559, Val Loss: 1.0501, Val Acc: 0.8085\n","Epoch: 595, Train Loss: 0.0970, Train Acc: 0.9560, Val Loss: 1.0344, Val Acc: 0.8057\n","Epoch: 596, Train Loss: 0.0969, Train Acc: 0.9561, Val Loss: 1.0286, Val Acc: 0.8067\n","Epoch: 597, Train Loss: 0.0970, Train Acc: 0.9565, Val Loss: 1.0067, Val Acc: 0.8077\n","Epoch: 598, Train Loss: 0.0950, Train Acc: 0.9569, Val Loss: 1.0258, Val Acc: 0.8072\n","Epoch: 599, Train Loss: 0.0960, Train Acc: 0.9565, Val Loss: 1.0933, Val Acc: 0.8077\n","Epoch: 600, Train Loss: 0.0966, Train Acc: 0.9564, Val Loss: 1.0297, Val Acc: 0.8085\n","Epoch: 601, Train Loss: 0.0967, Train Acc: 0.9563, Val Loss: 1.0362, Val Acc: 0.8114\n","Epoch: 602, Train Loss: 0.0956, Train Acc: 0.9564, Val Loss: 1.0536, Val Acc: 0.8095\n","Epoch: 603, Train Loss: 0.0954, Train Acc: 0.9569, Val Loss: 1.0389, Val Acc: 0.8084\n","Epoch: 604, Train Loss: 0.0947, Train Acc: 0.9570, Val Loss: 1.0506, Val Acc: 0.8077\n","Epoch: 605, Train Loss: 0.0937, Train Acc: 0.9569, Val Loss: 1.0732, Val Acc: 0.8072\n","Epoch: 606, Train Loss: 0.0969, Train Acc: 0.9564, Val Loss: 1.0626, Val Acc: 0.8071\n","Epoch: 607, Train Loss: 0.0975, Train Acc: 0.9562, Val Loss: 1.0266, Val Acc: 0.8078\n","Epoch: 608, Train Loss: 0.0935, Train Acc: 0.9569, Val Loss: 1.0869, Val Acc: 0.8082\n","Epoch: 609, Train Loss: 0.0968, Train Acc: 0.9561, Val Loss: 1.0754, Val Acc: 0.8084\n","Epoch: 610, Train Loss: 0.0947, Train Acc: 0.9566, Val Loss: 1.0170, Val Acc: 0.8095\n","Epoch: 611, Train Loss: 0.0972, Train Acc: 0.9556, Val Loss: 0.9728, Val Acc: 0.8105\n","Epoch: 612, Train Loss: 0.0956, Train Acc: 0.9572, Val Loss: 1.0394, Val Acc: 0.8080\n","Epoch: 613, Train Loss: 0.0956, Train Acc: 0.9558, Val Loss: 1.0299, Val Acc: 0.8100\n","Epoch: 614, Train Loss: 0.0939, Train Acc: 0.9571, Val Loss: 1.0515, Val Acc: 0.8105\n","Epoch: 615, Train Loss: 0.0948, Train Acc: 0.9566, Val Loss: 1.0283, Val Acc: 0.8118\n","Epoch: 616, Train Loss: 0.0953, Train Acc: 0.9561, Val Loss: 1.0584, Val Acc: 0.8081\n","Epoch: 617, Train Loss: 0.0958, Train Acc: 0.9562, Val Loss: 1.0338, Val Acc: 0.8093\n","Epoch: 618, Train Loss: 0.0944, Train Acc: 0.9568, Val Loss: 0.9959, Val Acc: 0.8084\n","Epoch: 619, Train Loss: 0.0947, Train Acc: 0.9562, Val Loss: 1.0491, Val Acc: 0.8104\n","Epoch: 620, Train Loss: 0.0945, Train Acc: 0.9571, Val Loss: 1.0979, Val Acc: 0.8076\n","Epoch: 621, Train Loss: 0.0965, Train Acc: 0.9560, Val Loss: 1.0443, Val Acc: 0.8072\n","Epoch: 622, Train Loss: 0.0942, Train Acc: 0.9578, Val Loss: 1.0736, Val Acc: 0.8058\n","Epoch: 623, Train Loss: 0.0944, Train Acc: 0.9565, Val Loss: 1.0918, Val Acc: 0.8076\n","Epoch: 624, Train Loss: 0.0956, Train Acc: 0.9571, Val Loss: 1.0411, Val Acc: 0.8071\n","Epoch: 625, Train Loss: 0.0959, Train Acc: 0.9563, Val Loss: 1.0608, Val Acc: 0.8095\n","Epoch: 626, Train Loss: 0.0948, Train Acc: 0.9568, Val Loss: 1.0844, Val Acc: 0.8077\n","Epoch: 627, Train Loss: 0.0953, Train Acc: 0.9564, Val Loss: 1.0427, Val Acc: 0.8081\n","Epoch: 628, Train Loss: 0.0961, Train Acc: 0.9568, Val Loss: 1.0194, Val Acc: 0.8067\n","Epoch: 629, Train Loss: 0.0943, Train Acc: 0.9567, Val Loss: 1.0110, Val Acc: 0.8100\n","Epoch: 630, Train Loss: 0.0937, Train Acc: 0.9565, Val Loss: 1.0480, Val Acc: 0.8103\n","Epoch: 631, Train Loss: 0.0944, Train Acc: 0.9573, Val Loss: 1.0539, Val Acc: 0.8109\n","Epoch: 632, Train Loss: 0.0950, Train Acc: 0.9572, Val Loss: 1.0320, Val Acc: 0.8110\n","Epoch: 633, Train Loss: 0.0938, Train Acc: 0.9577, Val Loss: 1.0295, Val Acc: 0.8109\n","Epoch: 634, Train Loss: 0.0952, Train Acc: 0.9568, Val Loss: 1.0498, Val Acc: 0.8093\n","Epoch: 635, Train Loss: 0.0967, Train Acc: 0.9563, Val Loss: 1.0351, Val Acc: 0.8083\n","Epoch: 636, Train Loss: 0.0938, Train Acc: 0.9566, Val Loss: 1.0122, Val Acc: 0.8083\n","Epoch: 637, Train Loss: 0.0946, Train Acc: 0.9565, Val Loss: 1.0161, Val Acc: 0.8100\n","Epoch: 638, Train Loss: 0.0941, Train Acc: 0.9568, Val Loss: 1.0803, Val Acc: 0.8071\n","Epoch: 639, Train Loss: 0.0953, Train Acc: 0.9567, Val Loss: 1.0409, Val Acc: 0.8120\n","Epoch: 640, Train Loss: 0.0933, Train Acc: 0.9578, Val Loss: 1.0140, Val Acc: 0.8109\n","Epoch: 641, Train Loss: 0.0926, Train Acc: 0.9581, Val Loss: 1.0696, Val Acc: 0.8091\n","Epoch: 642, Train Loss: 0.0953, Train Acc: 0.9565, Val Loss: 1.0584, Val Acc: 0.8081\n","Epoch: 643, Train Loss: 0.0953, Train Acc: 0.9566, Val Loss: 1.0737, Val Acc: 0.8082\n","Epoch: 644, Train Loss: 0.0945, Train Acc: 0.9573, Val Loss: 1.0431, Val Acc: 0.8102\n","Epoch: 645, Train Loss: 0.0946, Train Acc: 0.9564, Val Loss: 1.0107, Val Acc: 0.8125\n","Epoch: 646, Train Loss: 0.0948, Train Acc: 0.9566, Val Loss: 1.0709, Val Acc: 0.8104\n","Epoch: 647, Train Loss: 0.0952, Train Acc: 0.9565, Val Loss: 1.0572, Val Acc: 0.8100\n","Epoch: 648, Train Loss: 0.0942, Train Acc: 0.9570, Val Loss: 1.0626, Val Acc: 0.8096\n","Epoch: 649, Train Loss: 0.0945, Train Acc: 0.9567, Val Loss: 1.0364, Val Acc: 0.8076\n","Epoch: 650, Train Loss: 0.0927, Train Acc: 0.9577, Val Loss: 1.0225, Val Acc: 0.8111\n","Epoch: 651, Train Loss: 0.0946, Train Acc: 0.9568, Val Loss: 1.0642, Val Acc: 0.8071\n","Epoch: 652, Train Loss: 0.0945, Train Acc: 0.9567, Val Loss: 1.0530, Val Acc: 0.8081\n","Epoch: 653, Train Loss: 0.0933, Train Acc: 0.9574, Val Loss: 1.0892, Val Acc: 0.8108\n","Epoch: 654, Train Loss: 0.0947, Train Acc: 0.9559, Val Loss: 1.0695, Val Acc: 0.8076\n","Epoch: 655, Train Loss: 0.0963, Train Acc: 0.9564, Val Loss: 1.0354, Val Acc: 0.8083\n","Epoch: 656, Train Loss: 0.0933, Train Acc: 0.9580, Val Loss: 1.0723, Val Acc: 0.8062\n","Epoch: 657, Train Loss: 0.0952, Train Acc: 0.9567, Val Loss: 1.0619, Val Acc: 0.8103\n","Epoch: 658, Train Loss: 0.0944, Train Acc: 0.9566, Val Loss: 1.0268, Val Acc: 0.8089\n","Epoch: 659, Train Loss: 0.0949, Train Acc: 0.9572, Val Loss: 1.0761, Val Acc: 0.8104\n","Epoch: 660, Train Loss: 0.0938, Train Acc: 0.9572, Val Loss: 1.0877, Val Acc: 0.8081\n","Epoch: 661, Train Loss: 0.0950, Train Acc: 0.9566, Val Loss: 1.0328, Val Acc: 0.8105\n","Epoch: 662, Train Loss: 0.0935, Train Acc: 0.9567, Val Loss: 1.0513, Val Acc: 0.8114\n","Epoch: 663, Train Loss: 0.0942, Train Acc: 0.9563, Val Loss: 1.0305, Val Acc: 0.8083\n","Epoch: 664, Train Loss: 0.0945, Train Acc: 0.9567, Val Loss: 1.0561, Val Acc: 0.8073\n","Epoch: 665, Train Loss: 0.0935, Train Acc: 0.9568, Val Loss: 1.0570, Val Acc: 0.8068\n","Epoch: 666, Train Loss: 0.0940, Train Acc: 0.9569, Val Loss: 1.0729, Val Acc: 0.8057\n","Epoch: 667, Train Loss: 0.0930, Train Acc: 0.9570, Val Loss: 1.0840, Val Acc: 0.8087\n","Epoch: 668, Train Loss: 0.0956, Train Acc: 0.9565, Val Loss: 1.0515, Val Acc: 0.8063\n","Epoch: 669, Train Loss: 0.0933, Train Acc: 0.9573, Val Loss: 1.0615, Val Acc: 0.8094\n","Epoch: 670, Train Loss: 0.0943, Train Acc: 0.9568, Val Loss: 1.0325, Val Acc: 0.8069\n","Epoch: 671, Train Loss: 0.0931, Train Acc: 0.9575, Val Loss: 1.0281, Val Acc: 0.8088\n","Epoch: 672, Train Loss: 0.0952, Train Acc: 0.9572, Val Loss: 1.0415, Val Acc: 0.8082\n","Epoch: 673, Train Loss: 0.0941, Train Acc: 0.9573, Val Loss: 1.1141, Val Acc: 0.8048\n","Epoch: 674, Train Loss: 0.0946, Train Acc: 0.9568, Val Loss: 1.0229, Val Acc: 0.8092\n","Epoch: 675, Train Loss: 0.0919, Train Acc: 0.9580, Val Loss: 1.1149, Val Acc: 0.8113\n","Epoch: 676, Train Loss: 0.0950, Train Acc: 0.9572, Val Loss: 1.0084, Val Acc: 0.8111\n","Epoch: 677, Train Loss: 0.0944, Train Acc: 0.9573, Val Loss: 1.0260, Val Acc: 0.8101\n","Epoch: 678, Train Loss: 0.0937, Train Acc: 0.9571, Val Loss: 1.0313, Val Acc: 0.8110\n","Epoch: 679, Train Loss: 0.0929, Train Acc: 0.9575, Val Loss: 1.0325, Val Acc: 0.8110\n","Epoch: 680, Train Loss: 0.0940, Train Acc: 0.9569, Val Loss: 1.0289, Val Acc: 0.8100\n","Epoch: 681, Train Loss: 0.0929, Train Acc: 0.9571, Val Loss: 1.0407, Val Acc: 0.8110\n","Epoch: 682, Train Loss: 0.0933, Train Acc: 0.9570, Val Loss: 1.0359, Val Acc: 0.8106\n","Epoch: 683, Train Loss: 0.0960, Train Acc: 0.9564, Val Loss: 1.0337, Val Acc: 0.8095\n","Epoch: 684, Train Loss: 0.0937, Train Acc: 0.9568, Val Loss: 1.0829, Val Acc: 0.8059\n","Epoch: 685, Train Loss: 0.0944, Train Acc: 0.9565, Val Loss: 1.0561, Val Acc: 0.8097\n","Epoch: 686, Train Loss: 0.0909, Train Acc: 0.9586, Val Loss: 1.0441, Val Acc: 0.8067\n","Epoch: 687, Train Loss: 0.0935, Train Acc: 0.9566, Val Loss: 1.0293, Val Acc: 0.8116\n","Epoch: 688, Train Loss: 0.0914, Train Acc: 0.9585, Val Loss: 1.0557, Val Acc: 0.8130\n","Epoch: 689, Train Loss: 0.0949, Train Acc: 0.9567, Val Loss: 1.0391, Val Acc: 0.8122\n","Epoch: 690, Train Loss: 0.0940, Train Acc: 0.9570, Val Loss: 1.0804, Val Acc: 0.8091\n","Epoch: 691, Train Loss: 0.0915, Train Acc: 0.9577, Val Loss: 1.1306, Val Acc: 0.8081\n","Epoch: 692, Train Loss: 0.0943, Train Acc: 0.9566, Val Loss: 1.0509, Val Acc: 0.8095\n","Epoch: 693, Train Loss: 0.0926, Train Acc: 0.9572, Val Loss: 1.0952, Val Acc: 0.8114\n","Epoch: 694, Train Loss: 0.0952, Train Acc: 0.9568, Val Loss: 1.0329, Val Acc: 0.8096\n","Epoch: 695, Train Loss: 0.0946, Train Acc: 0.9574, Val Loss: 1.0445, Val Acc: 0.8079\n","Epoch: 696, Train Loss: 0.0924, Train Acc: 0.9585, Val Loss: 1.0690, Val Acc: 0.8115\n","Epoch: 697, Train Loss: 0.0925, Train Acc: 0.9570, Val Loss: 1.0553, Val Acc: 0.8073\n","Epoch: 698, Train Loss: 0.0944, Train Acc: 0.9563, Val Loss: 1.0434, Val Acc: 0.8124\n","Epoch: 699, Train Loss: 0.0930, Train Acc: 0.9571, Val Loss: 1.0402, Val Acc: 0.8076\n","Epoch: 700, Train Loss: 0.0933, Train Acc: 0.9575, Val Loss: 1.0435, Val Acc: 0.8106\n","Epoch: 701, Train Loss: 0.0926, Train Acc: 0.9578, Val Loss: 1.1157, Val Acc: 0.8102\n","Epoch: 702, Train Loss: 0.0939, Train Acc: 0.9572, Val Loss: 1.0639, Val Acc: 0.8119\n","Epoch: 703, Train Loss: 0.0935, Train Acc: 0.9575, Val Loss: 1.0498, Val Acc: 0.8124\n","Epoch: 704, Train Loss: 0.0931, Train Acc: 0.9571, Val Loss: 1.0676, Val Acc: 0.8100\n","Epoch: 705, Train Loss: 0.0968, Train Acc: 0.9557, Val Loss: 1.0179, Val Acc: 0.8098\n","Epoch: 706, Train Loss: 0.0929, Train Acc: 0.9576, Val Loss: 1.0734, Val Acc: 0.8089\n","Epoch: 707, Train Loss: 0.0916, Train Acc: 0.9582, Val Loss: 1.0514, Val Acc: 0.8126\n","Epoch: 708, Train Loss: 0.0939, Train Acc: 0.9571, Val Loss: 1.0450, Val Acc: 0.8097\n","Epoch: 709, Train Loss: 0.0927, Train Acc: 0.9578, Val Loss: 1.0301, Val Acc: 0.8110\n","Epoch: 710, Train Loss: 0.0909, Train Acc: 0.9581, Val Loss: 1.0495, Val Acc: 0.8070\n","Epoch: 711, Train Loss: 0.0936, Train Acc: 0.9572, Val Loss: 1.0478, Val Acc: 0.8075\n","Epoch: 712, Train Loss: 0.0914, Train Acc: 0.9581, Val Loss: 1.0811, Val Acc: 0.8076\n","Epoch: 713, Train Loss: 0.0941, Train Acc: 0.9575, Val Loss: 1.0131, Val Acc: 0.8079\n","Epoch: 714, Train Loss: 0.0932, Train Acc: 0.9573, Val Loss: 1.1010, Val Acc: 0.8078\n","Epoch: 715, Train Loss: 0.0935, Train Acc: 0.9571, Val Loss: 1.0647, Val Acc: 0.8067\n","Epoch: 716, Train Loss: 0.0929, Train Acc: 0.9573, Val Loss: 1.0595, Val Acc: 0.8089\n","Epoch: 717, Train Loss: 0.0930, Train Acc: 0.9570, Val Loss: 1.0454, Val Acc: 0.8112\n","Epoch: 718, Train Loss: 0.0928, Train Acc: 0.9575, Val Loss: 1.0385, Val Acc: 0.8116\n","Epoch: 719, Train Loss: 0.0938, Train Acc: 0.9575, Val Loss: 1.0411, Val Acc: 0.8089\n","Epoch: 720, Train Loss: 0.0948, Train Acc: 0.9569, Val Loss: 1.0473, Val Acc: 0.8076\n","Epoch: 721, Train Loss: 0.0933, Train Acc: 0.9574, Val Loss: 1.0375, Val Acc: 0.8108\n","Epoch: 722, Train Loss: 0.0947, Train Acc: 0.9577, Val Loss: 1.0471, Val Acc: 0.8090\n","Epoch: 723, Train Loss: 0.0925, Train Acc: 0.9582, Val Loss: 1.0391, Val Acc: 0.8063\n","Epoch: 724, Train Loss: 0.0925, Train Acc: 0.9574, Val Loss: 1.0778, Val Acc: 0.8090\n","Epoch: 725, Train Loss: 0.0918, Train Acc: 0.9572, Val Loss: 1.0577, Val Acc: 0.8093\n","Epoch: 726, Train Loss: 0.0945, Train Acc: 0.9568, Val Loss: 1.0323, Val Acc: 0.8102\n","Epoch: 727, Train Loss: 0.0924, Train Acc: 0.9580, Val Loss: 1.0445, Val Acc: 0.8112\n","Epoch: 728, Train Loss: 0.0938, Train Acc: 0.9574, Val Loss: 1.0650, Val Acc: 0.8103\n","Epoch: 729, Train Loss: 0.0927, Train Acc: 0.9578, Val Loss: 1.0229, Val Acc: 0.8106\n","Epoch: 730, Train Loss: 0.0918, Train Acc: 0.9577, Val Loss: 1.0227, Val Acc: 0.8102\n","Epoch: 731, Train Loss: 0.0924, Train Acc: 0.9579, Val Loss: 1.0235, Val Acc: 0.8118\n","Epoch: 732, Train Loss: 0.0937, Train Acc: 0.9570, Val Loss: 1.0399, Val Acc: 0.8101\n","Epoch: 733, Train Loss: 0.0921, Train Acc: 0.9573, Val Loss: 1.0256, Val Acc: 0.8096\n","Epoch: 734, Train Loss: 0.0929, Train Acc: 0.9569, Val Loss: 1.0552, Val Acc: 0.8101\n","Epoch: 735, Train Loss: 0.0939, Train Acc: 0.9570, Val Loss: 0.9986, Val Acc: 0.8096\n","Epoch: 736, Train Loss: 0.0926, Train Acc: 0.9577, Val Loss: 1.0768, Val Acc: 0.8092\n","Epoch: 737, Train Loss: 0.0930, Train Acc: 0.9576, Val Loss: 1.0286, Val Acc: 0.8117\n","Epoch: 738, Train Loss: 0.0914, Train Acc: 0.9578, Val Loss: 1.1069, Val Acc: 0.8106\n","Epoch: 739, Train Loss: 0.0934, Train Acc: 0.9569, Val Loss: 1.0671, Val Acc: 0.8093\n","Epoch: 740, Train Loss: 0.0921, Train Acc: 0.9580, Val Loss: 1.0568, Val Acc: 0.8083\n","Epoch: 741, Train Loss: 0.0925, Train Acc: 0.9578, Val Loss: 1.0200, Val Acc: 0.8108\n","Epoch: 742, Train Loss: 0.0918, Train Acc: 0.9580, Val Loss: 1.0028, Val Acc: 0.8100\n","Epoch: 743, Train Loss: 0.0931, Train Acc: 0.9573, Val Loss: 1.0248, Val Acc: 0.8100\n","Epoch: 744, Train Loss: 0.0930, Train Acc: 0.9580, Val Loss: 1.1031, Val Acc: 0.8082\n","Epoch: 745, Train Loss: 0.0925, Train Acc: 0.9577, Val Loss: 1.0235, Val Acc: 0.8096\n","Epoch: 746, Train Loss: 0.0915, Train Acc: 0.9581, Val Loss: 1.0386, Val Acc: 0.8091\n","Epoch: 747, Train Loss: 0.0922, Train Acc: 0.9574, Val Loss: 1.0528, Val Acc: 0.8105\n","Epoch: 748, Train Loss: 0.0923, Train Acc: 0.9579, Val Loss: 1.0554, Val Acc: 0.8099\n","Epoch: 749, Train Loss: 0.0921, Train Acc: 0.9572, Val Loss: 1.0578, Val Acc: 0.8098\n","Epoch: 750, Train Loss: 0.0929, Train Acc: 0.9583, Val Loss: 1.0507, Val Acc: 0.8079\n","Epoch: 751, Train Loss: 0.0919, Train Acc: 0.9574, Val Loss: 1.0217, Val Acc: 0.8089\n","Epoch: 752, Train Loss: 0.0931, Train Acc: 0.9576, Val Loss: 1.0668, Val Acc: 0.8093\n","Epoch: 753, Train Loss: 0.0919, Train Acc: 0.9575, Val Loss: 1.0173, Val Acc: 0.8091\n","Epoch: 754, Train Loss: 0.0921, Train Acc: 0.9577, Val Loss: 1.0532, Val Acc: 0.8132\n","Epoch: 755, Train Loss: 0.0914, Train Acc: 0.9581, Val Loss: 0.9997, Val Acc: 0.8115\n","Epoch: 756, Train Loss: 0.0931, Train Acc: 0.9572, Val Loss: 1.0504, Val Acc: 0.8079\n","Epoch: 757, Train Loss: 0.0908, Train Acc: 0.9588, Val Loss: 1.0151, Val Acc: 0.8082\n","Epoch: 758, Train Loss: 0.0923, Train Acc: 0.9576, Val Loss: 1.0298, Val Acc: 0.8064\n","Epoch: 759, Train Loss: 0.0910, Train Acc: 0.9573, Val Loss: 1.0262, Val Acc: 0.8103\n","Epoch: 760, Train Loss: 0.0916, Train Acc: 0.9579, Val Loss: 1.0595, Val Acc: 0.8120\n","Epoch: 761, Train Loss: 0.0920, Train Acc: 0.9575, Val Loss: 1.0684, Val Acc: 0.8097\n","Epoch: 762, Train Loss: 0.0934, Train Acc: 0.9572, Val Loss: 1.0198, Val Acc: 0.8132\n","Epoch: 763, Train Loss: 0.0928, Train Acc: 0.9573, Val Loss: 1.0610, Val Acc: 0.8097\n","Epoch: 764, Train Loss: 0.0912, Train Acc: 0.9580, Val Loss: 1.0557, Val Acc: 0.8089\n","Epoch: 765, Train Loss: 0.0916, Train Acc: 0.9579, Val Loss: 1.0416, Val Acc: 0.8086\n","Epoch: 766, Train Loss: 0.0926, Train Acc: 0.9577, Val Loss: 1.0328, Val Acc: 0.8074\n","Epoch: 767, Train Loss: 0.0904, Train Acc: 0.9592, Val Loss: 1.0681, Val Acc: 0.8090\n","Epoch: 768, Train Loss: 0.0918, Train Acc: 0.9584, Val Loss: 1.0405, Val Acc: 0.8091\n","Epoch: 769, Train Loss: 0.0924, Train Acc: 0.9575, Val Loss: 1.1011, Val Acc: 0.8083\n","Epoch: 770, Train Loss: 0.0915, Train Acc: 0.9582, Val Loss: 1.0244, Val Acc: 0.8100\n","Epoch: 771, Train Loss: 0.0908, Train Acc: 0.9581, Val Loss: 1.0901, Val Acc: 0.8093\n","Epoch: 772, Train Loss: 0.0917, Train Acc: 0.9582, Val Loss: 1.1103, Val Acc: 0.8098\n","Epoch: 773, Train Loss: 0.0919, Train Acc: 0.9582, Val Loss: 1.0333, Val Acc: 0.8097\n","Epoch: 774, Train Loss: 0.0925, Train Acc: 0.9572, Val Loss: 1.0779, Val Acc: 0.8099\n","Epoch: 775, Train Loss: 0.0918, Train Acc: 0.9578, Val Loss: 1.0744, Val Acc: 0.8085\n","Epoch: 776, Train Loss: 0.0926, Train Acc: 0.9577, Val Loss: 1.0554, Val Acc: 0.8091\n","Epoch: 777, Train Loss: 0.0915, Train Acc: 0.9578, Val Loss: 1.0655, Val Acc: 0.8099\n","Epoch: 778, Train Loss: 0.0906, Train Acc: 0.9582, Val Loss: 1.0568, Val Acc: 0.8083\n","Epoch: 779, Train Loss: 0.0913, Train Acc: 0.9577, Val Loss: 1.1045, Val Acc: 0.8098\n","Epoch: 780, Train Loss: 0.0912, Train Acc: 0.9575, Val Loss: 1.0940, Val Acc: 0.8088\n","Epoch: 781, Train Loss: 0.0912, Train Acc: 0.9577, Val Loss: 1.0413, Val Acc: 0.8090\n","Epoch: 782, Train Loss: 0.0924, Train Acc: 0.9571, Val Loss: 1.0762, Val Acc: 0.8097\n","Epoch: 783, Train Loss: 0.0918, Train Acc: 0.9577, Val Loss: 1.0312, Val Acc: 0.8098\n","Epoch: 784, Train Loss: 0.0908, Train Acc: 0.9581, Val Loss: 1.1457, Val Acc: 0.8065\n","Epoch: 785, Train Loss: 0.0924, Train Acc: 0.9583, Val Loss: 1.0175, Val Acc: 0.8068\n","Epoch: 786, Train Loss: 0.0915, Train Acc: 0.9583, Val Loss: 1.0531, Val Acc: 0.8068\n","Epoch: 787, Train Loss: 0.0922, Train Acc: 0.9579, Val Loss: 1.0703, Val Acc: 0.8091\n","Epoch: 788, Train Loss: 0.0901, Train Acc: 0.9579, Val Loss: 1.1139, Val Acc: 0.8072\n","Epoch: 789, Train Loss: 0.0914, Train Acc: 0.9581, Val Loss: 1.0354, Val Acc: 0.8097\n","Epoch: 790, Train Loss: 0.0919, Train Acc: 0.9584, Val Loss: 1.0668, Val Acc: 0.8084\n","Epoch: 791, Train Loss: 0.0913, Train Acc: 0.9578, Val Loss: 1.0955, Val Acc: 0.8082\n","Epoch: 792, Train Loss: 0.0930, Train Acc: 0.9576, Val Loss: 1.0369, Val Acc: 0.8090\n","Epoch: 793, Train Loss: 0.0909, Train Acc: 0.9575, Val Loss: 1.0566, Val Acc: 0.8107\n","Epoch: 794, Train Loss: 0.0917, Train Acc: 0.9583, Val Loss: 1.0449, Val Acc: 0.8102\n","Epoch: 795, Train Loss: 0.0922, Train Acc: 0.9577, Val Loss: 1.0423, Val Acc: 0.8089\n","Epoch: 796, Train Loss: 0.0931, Train Acc: 0.9578, Val Loss: 1.0279, Val Acc: 0.8112\n","Epoch: 797, Train Loss: 0.0915, Train Acc: 0.9575, Val Loss: 1.0687, Val Acc: 0.8093\n","Epoch: 798, Train Loss: 0.0914, Train Acc: 0.9582, Val Loss: 1.0545, Val Acc: 0.8082\n","Epoch: 799, Train Loss: 0.0906, Train Acc: 0.9580, Val Loss: 1.0172, Val Acc: 0.8099\n","Epoch: 800, Train Loss: 0.0913, Train Acc: 0.9577, Val Loss: 1.0622, Val Acc: 0.8069\n","Epoch: 801, Train Loss: 0.0927, Train Acc: 0.9576, Val Loss: 1.0915, Val Acc: 0.8070\n","Epoch: 802, Train Loss: 0.0920, Train Acc: 0.9577, Val Loss: 1.0772, Val Acc: 0.8083\n","Epoch: 803, Train Loss: 0.0903, Train Acc: 0.9585, Val Loss: 1.0667, Val Acc: 0.8085\n","Epoch: 804, Train Loss: 0.0916, Train Acc: 0.9576, Val Loss: 0.9977, Val Acc: 0.8131\n","Epoch: 805, Train Loss: 0.0920, Train Acc: 0.9581, Val Loss: 1.1078, Val Acc: 0.8056\n","Epoch: 806, Train Loss: 0.0911, Train Acc: 0.9583, Val Loss: 1.0103, Val Acc: 0.8084\n","Epoch: 807, Train Loss: 0.0908, Train Acc: 0.9582, Val Loss: 1.0826, Val Acc: 0.8108\n","Epoch: 808, Train Loss: 0.0904, Train Acc: 0.9582, Val Loss: 1.0869, Val Acc: 0.8077\n","Epoch: 809, Train Loss: 0.0916, Train Acc: 0.9582, Val Loss: 1.0733, Val Acc: 0.8117\n","Epoch: 810, Train Loss: 0.0920, Train Acc: 0.9571, Val Loss: 1.0788, Val Acc: 0.8097\n","Epoch: 811, Train Loss: 0.0905, Train Acc: 0.9575, Val Loss: 1.0882, Val Acc: 0.8098\n","Epoch: 812, Train Loss: 0.0916, Train Acc: 0.9581, Val Loss: 1.0424, Val Acc: 0.8120\n","Epoch: 813, Train Loss: 0.0916, Train Acc: 0.9583, Val Loss: 1.0749, Val Acc: 0.8113\n","Epoch: 814, Train Loss: 0.0915, Train Acc: 0.9578, Val Loss: 1.0508, Val Acc: 0.8102\n","Epoch: 815, Train Loss: 0.0909, Train Acc: 0.9580, Val Loss: 1.0367, Val Acc: 0.8073\n","Epoch: 816, Train Loss: 0.0901, Train Acc: 0.9582, Val Loss: 1.0231, Val Acc: 0.8104\n","Epoch: 817, Train Loss: 0.0913, Train Acc: 0.9579, Val Loss: 1.0238, Val Acc: 0.8113\n","Epoch: 818, Train Loss: 0.0904, Train Acc: 0.9587, Val Loss: 1.0516, Val Acc: 0.8108\n","Epoch: 819, Train Loss: 0.0927, Train Acc: 0.9577, Val Loss: 1.0573, Val Acc: 0.8098\n","Epoch: 820, Train Loss: 0.0898, Train Acc: 0.9590, Val Loss: 1.0435, Val Acc: 0.8095\n","Epoch: 821, Train Loss: 0.0903, Train Acc: 0.9584, Val Loss: 1.0515, Val Acc: 0.8077\n","Epoch: 822, Train Loss: 0.0912, Train Acc: 0.9583, Val Loss: 1.0614, Val Acc: 0.8080\n","Epoch: 823, Train Loss: 0.0907, Train Acc: 0.9576, Val Loss: 1.0237, Val Acc: 0.8059\n","Epoch: 824, Train Loss: 0.0917, Train Acc: 0.9577, Val Loss: 1.0093, Val Acc: 0.8091\n","Epoch: 825, Train Loss: 0.0909, Train Acc: 0.9581, Val Loss: 1.0343, Val Acc: 0.8099\n","Epoch: 826, Train Loss: 0.0920, Train Acc: 0.9576, Val Loss: 1.0385, Val Acc: 0.8104\n","Epoch: 827, Train Loss: 0.0909, Train Acc: 0.9576, Val Loss: 1.0367, Val Acc: 0.8099\n","Epoch: 828, Train Loss: 0.0915, Train Acc: 0.9579, Val Loss: 1.0816, Val Acc: 0.8093\n","Epoch: 829, Train Loss: 0.0983, Train Acc: 0.9560, Val Loss: 0.9908, Val Acc: 0.8076\n","Epoch: 830, Train Loss: 0.0909, Train Acc: 0.9586, Val Loss: 1.0289, Val Acc: 0.8107\n","Epoch: 831, Train Loss: 0.0903, Train Acc: 0.9585, Val Loss: 1.0353, Val Acc: 0.8095\n","Epoch: 832, Train Loss: 0.0905, Train Acc: 0.9576, Val Loss: 1.0667, Val Acc: 0.8083\n","Epoch: 833, Train Loss: 0.0894, Train Acc: 0.9583, Val Loss: 1.0364, Val Acc: 0.8110\n","Epoch: 834, Train Loss: 0.0911, Train Acc: 0.9581, Val Loss: 1.0331, Val Acc: 0.8120\n","Epoch: 835, Train Loss: 0.0889, Train Acc: 0.9588, Val Loss: 1.0845, Val Acc: 0.8098\n","Epoch: 836, Train Loss: 0.0914, Train Acc: 0.9578, Val Loss: 1.0134, Val Acc: 0.8117\n","Epoch: 837, Train Loss: 0.0892, Train Acc: 0.9589, Val Loss: 1.0851, Val Acc: 0.8085\n","Epoch: 838, Train Loss: 0.0900, Train Acc: 0.9588, Val Loss: 1.1015, Val Acc: 0.8080\n","Epoch: 839, Train Loss: 0.0914, Train Acc: 0.9580, Val Loss: 1.0713, Val Acc: 0.8107\n","Epoch: 840, Train Loss: 0.0899, Train Acc: 0.9585, Val Loss: 1.0320, Val Acc: 0.8122\n","Epoch: 841, Train Loss: 0.0892, Train Acc: 0.9584, Val Loss: 1.0775, Val Acc: 0.8091\n","Epoch: 842, Train Loss: 0.0897, Train Acc: 0.9583, Val Loss: 1.0558, Val Acc: 0.8100\n","Epoch: 843, Train Loss: 0.0895, Train Acc: 0.9590, Val Loss: 1.0648, Val Acc: 0.8112\n","Epoch: 844, Train Loss: 0.0899, Train Acc: 0.9585, Val Loss: 1.0517, Val Acc: 0.8078\n","Epoch: 845, Train Loss: 0.0919, Train Acc: 0.9582, Val Loss: 1.0353, Val Acc: 0.8083\n","Epoch: 846, Train Loss: 0.0881, Train Acc: 0.9592, Val Loss: 1.0398, Val Acc: 0.8126\n","Epoch: 847, Train Loss: 0.0908, Train Acc: 0.9584, Val Loss: 1.0420, Val Acc: 0.8058\n","Epoch: 848, Train Loss: 0.0898, Train Acc: 0.9586, Val Loss: 1.0588, Val Acc: 0.8093\n","Epoch: 849, Train Loss: 0.0908, Train Acc: 0.9584, Val Loss: 1.0282, Val Acc: 0.8085\n","Epoch: 850, Train Loss: 0.0903, Train Acc: 0.9581, Val Loss: 1.0122, Val Acc: 0.8114\n","Epoch: 851, Train Loss: 0.0918, Train Acc: 0.9583, Val Loss: 1.0300, Val Acc: 0.8092\n","Epoch: 852, Train Loss: 0.0908, Train Acc: 0.9583, Val Loss: 1.0503, Val Acc: 0.8102\n","Epoch: 853, Train Loss: 0.0895, Train Acc: 0.9585, Val Loss: 1.0803, Val Acc: 0.8115\n","Epoch: 854, Train Loss: 0.0907, Train Acc: 0.9577, Val Loss: 1.0859, Val Acc: 0.8104\n","Epoch: 855, Train Loss: 0.0899, Train Acc: 0.9588, Val Loss: 1.0775, Val Acc: 0.8093\n","Epoch: 856, Train Loss: 0.0902, Train Acc: 0.9579, Val Loss: 1.0683, Val Acc: 0.8079\n","Epoch: 857, Train Loss: 0.0903, Train Acc: 0.9581, Val Loss: 1.0333, Val Acc: 0.8105\n","Epoch: 858, Train Loss: 0.0905, Train Acc: 0.9582, Val Loss: 1.0786, Val Acc: 0.8104\n","Epoch: 859, Train Loss: 0.0913, Train Acc: 0.9579, Val Loss: 1.0212, Val Acc: 0.8117\n","Epoch: 860, Train Loss: 0.0899, Train Acc: 0.9589, Val Loss: 1.0635, Val Acc: 0.8108\n","Epoch: 861, Train Loss: 0.0900, Train Acc: 0.9586, Val Loss: 1.0798, Val Acc: 0.8105\n","Epoch: 862, Train Loss: 0.0902, Train Acc: 0.9584, Val Loss: 1.0435, Val Acc: 0.8093\n","Epoch: 863, Train Loss: 0.0906, Train Acc: 0.9585, Val Loss: 1.0148, Val Acc: 0.8094\n","Epoch: 864, Train Loss: 0.0905, Train Acc: 0.9581, Val Loss: 1.0298, Val Acc: 0.8081\n","Epoch: 865, Train Loss: 0.0913, Train Acc: 0.9583, Val Loss: 1.0399, Val Acc: 0.8105\n","Epoch: 866, Train Loss: 0.0898, Train Acc: 0.9588, Val Loss: 1.0441, Val Acc: 0.8074\n","Epoch: 867, Train Loss: 0.0894, Train Acc: 0.9589, Val Loss: 1.0927, Val Acc: 0.8112\n","Epoch: 868, Train Loss: 0.0894, Train Acc: 0.9588, Val Loss: 1.0678, Val Acc: 0.8100\n","Epoch: 869, Train Loss: 0.0891, Train Acc: 0.9579, Val Loss: 1.0739, Val Acc: 0.8089\n","Epoch: 870, Train Loss: 0.0906, Train Acc: 0.9582, Val Loss: 1.1059, Val Acc: 0.8118\n","Epoch: 871, Train Loss: 0.0901, Train Acc: 0.9589, Val Loss: 1.0967, Val Acc: 0.8091\n","Epoch: 872, Train Loss: 0.0898, Train Acc: 0.9588, Val Loss: 1.0744, Val Acc: 0.8105\n","Epoch: 873, Train Loss: 0.0888, Train Acc: 0.9591, Val Loss: 1.1373, Val Acc: 0.8098\n","Epoch: 874, Train Loss: 0.0908, Train Acc: 0.9578, Val Loss: 1.0631, Val Acc: 0.8095\n","Epoch: 875, Train Loss: 0.0907, Train Acc: 0.9579, Val Loss: 1.0322, Val Acc: 0.8102\n","Epoch: 876, Train Loss: 0.0887, Train Acc: 0.9589, Val Loss: 1.1170, Val Acc: 0.8073\n","Epoch: 877, Train Loss: 0.0898, Train Acc: 0.9587, Val Loss: 1.1280, Val Acc: 0.8074\n","Epoch: 878, Train Loss: 0.0909, Train Acc: 0.9584, Val Loss: 1.0768, Val Acc: 0.8112\n","Epoch: 879, Train Loss: 0.0905, Train Acc: 0.9584, Val Loss: 1.0631, Val Acc: 0.8111\n","Epoch: 880, Train Loss: 0.0904, Train Acc: 0.9586, Val Loss: 1.0440, Val Acc: 0.8105\n","Epoch: 881, Train Loss: 0.0886, Train Acc: 0.9588, Val Loss: 1.0926, Val Acc: 0.8091\n","Epoch: 882, Train Loss: 0.0892, Train Acc: 0.9586, Val Loss: 1.0417, Val Acc: 0.8118\n","Epoch: 883, Train Loss: 0.0903, Train Acc: 0.9589, Val Loss: 1.0733, Val Acc: 0.8099\n","Epoch: 884, Train Loss: 0.0899, Train Acc: 0.9589, Val Loss: 1.0714, Val Acc: 0.8097\n","Epoch: 885, Train Loss: 0.0904, Train Acc: 0.9583, Val Loss: 1.0822, Val Acc: 0.8066\n","Epoch: 886, Train Loss: 0.0904, Train Acc: 0.9585, Val Loss: 1.0428, Val Acc: 0.8069\n","Epoch: 887, Train Loss: 0.0890, Train Acc: 0.9588, Val Loss: 1.0837, Val Acc: 0.8093\n","Epoch: 888, Train Loss: 0.0901, Train Acc: 0.9577, Val Loss: 1.0818, Val Acc: 0.8102\n","Epoch: 889, Train Loss: 0.0910, Train Acc: 0.9581, Val Loss: 1.0272, Val Acc: 0.8110\n","Epoch: 890, Train Loss: 0.0904, Train Acc: 0.9582, Val Loss: 1.0758, Val Acc: 0.8143\n","Epoch: 891, Train Loss: 0.0901, Train Acc: 0.9578, Val Loss: 1.0612, Val Acc: 0.8091\n","Epoch: 892, Train Loss: 0.0898, Train Acc: 0.9589, Val Loss: 1.0620, Val Acc: 0.8113\n","Epoch: 893, Train Loss: 0.0895, Train Acc: 0.9589, Val Loss: 1.1084, Val Acc: 0.8086\n","Epoch: 894, Train Loss: 0.0899, Train Acc: 0.9584, Val Loss: 1.0853, Val Acc: 0.8075\n","Epoch: 895, Train Loss: 0.0892, Train Acc: 0.9585, Val Loss: 1.0776, Val Acc: 0.8101\n","Epoch: 896, Train Loss: 0.0907, Train Acc: 0.9582, Val Loss: 1.0813, Val Acc: 0.8096\n","Epoch: 897, Train Loss: 0.0897, Train Acc: 0.9583, Val Loss: 1.0783, Val Acc: 0.8102\n","Epoch: 898, Train Loss: 0.0898, Train Acc: 0.9589, Val Loss: 1.0757, Val Acc: 0.8095\n","Epoch: 899, Train Loss: 0.0894, Train Acc: 0.9591, Val Loss: 1.1042, Val Acc: 0.8108\n","Epoch: 900, Train Loss: 0.0901, Train Acc: 0.9587, Val Loss: 1.0855, Val Acc: 0.8072\n","Epoch: 901, Train Loss: 0.0900, Train Acc: 0.9585, Val Loss: 1.0755, Val Acc: 0.8091\n","Epoch: 902, Train Loss: 0.0892, Train Acc: 0.9587, Val Loss: 1.0745, Val Acc: 0.8124\n","Epoch: 903, Train Loss: 0.0903, Train Acc: 0.9583, Val Loss: 1.0763, Val Acc: 0.8101\n","Epoch: 904, Train Loss: 0.0878, Train Acc: 0.9589, Val Loss: 1.0837, Val Acc: 0.8095\n","Epoch: 905, Train Loss: 0.0892, Train Acc: 0.9587, Val Loss: 1.1246, Val Acc: 0.8087\n","Epoch: 906, Train Loss: 0.0891, Train Acc: 0.9586, Val Loss: 1.1131, Val Acc: 0.8113\n","Epoch: 907, Train Loss: 0.0899, Train Acc: 0.9581, Val Loss: 1.0591, Val Acc: 0.8117\n","Epoch: 908, Train Loss: 0.0893, Train Acc: 0.9584, Val Loss: 1.0924, Val Acc: 0.8084\n","Epoch: 909, Train Loss: 0.0901, Train Acc: 0.9586, Val Loss: 1.1157, Val Acc: 0.8095\n","Epoch: 910, Train Loss: 0.0897, Train Acc: 0.9588, Val Loss: 1.0851, Val Acc: 0.8106\n","Epoch: 911, Train Loss: 0.0909, Train Acc: 0.9582, Val Loss: 1.1082, Val Acc: 0.8075\n","Epoch: 912, Train Loss: 0.0880, Train Acc: 0.9594, Val Loss: 1.0896, Val Acc: 0.8104\n","Epoch: 913, Train Loss: 0.0897, Train Acc: 0.9583, Val Loss: 1.1271, Val Acc: 0.8086\n","Epoch: 914, Train Loss: 0.0899, Train Acc: 0.9589, Val Loss: 1.0763, Val Acc: 0.8104\n","Epoch: 915, Train Loss: 0.0890, Train Acc: 0.9591, Val Loss: 1.1098, Val Acc: 0.8112\n","Epoch: 916, Train Loss: 0.0886, Train Acc: 0.9591, Val Loss: 1.0712, Val Acc: 0.8132\n","Epoch: 917, Train Loss: 0.0890, Train Acc: 0.9588, Val Loss: 1.0486, Val Acc: 0.8116\n","Epoch: 918, Train Loss: 0.0897, Train Acc: 0.9585, Val Loss: 1.0885, Val Acc: 0.8118\n","Epoch: 919, Train Loss: 0.0893, Train Acc: 0.9590, Val Loss: 1.1011, Val Acc: 0.8110\n","Epoch: 920, Train Loss: 0.0896, Train Acc: 0.9586, Val Loss: 1.0630, Val Acc: 0.8083\n","Epoch: 921, Train Loss: 0.0902, Train Acc: 0.9586, Val Loss: 1.0796, Val Acc: 0.8124\n","Epoch: 922, Train Loss: 0.0884, Train Acc: 0.9589, Val Loss: 1.1364, Val Acc: 0.8118\n","Epoch: 923, Train Loss: 0.0890, Train Acc: 0.9590, Val Loss: 1.1200, Val Acc: 0.8101\n","Epoch: 924, Train Loss: 0.0895, Train Acc: 0.9585, Val Loss: 1.1195, Val Acc: 0.8104\n","Epoch: 925, Train Loss: 0.0894, Train Acc: 0.9585, Val Loss: 1.0466, Val Acc: 0.8109\n","Epoch: 926, Train Loss: 0.0886, Train Acc: 0.9596, Val Loss: 1.0879, Val Acc: 0.8091\n","Epoch: 927, Train Loss: 0.0899, Train Acc: 0.9583, Val Loss: 1.0497, Val Acc: 0.8119\n","Epoch: 928, Train Loss: 0.0894, Train Acc: 0.9582, Val Loss: 1.0834, Val Acc: 0.8116\n","Epoch: 929, Train Loss: 0.0890, Train Acc: 0.9584, Val Loss: 1.1115, Val Acc: 0.8077\n","Epoch: 930, Train Loss: 0.0907, Train Acc: 0.9581, Val Loss: 1.0540, Val Acc: 0.8120\n","Epoch: 931, Train Loss: 0.0888, Train Acc: 0.9584, Val Loss: 1.0696, Val Acc: 0.8118\n","Epoch: 932, Train Loss: 0.0890, Train Acc: 0.9593, Val Loss: 1.0691, Val Acc: 0.8117\n","Epoch: 933, Train Loss: 0.0897, Train Acc: 0.9582, Val Loss: 1.1135, Val Acc: 0.8107\n","Epoch: 934, Train Loss: 0.0911, Train Acc: 0.9578, Val Loss: 1.0452, Val Acc: 0.8077\n","Epoch: 935, Train Loss: 0.0872, Train Acc: 0.9589, Val Loss: 1.1451, Val Acc: 0.8092\n","Epoch: 936, Train Loss: 0.0893, Train Acc: 0.9582, Val Loss: 1.1175, Val Acc: 0.8105\n","Epoch: 937, Train Loss: 0.0886, Train Acc: 0.9590, Val Loss: 1.0708, Val Acc: 0.8105\n","Epoch: 938, Train Loss: 0.0882, Train Acc: 0.9593, Val Loss: 1.0570, Val Acc: 0.8119\n","Epoch: 939, Train Loss: 0.0896, Train Acc: 0.9589, Val Loss: 1.0768, Val Acc: 0.8105\n","Epoch: 940, Train Loss: 0.0906, Train Acc: 0.9586, Val Loss: 1.0632, Val Acc: 0.8125\n","Epoch: 941, Train Loss: 0.0875, Train Acc: 0.9597, Val Loss: 1.0989, Val Acc: 0.8093\n","Epoch: 942, Train Loss: 0.0887, Train Acc: 0.9588, Val Loss: 1.1022, Val Acc: 0.8105\n","Epoch: 943, Train Loss: 0.0888, Train Acc: 0.9588, Val Loss: 1.1059, Val Acc: 0.8106\n","Epoch: 944, Train Loss: 0.0897, Train Acc: 0.9589, Val Loss: 1.1064, Val Acc: 0.8105\n","Epoch: 945, Train Loss: 0.0880, Train Acc: 0.9589, Val Loss: 1.0942, Val Acc: 0.8105\n","Epoch: 946, Train Loss: 0.0886, Train Acc: 0.9592, Val Loss: 1.0736, Val Acc: 0.8114\n","Epoch: 947, Train Loss: 0.0886, Train Acc: 0.9588, Val Loss: 1.1169, Val Acc: 0.8102\n","Epoch: 948, Train Loss: 0.0898, Train Acc: 0.9579, Val Loss: 1.1612, Val Acc: 0.8100\n","Epoch: 949, Train Loss: 0.0900, Train Acc: 0.9582, Val Loss: 1.0783, Val Acc: 0.8112\n","Epoch: 950, Train Loss: 0.0878, Train Acc: 0.9587, Val Loss: 1.1162, Val Acc: 0.8063\n","Epoch: 951, Train Loss: 0.0894, Train Acc: 0.9594, Val Loss: 1.0624, Val Acc: 0.8140\n","Epoch: 952, Train Loss: 0.0903, Train Acc: 0.9587, Val Loss: 1.1378, Val Acc: 0.8098\n","Epoch: 953, Train Loss: 0.0914, Train Acc: 0.9574, Val Loss: 1.0550, Val Acc: 0.8142\n","Epoch: 954, Train Loss: 0.0904, Train Acc: 0.9584, Val Loss: 1.1149, Val Acc: 0.8124\n","Epoch: 955, Train Loss: 0.0889, Train Acc: 0.9589, Val Loss: 1.0479, Val Acc: 0.8129\n","Epoch: 956, Train Loss: 0.0878, Train Acc: 0.9590, Val Loss: 1.1123, Val Acc: 0.8112\n","Epoch: 957, Train Loss: 0.0889, Train Acc: 0.9594, Val Loss: 1.0269, Val Acc: 0.8113\n","Epoch: 958, Train Loss: 0.0888, Train Acc: 0.9588, Val Loss: 1.0740, Val Acc: 0.8118\n","Epoch: 959, Train Loss: 0.0881, Train Acc: 0.9591, Val Loss: 1.0804, Val Acc: 0.8094\n","Epoch: 960, Train Loss: 0.0888, Train Acc: 0.9589, Val Loss: 1.1313, Val Acc: 0.8075\n","Epoch: 961, Train Loss: 0.0950, Train Acc: 0.9575, Val Loss: 1.0708, Val Acc: 0.8101\n","Epoch: 962, Train Loss: 0.0887, Train Acc: 0.9585, Val Loss: 1.0972, Val Acc: 0.8095\n","Epoch: 963, Train Loss: 0.0882, Train Acc: 0.9591, Val Loss: 1.0718, Val Acc: 0.8108\n","Epoch: 964, Train Loss: 0.0881, Train Acc: 0.9595, Val Loss: 1.1069, Val Acc: 0.8125\n","Epoch: 965, Train Loss: 0.0907, Train Acc: 0.9590, Val Loss: 1.0511, Val Acc: 0.8114\n","Epoch: 966, Train Loss: 0.0868, Train Acc: 0.9596, Val Loss: 1.0854, Val Acc: 0.8095\n","Epoch: 967, Train Loss: 0.0884, Train Acc: 0.9591, Val Loss: 1.1178, Val Acc: 0.8110\n","Epoch: 968, Train Loss: 0.0877, Train Acc: 0.9592, Val Loss: 1.1085, Val Acc: 0.8102\n","Epoch: 969, Train Loss: 0.0901, Train Acc: 0.9588, Val Loss: 1.0663, Val Acc: 0.8117\n","Epoch: 970, Train Loss: 0.0889, Train Acc: 0.9590, Val Loss: 1.0234, Val Acc: 0.8124\n","Epoch: 971, Train Loss: 0.0889, Train Acc: 0.9587, Val Loss: 1.0254, Val Acc: 0.8101\n","Epoch: 972, Train Loss: 0.0879, Train Acc: 0.9595, Val Loss: 1.1353, Val Acc: 0.8091\n","Epoch: 973, Train Loss: 0.0894, Train Acc: 0.9582, Val Loss: 1.0792, Val Acc: 0.8090\n","Epoch: 974, Train Loss: 0.0879, Train Acc: 0.9585, Val Loss: 1.1221, Val Acc: 0.8102\n","Epoch: 975, Train Loss: 0.0878, Train Acc: 0.9596, Val Loss: 1.0890, Val Acc: 0.8130\n","Epoch: 976, Train Loss: 0.0916, Train Acc: 0.9580, Val Loss: 1.0368, Val Acc: 0.8104\n","Epoch: 977, Train Loss: 0.0882, Train Acc: 0.9594, Val Loss: 1.0907, Val Acc: 0.8104\n","Epoch: 978, Train Loss: 0.0878, Train Acc: 0.9588, Val Loss: 1.1078, Val Acc: 0.8103\n","Epoch: 979, Train Loss: 0.0891, Train Acc: 0.9588, Val Loss: 1.0986, Val Acc: 0.8095\n","Epoch: 980, Train Loss: 0.0889, Train Acc: 0.9579, Val Loss: 1.0894, Val Acc: 0.8128\n","Epoch: 981, Train Loss: 0.0865, Train Acc: 0.9600, Val Loss: 1.1174, Val Acc: 0.8071\n","Epoch: 982, Train Loss: 0.0885, Train Acc: 0.9588, Val Loss: 1.0210, Val Acc: 0.8108\n","Epoch: 983, Train Loss: 0.0870, Train Acc: 0.9597, Val Loss: 1.0667, Val Acc: 0.8121\n","Epoch: 984, Train Loss: 0.0881, Train Acc: 0.9591, Val Loss: 1.0943, Val Acc: 0.8089\n","Epoch: 985, Train Loss: 0.0894, Train Acc: 0.9581, Val Loss: 1.1422, Val Acc: 0.8093\n","Epoch: 986, Train Loss: 0.0877, Train Acc: 0.9592, Val Loss: 1.1011, Val Acc: 0.8118\n","Epoch: 987, Train Loss: 0.0883, Train Acc: 0.9592, Val Loss: 1.0811, Val Acc: 0.8102\n","Epoch: 988, Train Loss: 0.0877, Train Acc: 0.9589, Val Loss: 1.0792, Val Acc: 0.8104\n","Epoch: 989, Train Loss: 0.0873, Train Acc: 0.9596, Val Loss: 1.1116, Val Acc: 0.8118\n","Epoch: 990, Train Loss: 0.0888, Train Acc: 0.9588, Val Loss: 1.0908, Val Acc: 0.8101\n","Epoch: 991, Train Loss: 0.0874, Train Acc: 0.9598, Val Loss: 1.1474, Val Acc: 0.8099\n","Epoch: 992, Train Loss: 0.0891, Train Acc: 0.9591, Val Loss: 1.0660, Val Acc: 0.8100\n","Epoch: 993, Train Loss: 0.0900, Train Acc: 0.9586, Val Loss: 1.0622, Val Acc: 0.8112\n","Epoch: 994, Train Loss: 0.0882, Train Acc: 0.9595, Val Loss: 1.1113, Val Acc: 0.8122\n","Epoch: 995, Train Loss: 0.0900, Train Acc: 0.9580, Val Loss: 1.0697, Val Acc: 0.8105\n","Epoch: 996, Train Loss: 0.0885, Train Acc: 0.9589, Val Loss: 1.0630, Val Acc: 0.8089\n","Epoch: 997, Train Loss: 0.0881, Train Acc: 0.9595, Val Loss: 1.0920, Val Acc: 0.8108\n","Epoch: 998, Train Loss: 0.0874, Train Acc: 0.9593, Val Loss: 1.0583, Val Acc: 0.8097\n","Epoch: 999, Train Loss: 0.0881, Train Acc: 0.9594, Val Loss: 1.0701, Val Acc: 0.8110\n","Epoch: 1000, Train Loss: 0.0903, Train Acc: 0.9582, Val Loss: 1.0688, Val Acc: 0.8108\n"]}],"source":["view_line('logs/res4.log')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Model assess"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["name_acc_list = {\n","    \"name\": [],\n","    \"acc\": []\n","}\n","\n","def model_assess(model, name='Default'):\n","    \n","    if name == \"BERT\":\n","        max_length = 64 \n","        loss_fn = nn.CrossEntropyLoss()\n","        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        \n","        val_dataset = TextDataset(X_val2, y_val2, tokenizer, max_length)\n","        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False) # If your GPU memory is not enough, reduce the batch_size\n","        \n","        _, val_acc = eval_bert_classifier(model, val_loader, loss_fn, device, len(val_dataset))\n","        val_acc = val_acc.item() * 100 \n","        name_acc_list[\"name\"].append(name)\n","        name_acc_list[\"acc\"].append(val_acc)\n","        \n","        print(f'Model: {name}, Accuracy: {val_acc}%')\n","        \n","    \n","    elif name == \"TextResNet\":\n","        criterion = nn.CrossEntropyLoss()\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","        X_val_tensor = torch.tensor(X_val, dtype=torch.float).unsqueeze(1).to(device)  \n","        y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n","        \n","        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n","        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False) # If your GPU memory is not enough, reduce the batch_size\n","        \n","        _, val_acc = evaluate_res_classifier(model, val_loader, criterion)\n","        val_acc = val_acc * 100 \n","        name_acc_list[\"name\"].append(name)\n","        name_acc_list[\"acc\"].append(val_acc)\n","        \n","        print(f'Model: {name}, Accuracy: {val_acc}%') \n","        \n","    else:\n","        model.fit(X_train, y_train)\n","        prds = model.predict(X_val)\n","        acc = 100 * accuracy_score(y_val, prds)\n","        name_acc_list[\"name\"].append(name)\n","        name_acc_list[\"acc\"].append(acc)\n","        print(f'Model: {name}, Accuracy: {acc}%')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**DecisionTree** and **RandomForest** implemented by sklearn"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: DT, Accuracy: 65.17372966459128%\n","Model: RF, Accuracy: 79.35997857668876%\n"]}],"source":["model_0 = DecisionTreeClassifier()\n","model_assess(model_0, \"DT\")\n","\n","model_1 = RandomForestClassifier()\n","model_assess(model_1, \"RF\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**MLP** \n","The fitting requires time. You can directly use the `mlp_classifier.pth` I trained before."]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: MLP, Accuracy: 73.82339157796078%\n"]}],"source":["# NOTICE: this code will fit again, so it will take some time, if you want to save time, you can skip this cell\n","model_2 = ManualModel(X_train.shape[1])\n","model_assess(model_2, \"MLP\")"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: MLP, Accuracy: 73.89033942558747%\n"]}],"source":["# directly use the model fitted in the previous section\n","model_2 = ManualModel(X_train.shape[1])\n","model_2.model.load_state_dict(torch.load(\"models/mlp_classifier.pth\"))\n","prds = model_2.predict(X_val)\n","acc = 100 * accuracy_score(y_val, prds)\n","name_acc_list[\"name\"].append('MLP')\n","name_acc_list[\"acc\"].append(acc)\n","print(f'Model: MLP, Accuracy: {acc}%')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Bert** and **TextResNet**:\n","We assess the model I trained before, because retraining takes a lot of time \\\n","If your GPU memory is not enough, reduce the batch_size as I write in function `model_assess`"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Evaluating: 100%|| 934/934 [00:37<00:00, 25.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Model: BERT, Accuracy: 93.13784561826338%\n","Model: TextResNet, Accuracy: 81.03907922605886%\n"]}],"source":["# BERT\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","model_3 = BertClassifier(4).to(device)\n","model_3.load_state_dict(torch.load(\"models/bert_classifier.pth\"))\n","model_assess(model_3, \"BERT\")\n","del model_3\n","torch.cuda.empty_cache()\n","\n","# TextResNet\n","model_4 = TextResNetClassifier(1, 4).to(device)\n","model_4.load_state_dict(torch.load(\"models/resnet_classifier.pth\"))\n","model_assess(model_4, \"TextResNet\")\n","del model_4\n","torch.cuda.empty_cache()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Visualize the assess results"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 400x400 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["def plot_acc():\n","    plt.rcParams['figure.figsize']=4,4\n","    sns.set_style('darkgrid')\n","    ax = sns.barplot(x=name_acc_list[\"name\"], y=name_acc_list[\"acc\"], palette=\"coolwarm\", saturation=2.0)\n","    plt.xlabel('Classifier Models', fontsize=12)\n","    plt.ylabel('% of Accuracy', fontsize=12)\n","    plt.title('Accuracy of different Classifier Models', fontsize=16)\n","    plt.xticks(fontsize=12, horizontalalignment='center')\n","    plt.yticks(fontsize=12)\n","    for i in ax.patches:\n","        width, height = i.get_width(), i.get_height()\n","        x, y = i.get_xy() \n","        ax.annotate(f'{round(height,2)}%', (x + width/2, y + height), ha='center', fontsize='x-large')\n","    plt.show()\n","    \n","plot_acc()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","### **Tune model for good performance on validation set.**\n","\n","**Note:** you should only tune your model on the validation set, and keep the test data **unseen until the model is selected**.\n","\n","Now it is your time to provide the final solution.\n","\n","> Requirements\n","- Tune model's hyperparameters evaluate all your selected models. And give a detailed report on the **performance** and **computational efficiency**.\n","- Evaluate your final model on test set, and report the final result.\n","- It is appreciated if other machine learning techniques that help to improve performance are employed."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Tune models for classification"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Bert\n","Refer to the log file and the training experience, I found that the following set of parameters works well:\n","\n","> max length of tokens: 64,\\\n","batch size = 64, \\\n","epochs = 75, \\\n","learning rate = 2e-5,\\\n","dropout rate = 0.1\n","\n","The training techniques were used include: Dropout, ADAM.\n","\n","The training requires high GPU performance and time. Each epoch takes about 3 minutes on `NVIDIA A100 80GB PCIe`, so the whole training process takes about 4 hours. The GPU memory usage is approximately 5641 MiB.\n","\n","You can **SKIP** the following TRAINING step and directly use the `bert_classifier.pth` I trained before\n"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Training: 100%|| 1868/1868 [10:09<00:00,  3.07it/s]\n","Evaluating: 100%|| 467/467 [00:54<00:00,  8.62it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1, Train Loss: 0.0266, Train Acc: 0.6572, Val Loss: 0.0169, Val Acc: 0.7974\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# **NOTICE:** You can skip this cell if you want to save time\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Set up tokenizer, use to convert text to tokens\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  # lowercase text\n","\n","# Hyperparameters\n","max_length = 64 # max length of tokens\n","batch_size = 64 \n","num_classes = 4 \n","num_epochs = 75\n","learning_rate = 2e-5\n","\n","# Create model, optimizer, and loss function\n","model = BertClassifier(num_classes).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Create Dataset\n","train_dataset = TextDataset(X_train2, y_train2, tokenizer, max_length)\n","val_dataset = TextDataset(X_val2, y_val2, tokenizer, max_length)\n","\n","# Create DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  # usually we don't shuffle validation and test sets\n","\n","# Train and evaluate the model\n","for epoch in range(num_epochs):\n","    train_loss, train_acc = train_bert_classifier(model, train_loader, loss_fn, optimizer, device, len(train_dataset))\n","    val_loss, val_acc = eval_bert_classifier(model, val_loader, loss_fn, device, len(val_dataset))\n","    print(f\"Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","# Save the trained model\n","torch.save(model.state_dict(), \"models/bert_classifier.pth\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### TestResNet \n","Refer to the log file and the training experience, I found that the following set of parameters works well:\n","\n","> batch size = 32, \\\n","learning rate = 0.001, \\\n","epochs = 890,  \\\n","dropout rate = 0.5, \\\n","kernel size = 9, \\\n","num channels = 256, \n","\n","\n","The training techniques were used include: Batch Normalization (BN), Dropout, ADAM\n","\n","The training requires high GPU performance and time. \n","Each epoch takes about 30 seconds on `NVIDIA A100 80GB PCIe`, so the whole training process takes about 7-8 hours. The GPU memory usage is approximately 1881 MiB.\n","\n","You can **SKIP** following TRAINING step and directly use the `resnet_classifier.pth` I trained before"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1, Train Loss: 1.2714, Train Acc: 0.4335, Val Loss: 1.1827, Val Acc: 0.4818\n","Epoch: 2, Train Loss: 1.1731, Train Acc: 0.4862, Val Loss: 1.1724, Val Acc: 0.4804\n"]}],"source":["# **NOTICE:** You can skip this cell if you want to save time\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Convert data into tensors\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float).unsqueeze(1).to(device)  # add dimension at position 1\n","y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n","X_val_tensor = torch.tensor(X_val, dtype=torch.float).unsqueeze(1).to(device)  # add dimension at position 1\n","y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n","\n","# Create datasets and data loaders\n","batch_size = 32\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n","\n","# Initialize model\n","num_classes = 4\n","input_dim = 1\n","model = TextResNetClassifier(input_dim, num_classes).to(device)\n","\n","# Set up training\n","learning_rate = 0.001\n","num_epochs = 890\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Train the model\n","for epoch in range(num_epochs):\n","    train_loss, train_acc = train_res_classifier(model, train_loader, optimizer, criterion)\n","    val_loss, val_acc = evaluate_res_classifier(model, val_loader, criterion)\n","    print(f\"Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","torch.save(model.state_dict(), \"models/resnet_classifier.pth\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### MLP \n","Refer to the training experience, I have tune the parameters as the shown in ManualModel class:\n","\n","> batch size = 64, \\\n","Initial learning rate = 0.001, decay every 200 epochs\\\n","epochs = 400,  \\\n","dropout rate = 0, \\\n","hidden dimension = 128,\\\n","weight decay=1e-5     \n","\n","The training techniques were used include:\n","learning rate decay, wight decay, ADAM\n","\n","Each epoch takes about 1.5 seconds on `NVIDIA A100 80GB PCIe`, so the whole training process takes about 10 minutes. The GPU memory usage is approximately 1317 MiB.\n","\n","The training requires time. You are recommended to use the `mlp_classifier.pth` I trained before."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Evaluate final model on test set \n","When using the test set evaluation model, you may encounter the situation that the GPU memory is insufficient. \\\n","In this case, please adjust `batch_size` according to the comments below. `batch_size` can be reduce to 1\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["name_eval_list = {\n","    \"name\": [],\n","    \"acc\": []\n","}\n","\n","def model_eval(model, name='Default'):\n","    if name == \"BERT\":\n","        max_length = 64\n","        loss_fn = nn.CrossEntropyLoss()\n","        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        \n","        test_dataset = TextDataset(X_test2, y_test, tokenizer, max_length)  \n","        test_loader = DataLoader(test_dataset, batch_size= 16, shuffle=False)  # If your GPU memory is not enough, reduce the batch_size\n","        \n","        _, acc = eval_bert_classifier(model, test_loader, loss_fn, device, len(test_dataset))\n","        acc = acc.item() * 100 \n","        name_eval_list[\"name\"].append(name)\n","        name_eval_list[\"acc\"].append(acc)\n","        print(f'Model: {name}, Accuracy: {acc}%')\n","        \n","    elif name == \"TextResNet\":\n","        criterion = nn.CrossEntropyLoss()\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        \n","        X_test_tensor = torch.tensor(X_test, dtype=torch.float).unsqueeze(1).to(device)  \n","        y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n","        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # If your GPU memory is not enough, reduce the batch_size\n","        \n","        _, acc = evaluate_res_classifier(model, test_loader, criterion)\n","        acc = acc * 100 \n","        name_eval_list[\"name\"].append(name)\n","        name_eval_list[\"acc\"].append(acc)\n","        print(f'Model: {name}, Accuracy: {acc}%') \n","         \n","    else:\n","        prds = model.predict(X_test)\n","        acc = 100 * accuracy_score(y_test, prds)\n","        name_eval_list[\"name\"].append(name)\n","        name_eval_list[\"acc\"].append(acc)\n","        print(f'Model: {name}, Accuracy: {acc}%')"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: DT, Accuracy: 84.2%\n","Model: RF, Accuracy: 90.10000000000001%\n","Model: MLP, Accuracy: 85.7%\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Evaluating: 100%|| 63/63 [00:02<00:00, 22.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Model: BERT, Accuracy: 97.3%\n","Model: TextResNet, Accuracy: 89.18650793650794%\n"]}],"source":["# Decision Tree, Random Forest, MLP\n","model_eval(model_0, \"DT\")\n","model_eval(model_1, \"RF\")\n","model_eval(model_2, \"MLP\")\n","\n","# BERT, TextResNet\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","model_3 = BertClassifier(4).to(device)\n","model_3.load_state_dict(torch.load(\"models/bert_classifier.pth\"))\n","model_eval(model_3, \"BERT\")\n","del model_3\n","torch.cuda.empty_cache()\n","\n","model_4 = TextResNetClassifier(1, 4).to(device)\n","model_4.load_state_dict(torch.load(\"models/resnet_classifier.pth\"))\n","model_eval(model_4, \"TextResNet\")\n","del model_4\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAaYAAAGNCAYAAABaGtX2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAABxqUlEQVR4nO3ddVhU2f/A8TegoJSiKAa2DgYKFhjYhYW5dvtdu1vXLuyuNVldWzHWRV1rVeyOdQ0UFbAFlFARmN8f/GaWkSEGQQf5vJ7HZ5d7z9x7ztyZ+dwT9xwDpVKpRAghhNATht87A0IIIURsEpiEEELoFQlMQggh9IoEJiGEEHpFApMQQgi9IoFJCCGEXpHAJIQQQq9IYBJCCKFXJDAJkQbIc/AiPdH7wPTs2TNKlCiBnZ0dM2bM+N7ZEfEIDg5m5MiRODs7U7p0aerWrUtkZGSyjtW+fXvs7Oy4cOGCetuYMWOws7Nj586dGmnv3r1L586dKVu2LGXLlmX48OEAnD9/ntatW+Pg4ED58uWZM2dO8gv3Hb1//57p06ezd+9enV8bERGBl5cXvXv3pk6dOpQuXZoKFSrQvn17fvvtNz59+hTnNfG9z/qidu3a2NnZ8eTJE43tXl5eNG3alDJlyuDk5MSmTZu+a1mWLl2KnZ0ddnZ2NG3aNNH0f/75pzr9iBEjUi1fCxcuxM7OjqVLlyb7GP7+/tjZ2VG9evUUzJkmvQ9Mnp6eREdHY2Jiwr59+/j48eP3zpLQYubMmezfvx8jIyNq1apFlSpVyJAhQ6qeU6lU0rdvXy5evEju3LmpXr06jo6OhISE0K9fP27dukXRokVxcXHB3t4+VfOSWtzd3dm0aZPOQd7Hx4eWLVsydOhQLly4gLW1NbVq1aJo0aLcvn2bmTNn0qJFC169epVKOf92fH19GTFiBPfv36dMmTJUqlQJhULxvbOldv/+fXx9fRNM8+eff36j3KQNqfvL8ZWUSiWenp6YmprSqlUrNm3ahJeXFy1btvzeWRNfuHHjBhBzR+bs7Jzixx82bBg///wzOXPmVG97/fo1z549I1OmTHh6epIpUyYArl27RlhYGLa2tuzatQsDA4MUz8+3kpwmvCdPntCmTRvCwsLo3Lkz/fv3x8rKSr3/xYsXjB07lrNnz9K1a1d2796NqalpSmY71Xh4ePD582fy5Mmj3vbPP/8QFRWlrimpFCpUKM5n5luztLTk/fv3HDp0iL59+2pNExoayunTp8mYMSOfP3/+xjnUT3pdYzp//jwBAQFUqlRJXR3etm3bd86V0Eb1hcqVK1eqHD9nzpwUKVIECwsL9baIiAgAsmTJog5KsbfnzJkzTQel5FAqlQwfPpywsDB69+7N+PHjNYISxFyj5cuXU7BgQR49esSOHTu+U251lz9/fooUKULGjBnV21TX28bGRiOtts/Mt1anTh0MDAw4dOhQvGmOHDlCREQE1apV+4Y50296HZh2794NgKurKw4ODhQsWJAbN27w77//ak2vVCrZtWsX7du3p2LFijg5OdGuXTsOHDig9c7zyJEjdO/enUqVKlGuXDlatmzJ5s2bNe5aVG3FCxcujPP6+Npa7ezsaNasGRcvXsTV1ZXSpUtTv359Hj9+DMC7d+9YunQpLVu2pHz58tjb2+Pi4sKgQYO4efOm1rI9efKEiRMnUrt2bcqUKUO9evWYMGECz58/B2K+nM7OztjZ2cXbbNCzZ0/s7Oy4ePGi1v2xffz4kZUrV6rb7cuVK0eHDh3Yv3+/RjpVO35AQAAA9evXj9M/FJ/bt2/Tr18/KleuTNmyZfn555+5f/++1rRf9hfUrl2bOnXqAPDy5Ut1+7ydnR1dunQB4OrVq9jZ2VG7dm2NY509e5aff/5Z3R/WsGFDli5dSnh4uEY61fXt168fBw8epFatWpQuXZqmTZsSEhKiTufl5UXnzp0pX748Dg4ONGvWTH1nH9uFCxfUfaWPHj1i0KBBODs7U6ZMGVq2bKn+vKvY2dmxZ88eAMaPH4+dnR2enp4JvqdXrlzh1q1b5MiRg379+sWbztTUlL59+1K+fPkkBW9dP7MvX75k4sSJNGzYUN3v06VLlzifH4jpR5s9ezZNmzbF0dGR8uXL065dOzZv3hynCTN2H5Pq+owdOxaAP/74Azs7Ozp37gwk3F+WnGu2adMmqlSpgoODAx06dCA6OjrR983GxgZHR0fu3r0bp19M5c8//8TCwoIaNWrEexwfHx9GjRpFtWrV1O/9yJEj8fHx0Zr+1atXTJkyhVq1aqk/X0ePHk0wr7dv32bQoEFUrlwZe3t76tSpg7u7O4GBgYmWE3S7jonR26a8kJAQjhw5gpmZGfXr1wegefPmLFq0iK1btzJ16lSN9FFRUQwcOJBjx45hampKxYoVgZgP1vDhw/nnn38YPXq0Ov20adP4/fffyZgxIxUqVCBz5sxcvnyZqVOncunSJRYuXPhVd9tv376lb9++5MmTBxcXF/z9/SlQoABv376lXbt2PH36FFtbWypVqsTnz5/5559/OHz4MMePH2fr1q2ULl1afazz58/Tv39/QkNDKVasGDVr1sTHx4cdO3Zw7NgxduzYga2tLW5ubmzcuJG9e/cydOhQjfy8fPmSc+fOkT9/fvV7E5+goCC6dOnC/fv3yZo1K9WqVePjx49cvHiRK1eucObMGWbNmoWBgQFly5YlMjKSY8eOER4eTp06dTA1NcXa2jrBc5w8eZIBAwYQERGBo6MjNjY2XLlyhfbt22Nubp7o+1u3bl0CAgI4evQomTNnpm7duhrv/dmzZ8mWLRtVq1YlW7Zs6n2rV69m/vz5ZMyYEXt7e3LkyMH169dZtmwZx44d47fffiNLliwa57p37x4nT56kVKlSFC1alMjISPVd+MSJE9m+fTuZMmWiTJkyWFhYcOXKFdzd3Tl16hSrVq3C2NhY43g+Pj789NNPmJiYULZsWYKDg7l27Rrjxo3j3bt39OjRA4CmTZty/fp1/Pz8cHR0JF++fOTPnz/B98XLy0v9/sSuRWrTvHlzmjdvnvAb/f/vpy6f2bdv39K6dWtevXqFQqGgZs2avHv3jkuXLnHhwgWePHnCwIEDgZgboI4dO3L//n3y58+Pi4sLHz584NKlS1y7do1bt24xa9YsrfkyNTWladOm+Pv7c+3aNWxtbSlbtixFihRJsDzJuWanTp3iyZMnODk5YWBgQJ48eTA0TNp9fcOGDbl27RqHDh2id+/eGvuCgoI4d+4cTZs2jXNOlePHjzNkyBA+ffqEnZ0d5cqVw9fXl/379/PXX3+xaNEiatWqpU7v7+9Pp06deP78OQULFqRmzZo8fPiQ/v37U7RoUa3n2LdvH+PGjSMqKopSpUqRN29e/v33Xzw8PDhy5AgbN27E1tY23jJ+zXXUSqmntmzZolQoFMpffvlFve3FixfKEiVKKB0dHZUhISEa6Tds2KBUKBTKJk2aKF+8eKHe/vTpU2WVKlWUCoVCefv2baVSqVT+9ddfSoVCoaxWrZrSx8dHnfbt27dKV1dXpUKhUB4+fFipVCqVS5YsUSoUCuWCBQvi5NHPz099nNgUCoVSoVAoe/furYyOjlYqlUplVFSUUqlUKqdNm6ZUKBTKqVOnqvcplUrlx48flX379o1T5tDQUGW1atWUCoVCuWnTJvX26Oho5ezZs9XnUSqVyn///VepUCiUtWrV0ji2UqlUrl69WqlQKJTLly+P9z1XGTBggFKhUCj79OmjDA0NVW9//Pixsk6dOkqFQqHcuHGjxmtq1aqlVCgUysePHyd6/NDQUGXVqlWVdnZ2yj/++EO9PSwsTNmjRw/1+3f+/Hn1vtGjRysVCoVyx44d6m3xvf/nz59XKhQKZbt27TS2nzt3TmlnZ6esXr268t9//1Vv//Tpk3LMmDFKhUKhHD58eJzjKxQK5ZQpU9TbVddy9+7dSoVCoWzatKnSz89PvT8kJERdjtifG1W+FAqFcsiQIcqwsDD1vo0bNyoVCoWycuXKGtdOW7kT0rVrV6VCoVB6enomKf2XtJ1P18/ssmXLlAqFQjl//nyNY9+4cUNZqlQpZZkyZZQfPnxQKpVK5Z49e9Tve+xjP3nyRFmxYkWlQqFQPn36VL1d2+dMdR1iX7v4yvI112zNmjXq7arPQHxi/268ePFCWbx4cWXz5s3jpNu2bZtSoVAoT506pbUcr169Ujo6Oirt7OziXNOdO3cq7ezslGXLltX4zevdu7dSoVAoJ0+erIyMjFQqlTG/F4sWLVKXZcmSJer0Dx8+VNrb2yvLli2rvHDhgkYZFyxYoFQoFMr27durt2v73ul6HROjt015qiaL2AMdbGxscHFxITw8PE6TwNatWwGYMWOGRltzvnz56NOnDwqFgkePHgGwZcsWIKaqH/vuKlu2bAwfPpzChQvz7Nmzry5Dx44d1bUu1d1VlixZqFatGoMGDdKokZmYmKjL6u/vr95+7NgxXr58SfXq1enUqZN6u4GBAUOGDKFYsWJEREQQGRlJ8eLFsbe3JyAgIE5z3d69ezE0NKRFixYJ5jkgIIC//voLCwsL5s6di5mZmXpfgQIFmDlzJgBr165NzlsCwNGjR3n9+jV169alSZMm6u2mpqbMmjVLo/8gJa1duxalUskvv/xC8eLF1duNjY2ZNGkS1tbWeHl58fLlyzivjf3eq67lmjVrgJiRc7HvJs3NzXF3dydjxoxs3rxZ3QeikiFDBiZNmqQx4KBt27YYGxvz9u1b3r59m+wyvn79GoDs2bMn+xhf0vUzq8pD7AEKAGXKlGH69OnMnDlT3QymSps7d26NY+fPn5+ZM2cyZ84cjc/g10ruNTMyMqJDhw7qv5NaW4KY361y5cpx584dnj59qrHPy8uLbNmyUblyZa2v3b59O+Hh4bRo0SLOd7d169a0aNGCsLAw9e/fixcvOHHiBNbW1owdOxYjIyMg5vdi0KBB2NnZxTnHb7/9RkREBAMHDsTJyUmjjEOGDMHOzo4rV65w/fr1eMuY0tdRLwPTgwcPuHnzJoUKFaJcuXIa+1q3bg1oDoJ4+fIljx8/xtramjJlysQ5XufOnfnjjz9o2rQpSqWSS5cuYWhoqFH9Valbty4HDx6kW7duX12O2D9+KgMHDmTt2rUazUWqZg5vb28AjS+FKsB82U8CMT+oBw4cYP369eqh2a1atQLQeO7l1q1b+Pj4ULlyZXLnzp1gni9dugRAlSpVtDapOTk5kSNHDl68eBHnS5ZUqnNoew4iR44cODg4JOu4CYmKiuLy5csAWkcNZsqUiQoVKmiki72vUKFCGttev37No0ePsLCwoFSpUnGOlzNnTooXL05ISAh37tzR2FegQAGyZs2qsc3Y2Fg9SOHDhw86l09F9TmIiopK9jG+pOtnVtVUPGPGDMaNG8eRI0cIDQ0FYpoPGzdurA7KqrRr165lyJAh/PHHH+o+jbp169KsWTONptiv8TXXLH/+/F81crFhw4YAGoMg3rx5w6VLl2jQoEG8j1aovisNGjTQur9Ro0bAf78Tqv9WqlQpTtOggYGBul82NlV/sLbvhYGBAS4uLhrH1ialr6Ne9jGpOoFDQ0PVHZkqqs7Je/fucfXqVcqVK6d+FiOxH12IadP9/Pkz2bJlI3PmzCmcc01f9lWo+Pn5sWXLFq5cucLjx4959+4dgPpOQxlroEZ8d5/xadq0KbNnz+bw4cNMmjSJTJkyqYNUUobZq97LvHnzxpvG1taW169f8+rVq0T7PBI6x5ejqGIf/8vg8LWCg4PVP/ix7wq1UQ0oUbGwsIjT36hKExISovUu9Mu0jo6O6r8tLS21plP9OCWlUz0+OXLk4O7du0nusE4qXT6zjRs35vbt23h4eLB79252795NhgwZKFu2LA0bNqRVq1bq/i9HR0fGjh3L/PnzOXjwIAcPHsTAwIBSpUrRoEED2rZtG+/3SFdfc82+vJHQVYMGDZgxYwaHDh2iV69eABw8eJCoqCgaN24c7+sS+z6qan2q34mkfLe+pHpfEmtN+fJ7EVtKX0e9C0yfP39WN9O9fv1a/YZrs23bNsqVK6fT3WFK3kkm9gOirbp/4MABRo8eTWRkJPny5aNy5coULlwYe3t7lEol/fv310iv62gWCwsL6tevz/79+zl27BgNGjTgwIEDWFpaUq9ePZ2OFR/VexhfZ21iEhtUomp+SEmx8xzf3adKgQIFNP7Wdh1Vx1MNDklIjhw5NP5OzSHspUqV4vTp09y4cUNde45PaGgoK1euxMnJiSpVqsTbhKrrZxZg9OjRdOrUiSNHjnD69GmuXr3KpUuXuHTpEps2bWLr1q3qGmK3bt1o2rQpR44c4dSpU1y6dInbt29z+/ZtfvvtN7Zs2RLnmiTH97xmOXLkoEKFCly8eBE/Pz/y5cuHl5cXNjY2VKhQId7XKRN5jk31G6T6Libnu6V6Xxo3bpxgE6W2FqDYUvI66l1gOnnyJG/fvsXR0ZHt27drTXPr1i1at27NoUOHGDdunPpD9OLFC63p37x5w7FjxyhevDglS5YkY8aMvHv3jo8fP8YZufTp0yd27dpF4cKFqVy5svpCawtC79+/16lsYWFhTJw4EaVSyfLlyzVGkkHM8PUvqR4OjK9sR48eJSIiAhcXF/WdeKtWrdi/fz+HDx/GysqK4OBg2rVrh4mJSaJ5VJ1PNfxbG1V/QmIj7+KjupuL7xypMRtB1qxZyZgxI5GRkcycOTPZQVVF9ZkzMTFh3rx5KZHFFFGvXj1WrVrFiRMn+PTpU4LX/K+//mLt2rXs2rWLM2fOaE2TnM+sSt68eenWrRvdunXj8+fPnDt3jmnTpuHr68vWrVs1hrNnz56ddu3a0a5dO6Kjo7l69Sru7u7cvn2b1atXp8h0ZN/7mjVs2JCLFy9y6NAhmjZtyrVr1+jWrVuCwSRnzpz4+voSEBBAsWLF4uz38/MD/utTVH234usj1/bdypkzJwEBAQwePPirbwBS6jrqXR/Trl27ADQ6xb9UunRpihYtyqdPn/D09CRv3rzY2Njw+vVrrc84HTlyhIkTJ3LgwAEyZsxI6dKliYqKUrePx3bhwgWmTp2qfoJc1WGnrUNaNdtBUj148ICwsDAUCkWcLzigzk/suyRVH9vJkyfjpI+KimLKlCmMGDFCo2bl7OxMvnz5OH36NAcOHABI9O5ZRfVcy5kzZ9T9ArGdP3+ewMBAbG1tk9y8+CVVR6+2H7WQkBCuXLmSrOMmxNjYGEdHR6Kjozl9+nSc/Uqlks6dO9OuXbt4nyWLTVX+ly9fcvfu3Tj7P3z4gJubGx07dtQYGKArXe/U7e3tcXJy4tWrV6xcuTLedMHBwer9bdq0ibePIzmf2SFDhuDs7Kxx45ExY0aNATyqGy13d3dcXFzUfSkQU0OtUKGCeqaE+G7KdPWtrll8GjRogJGREYcPH+bQoUMolUp1H1F8VH03hw8f1rr/4MGDwH/N05UqVcLQ0JCzZ88SFhYWJ7223xHVObTtAxg+fDitWrXi2LFj8eYzpa+jXgWm169fc/r0aYyMjNSdhfFRPX+xfft2lEql+gM/fvx4jfZ1Pz8/li9fjqGhoXr2CFVad3d3jQ9gYGCgerJPNzc34L/q69GjRzW+aA8ePEjwi6+NqvPP19dXPUIQYr7UW7duVT+BH3tyzUaNGmFlZcWxY8c0Hq5UKpUsXLiQV69eUa1aNY2ORQMDA1q2bEl4eDh79uyhaNGiWgeFaJMvXz7q1KlDaGgoI0eO1Phw+/n5MX78eEBzlJquateuTf78+Tl79iweHh7q7REREYwfPz7Og64pRTWgZdq0aRqd29HR0SxatIiLFy/i7++faJOFSteuXQEYNWqUxkCQiIgIJk+ezL179wgPD0/w+Y/EqGp2sR/oTczkyZPJnDkzK1euZNasWer+IBU/Pz/69OnD06dPyZ8/f5xna2JLzmc2R44cBAcHM2fOHI1BER8/flTfjKieecqdOzevX79mwYIFGjdCkZGR6h/d2M/0fa1vcc3ikz17dpycnLh9+zbbtm0jf/78iX4v27Rpg6mpKXv27FE/bK2ye/du9u3bh6mpqbp/yNramsaNG/Pu3Tt++eUXjfffw8NDa99t586dMTIyYvHixZw7d05j39atWzlw4AAPHjxIcFBSSl9HvWrK27t3L5GRkVStWjXRZiI3NzcWLFjA48ePOX/+PD169ODSpUucOnWKevXq4eTkREREBJcvX+bjx48MGjRI/SFo3Lgx586dY+fOnTRq1AgnJyeMjIy4cuUKISEhtGrVCldXVyCm9mFvb8/t27dp2rQpzs7OhIeHc+nSJVxcXJJ0d62SP39+ateuzfHjx2nevDlOTk6YmJhw584dnj17RtGiRfHx8eHNmzfq15iZmTF//nz69evH2LFj2bRpE/ny5VNPDJkzZ06mT58e51wtW7Zk6dKlREdH6zy34NSpU3n8+DHHjx+nTp06VKhQgQ8fPnDx4kUiIiJo0qSJ+gueHKqmlP/973+4u7uzd+9e8ufPz82bNwkMDKRkyZJxRkWlhLp169KjRw/Wr1/PTz/9RKlSpciZMyd3797Fz8+PzJkzs2TJkiQ383Xp0oUbN27g5eVFkyZNKF26NFmzZuXmzZu8evWK7Nmzs2DBgq/Kc8GCBQFYsWIF165do1mzZlprLrEVKVKE3377jd69e7Nhwwb1w6/W1ta8ePGCmzdvEhUVRdGiRfn1118TfKA5OZ/Zfv36ceLECQ4dOsSVK1fUE+jevHmTt2/fUqFCBZo1awbEzCTv5eXF1atXqV27Ng4ODhgbG6uPX7hwYbp37/5V72Fs3+KaJaRhw4acO3cOX19f+vTpk2h6GxsbZs+ezbBhwxgzZgweHh4UKlQIX19f7t69S+bMmZkzZ47G4Ihx48Zx7949Dh48yLVr13BwcMDPz487d+5QtmxZrl27pnEOe3t7xo0bx/Tp0+nWrRslS5bE1tYWX19fHjx4gJGREXPnzk3wNzmlr6Ne1ZhUNYKEmvFUbGxsqFKlChAT1TNkyMDKlSuZMGECBQoU4Ny5c1y+fJnixYszf/78OB2006dPZ+7cuZQqVYorV65w9uxZ8ubNy8SJEzV+6A0NDdmwYQNdu3bF0tKS06dP8/z5cwYOHMjy5ct17qhfuHAhgwYNwtbWlosXL3L27FmyZs3K8OHD8fT0RKFQ8OrVK27fvq1+TdWqVfH09KRp06a8fv2aY8eOERYWRtu2bdm9e3ecjlqImQ8tb968ZMiQQf0jkFTZs2dn+/btDBw4kOzZs3Pq1Clu375N2bJlWbhwIfPnz9fpOQ5tHBwc2LFjh7pMp06dIm/evHh4eCQ6YuprjB49mpUrV1KpUiUeP37MyZMnMTQ0pHXr1uzbty/O4wkJMTQ0ZMGCBcyePZvSpUtz9+5dvL29sbCwoHv37uzduzfOMHNdtW/fXt06oLoOSeHg4ICXlxcDBw7Ezs6Oe/fu8ddff/Hw4UPKlSvHxIkT2bNnT5JqBrp+Zq2srNiyZQsdOnQgU6ZMeHt7c+HCBWxsbBg5ciQbNmxQB38TExPWrVtHr169yJ49OxcuXMDb2xtTU1P69OnDzp07U2xUHnyba5aQevXqqZtNExqNF1v9+vXZtWsXTZo04e3btxw9epT379/TunVrdu/eHWdQU7Zs2diyZQu9evUiY8aMnDhxgoiICGbMmEGbNm20nqNTp05s3ryZevXqqZ+FCg8Pp1GjRuzatSvRAUMpfR0NlIkN+xBp0p07d2jRogX169f/qrVXhBDiW9OrGpP4OhEREURHR/Pu3Tt1re9r+oKEEOJ70Ks+JvF1Ll68SJ8+fYiKiiI6OpqaNWumytpIQgiRmqTG9AMpUKCAem2iRo0a6dXzNUIIkVTSxySEEEKvSI1JCCGEXpHAJIQQQq9IYBJCCKFXJDAJIdQCAwOZNGkSLi4uODg40KxZM7Zs2aIxiXHt2rWxs7NL8F/s6bPi8+zZM0aPHk3NmjVxcHCgRYsW7Ny5U+uM2itWrKBy5co4OzszevRorRMonz59muLFi2udB0+kLXo1XPzFixc0adKE5cuXawxzfvLkCe7u7ly+fBkjIyNcXV0ZOXKkxlQqYWFhzJs3j7/++ovw8HAqVKjA2LFjKVy4sM75ePs2BBkSItKboKBAevXqzrNnAZQsaU/t2vW4f/8uU6ZMwdv7LFOmzMTAwIBWrdoSGhp37r5Pnz6xdevvGBsbY2tbmDdv4p/f79Wrl/Ts2YV374KpVasOOXLk5MKFc4wfP56rV28wfPhoddq//z7O4sWLcXKqhK1tPg4c2M+7dyFMnz5b45jz5i2gVq26WFvnTfDc6Y2BAWTPbvG9s6ETvQlMz58/p2fPnnEmq3z//j1du3bF2tqaWbNmERgYyNy5c/H392fdunXqdMOHD+fGjRvqgLVs2TK6dOnCn3/+qfOUJkolEphEurN8+RKePQugdeu2DB48Qj2z+YoVi9myZRPOzlVo1Kgpbdp00Pr6RYvmEh0dzZAhIyhUqEiC36Hly5cQGPiWWbPm4+JSA4DevQcwaFAfPD130rx5KwoXLgrAgQP7KFiwEPPnL8XAwIAcOWz49ddlBAcHkyVLVgBOnjzBgwf3mDBhqnx3fwDfvSkvOjoaT09PmjdvrnVpia1btxIcHMzq1aupW7cubdq0Yd68eXh7e6uXR7h27RonTpxg1qxZtGzZkvr16+Ph4UFYWBhbtmz51kUSIs2JjIzk77+PY2mZhT59Bmost9GzZx9MTc3Yvj3+79KNG9fZvXsHFSs606RJ8wTPpVQqefPmNcWLl1QHJYhZwbdWrZgJav/55785AZ8/f0aRIkXVeSpWTKHeDjG/IWvXrqR+/YYUKFBQp3IL/fTdA9O9e/eYNGkSzZs3Vy85EZu3tzfly5fXWNbBxcUFMzMzTp06pU5jamqqXpseYiYyrFixYrxrjAgh/hOz9Hw4hQsXibN4pomJCfny5efRIx/CwuKu0QWwbNlCDA0NGTp0ZKLnMjAwYNmy1axduzHOvidPHgNgZfXf993CwoIPHz6o/1YtxWJmFtOUf/ToYfz8ntK9+8+JnlukDd+9KS937twcOXKEXLlyceHChTj7Hz58GGcxLSMjI/W07Ko0tra2cWb6zp8/P3/88YfOeUrFla+F0EsmJjHLqn/+/Fnr5z8sLBSlUsmrVy/UTWwqf/99nH///Qc3txbJqrFER0fz5s1r/vzzD/bv90ShsKNy5SrqfJQqVRpPz13cvn2DfPkKsHfvTrJntyZPnjxERUWyfv1qGjd201j6QfwnLf6efffAlDVr1gT3h4SEqFeRjc3MzEy9IFVISIjWNWXMzMy0ruKYmLTWUSjE17K2tsDW1hYfn/t8+BBMvnz51PsePHjAs2cxi2QaGUVjba35/di9exuGhob0798nzr6kGDlyJPv37wegUKFCbNiwHmtrK/X+QYP6c+bMKfr06QnELJ64YMECcuWyYseOHbx69ZJhwwarzx0dHf3Vy7KI7+u7B6bEJDRjkqrNOSlpdCGj8kR61KZNBxYsmEOvXr0ZOXIsRYsqePDgPrNnT8fExIQPHz4QHByuMeLt/v27XL16lRo1amFunj1Zo+EKFChMx45dePDgPhcvnqdNm7YsXryC3Lnz/H8KY9at28SpU38TGhqKk1MlChQoyLNnb1m2bBnNm7ciQwYz9u8/yIIFc3jx4jl2diUYO3YCRYsWS6F3J+2SUXmpwNzcXGutJzQ0FBsbG3Wa2CtoqoSFhWFhofsFkVF5Ij1q0eIn/Pz82LVrG337/k+9vX79hpQtW569e3djYpJJ47tx8OCfALi5tUz2d6Zt2/+WZvH03MmCBbOZP382c+cuVm83NTXH1fW/BUSVStizZzfv37+nc+fuBAUFM2HCGKpUqcaIEWPZtGkD48aNZPPmXeqF+UTaofdXrFChQjx9+lRjW1RUFP7+/tSvX1+dxtvbO04V/smTJxQpUuSb5leItMrAwIDBg4fTpEkzLl++gFKpxNGxHMWLl2T8+JjnimIPQgI4c+Y0lpZZKF++YorkoWXLn9ixYwsXLpzj8+fPZMyYUWu6Dx8+8PvvHrRq1RYrq2zs3LmNz58/M2LEWCwtLcme3Zpu3dpz4cI5qlatliJ5E9+O3jfEVq1alUuXLhEYGKje5u3tTXh4OFWrVgViRumFhYVx+vRpdZrAwEAuX76sTiOESJoiRYrStm1H2rXrRPHiJQG4d+9fzM3NyZEjpzrd06ePCQjwx8Wluk61kg8fPnDu3Blu3bqhdb+NTW6io6N5//5dvMfYtWsbERGf6NChMwD+/k/JkiUrlpaWAOTLlx+AgAD/JOdL6A+9D0wdOnTAxMSE7t27c+TIEXbu3MnIkSOpXr065cqVA6BixYo4OTkxcuRIdu7cyZEjR+jWrRsWFha0b9/+O5dAiLRh0qRxNG/ekKioKI3t9+/f5fnzZ1SsWElju+pZozJlHHU6T2hoCCNHDmbhwrlx9kVGRvL48SPMzMzUD8/GfX0oW7Zsom3bjlhaxjw8HxUVRVRUpDpNREQEkLw+ZvH96X1gypYtGxs3bsTKyooRI0awcOFCXF1dWbhwoUa6ZcuWUadOHebMmcOYMWOwsbHBw8ND51kfhEivChQoyJs3rzl69LB628ePH1m0KGbByY4du2qkv3//HgB2dsV1Ok+OHDkpXboM9+/f1TiXUqlkzZqVvH37BlfXxvHWwrZu3YShoQFt2/43A0X+/AV49+4dfn4xzf7//HML+K/mJNIWWShQizdvZFSeSH/CwkLp1q0Dr1+/ol69BlhZZePkyRMEBPjTq1dfevTQfIB1+PDBnDlzGi+vo3H6nmLbtm0zISEhtGvXUT0YycfnAX369CQ8PJzq1WuSO3cebt68zj//3KZEiZIsX74aU1PTOMcKCgqiVaumdO/+Pzp37qbe/ubNa9q0aUG2bNlxcanG4cOHsLS0ZOPG7XGeb0xvDAxI1jD+70kCkxYSmER6ZGhowOfPocyfP5/z588TFhaGQqGgR48e6oFGsbVr145r165x69YtjI2N4z1u7dq1CQgI4NixY9ja2qq3P378mCVLlnDmzBnCwsLImzcvjRs35ueffyZz5sxajzVr1iwOHDjAkSNH4qS5ePEiU6dO5fHjxziUKcPwEePIl69AMt+NH4cEph+EBCaRHmXIYIiVlRkRN/9GGRr8vbOTbAbmWTEuU5OgoDAiI6MTf8EPLi0GJr0fLi6E+LaUocEoQ+JOqCzEt6L3gx+EEEKkLxKYhBBC6BUJTEIIIfSKBCYhhBB6RQKTEEIIvSKBSSQoKCiIefPcadbMlTp1qtKtWwf27NlFdHTcYbgHDx6ge/cO1K3rQosWjVi6dAHh4eHJOu+DB/eoUcOZU6f+1rrfw2MtTZrUo1GjOkyfPomQkLjLLVy4cI5q1Sry4MH9ZOVBCPF9SGAS8QoKCqRXr27s3bsbG5tcNG/eCnNzc+bPn8WUKb9orIO1adMGZsyYTHS0klat2lK0aDG2b9/CsGED+Pz5s07nffv2DePHj44zZ5vKyZPHWbt2FQqFHXXq1Of48aPMnj09Tro1a1ZSq1ZdihVT6FZwIcR3Jc8xiXitWLGE588DaN26LYMHj1BPiLlixWK2bNmEs3MVGjVqyosXz1m7dhX29mVYtmy1eo6ztWtX4eGxlv37PWnVqm2SzvngwX1++WWkesVUbQ4c2EfBgoWYP38pBgYG5Mxpw6+/LuPdu2D1xJ8nT57gwYN7TJgw9eveBCHENyc1JqFVZGQkf/99HEvLLPTpM1BjluaePftgamrG9u1bANi3z5OoqCg6d+6uMfFm587dMTMz448/9iXpnCtWLKZXr668ffsmwRmrnz9/RpEiRdV5UtWInj9/BsQsrb127Urq129IgQIFdSm2+MG9exfMvHnuNG/ekJo1K9G6dVNWrFjMx48fNdLp0oSdFGfOnMbFpQIPHtzTuv/ZswCmTBlPkyZ1qVevGr16dePYsb+0pk0PzdgSmIRWwcHBfPgQTuHCRciUKZPGPhMTE/Lly8+jRz6EhYVy48Y1AMqWLR8nXalSZfDxuU9oaGii59yyZRPFi5dk3brfE1x4zsLCgg8fPqj/Vq1wbGZmDsDRo4fx83tK9+4/a329SJ/Cw8Pp1+9/7N27m/z5C/DTT+2xts7Bli2bGDq0H5GRMctm6NKEnRSPH/vi7j4l3v1Pnjzmf//rwpEjhyhSRIGbWwuioqKYNGkcK1Ys1kibXpqxpSlPaGVsHLNyaHz9Q2FhoSiVSl6+fEFAgD/ZsmXXOht07ty5AfDze0KJEqUSPOfcuYuoXNkl0byVLFmaPXt2cevWDfLlK8CePTvJnt2a3LnzEBkZyfr1q2nc2I08efImeiyRfuzb58mTJ4/56af2DB48HIhZamPatIn89ddBjhw5RMOGTZLchJ0UV69eZuLEsQQHB8ebZu7cmbx//45+/QarFz6Miopi6tQJbNmyiUqVqlKuXAUg/TRjS41JaGVpmYXcufPy4MH9OP09jx49VG8LDQ3l/ft3mJubaz2OqhaTlBpTUoISQKdOXbG2tqZv3540aVKXO3duM2zYaDJkyICX1x+8evWSrl17qtMnt/lF/Fju3v0HgMaN3dTbDAwMaNKkGRCzhpMuTdgJ+fTpI7NmTWPIkH4oldEoFNrXrHr79g3Xr1+lYMFCtG/fSb3dyMiIAQOGALBjx3/nSy/N2BKYRLzatetIRMQnxowZxs2b1wkPD+fGjetMmDAaExMTAJTKmP6ojBm1L3ugWg5BtaJoSrCyysaGDZv55ZfJDBkyAg+PLdSoUYuIiAg8PNbSvHkrcua04dw5b376yY0aNZz53/+64OPzIMXyINIeS8usALx48Vxj+5s3rwHImtVKpybshAQGBnLgwD4qV66Kh8dWihQpqjWdKqCUKFEqzmq7OXLkJEuWLNy8+d8S9OmlGVsCk4hXy5Y/8dNP7fH1fUS/fv+jfv3q9O//P+zsStCgQSMAMmXKhImJCZGR2pv8VAEpvvV1ksvMzJyGDZvQunU78ucvCMDevbsJCXlP587dCQ4OZsKEMZQoUYp585aQKVMmfvllpLofQaQ/jRu7kTFjRpYuXcDNm9f5+PEjV69eZuXKpZibm9O4sZtOTdgJsbCwZMWKtcyevZAcOXLGm051Q6ftxi06OpoPHz7w/v079fOAJUuW5vLlS9y6dYPg4OAfthlb+phEvAwMDBg8eDhNmjTj8uULKJVKHB3LUbx4ScaPHw1AtmzZsLCwjLepTnVnqbqjSy0fPnzg9989aNWqLVZW2di5cxufP39mxIixWFpakj27Nd26tefChXNUrVotVfMi9FPx4iVYuHA5kyf/Qr9+/1Nvt7HJxYoV68idOw+ARhN27B/4L5uwE2Jubp7gyFKVggULYWJiwtWrlwkNDdVoEr9w4Zw6YIWFhWJqakqnTl05ffpv+vaNaao2NjZm0qQZZMiQgf3792htxjYySnv1j7SXY/HNFSlSlLZtO9KuXSeKFy8JwL17/2Jubk6OHDnJly8/QUGBfPr0Mc5rnz9/hqGhIfny5UvVPO7atY2IiE/qzmN//6dkyZIVS0tLAPLlyw9AQIB/quZD6K+goEB+/XU5b9++oWrVarRr14myZcvz8uUL5s6dqR52ndQm7JRgYmJCy5ZtCAoKZPTooTx4cI/w8HDOnfNm9uzp6pYG1UjA5DRj9+zZhbt376ZMhr8RqTGJeE2aNI4bN66xe/cBjIyM1Nvv37/L8+fPqFWrLgBlyjhy9eplbty4jpNTJXW6T58+8c8/tyhUqDCmpmapls/Q0FC2bNlE27YdsbTMAsSMaoqK+q/ZTnXn+WU7vkg/Jk8ez61bN5gyxZ06deqpt2/fvpmlSxcyZ84Mpk2bRcuWP+Hv78euXds0alb16zekbNny7N27O07/09fo1asfL1++4PjxI3Tv3hGI+Zy2b9+Zx499OXv2tMb5VM3YsWlrxq5SpRojRozl9983MHDgQLy8vMiYMWOK5Ts1SWAS8SpQoCDHjv3F0aOH1X1KHz9+ZNGieQB07NgVgHr1XNm0aQPr16/G0bGcesDDpk0bCAsLw82tRarmc+vWTRgaGtC2bQf1tvz5C/Du3Tv8/J6SL19+/vnnFvBfzUmkL69eveTKlYs4OpbTCEoAbdt25I8/9nHy5HHCw8MwNTVLUhN2SsmYMSNTp7rTpk17bt68TsaMxjg5VaJAgYL8739dyJgxIxYWlvG+PrFmbGtra7p2bY+3tze1atVKsXynJglMIl5t23bg4MEDuLtP5fLlC1hZZePkyRMEBPjTq1df7O1jnksqUqQwHTp0ZtMmD3r06IiLS3V8fR9y5ow3Zco40qJFKzJk+K/VeNu2zYSEhNCuXUcsLCy0ntvQMKZmY2RkoPHaLwUFBbFz51a6d/8flpb/Hat+/QasXbuKkSMH4+JSjcOHD1GgQEEqVnROtNzv3gWzZs1KvL1PERwchLV1DmrXrkuPHr017lzXrFnJb7+t03qMOnXqMWWKe7znuHr1MoMG9Uk0L97el4GYkY/Lli3k0CEvTEyMqVOnPn36DFTfBKjs3bubZcsWsn37XrJnt070+OnFq1cvAeIdQl2wYCEeP37E69evKVAgpnZfpEjROKPpYjdhpzR7+zLY25dR//3p0ycePXpI/vwFE6zpJ7UZ++nTpyme59QigUnEy8LCgm3btjJ//nzOnz9PWFgYCoWCMWNGU79+fY20v/wyhkKF8rNlyxZ27NhKjhw56NatGwMGDIgTfHbu3EZAQAAdOrTFykp7E1/mzDE/uObmmeJNA/Drr0sxMzPj5597aIz8s7Iy49dff2Xq1Kns3r2TMmUcGDFirEaTpDaq2QGePHlMuXIVqFfPlVu3brBlyyZu3brB0qX/zQXo43MfY2Njdc0xtsKFiyR4nty588Q7pPfff//h/PmzODqWU2/btWsbu3Ztp2bNOpiamrJr13aMjDLQv/9gdZpPnz7x22/raNmyjQSlL1hZxdRw/Py0/zj7+/thYGCAlZVVkpuwU0rfvj35/Pkza9du1Nh+/vxZIiI+4excKZ5X/rjN2BKYRLwMDQ3IlSsXrXvNpn5nzX1HbsZ9aDVn6fYMcW+vse28L4Bm2rGLjgLwbyD8G6j94dfiNfqzrEZ/lPGcS6V8o1GUbzQK7wdxz0OmCgyeuR/LzOBczJCgoDAiIxN+2DapswMAPHzoQ8GChejZs3eCx9Qmd+48Wl8XGhpKly5tyZo1K1OmzFRvP3BgP87OVZg+fTYQ0/yzb5+nRmDau3cX4eFhdOzYRef8/Ojy5rXFzq4E165d4fTpv6lWraZ634EDe/HxuY+zcxUsLbMkuQk7peTLlx8vrz+4ceMaDg5lgZgpwVatWoqxsQlt2nSI97W6NGMXLFgwRfOdmiQwiUS9/wDByVtWKc1JaHaAv/46yD//3KJhwyaEhYXy4sXzOPMDfq3lyxfx6tVLJk6crlHref48QGOYe7FiCvbt8yQ4OJisWbMSHh7Opk0etGnTQT01jdA0ZswEBg7szS+/jKJq1Wrky1eAhw99uHDhLNbW1owePZYMGQzp2LFTkpqwVdasWQXAzz/H3zSrqq0YGRnGaZru1asPp0//zYgRg2jQoCHGxiYcO3aEwMC3TJgwhVy5bLQeU1szdnS0ktq167NmTUwzdpUqLhw5cpjChQtTtWrVZL5z354EJiFiiT07QNGixdTbY88OAODj4wMQ7xP9yfHokQ9//rmfMmUcqV/fVWOfhYUlHz78d3cQFhaGoaGhen7CnTu3Eh0dTbt2HVMsPz+aYsUUrFu3iQ0b1nDp0nnOnvUmW7bsNGvWkuHDBmOTKxcQ0wy8ffs25s2bl2gTNsC6dasBGDVqeLznNjGJ+am1tMwcp2nayqooO3bsYN68eZw69TdRUVGULFmSPn3mU6lS/M142pqxlVFRQA7mzFnIggWz2bNnFyVL2jNr1sxEm7H1iYFS16ly04E3b0I0nlNIamf4l5YtW8S2bb+zZMkq9SSMibl7919++20tN25cJzw8jJw5bahVqy7duv1Pow/lW3SGZ8hgiJWVGUduRqfpGlNWU6hXJmlNeXfv/kvfvj3ImdOGX36ZjEJRnDt3bjNt2kQ+fAhnw4Yt5M6dB0/PnSxYMJtOnbpx9+4d7t79F4AKFSrSq1c/9WwUuhg9eihnzpxm5cp1lC7toLHvl19GcuvWTRYvXompqSnDhw8kc+bMrFmzkZCQEH76yY2OHbvSuXM3nc+rorren87uRRnyNtnH+d4MLLJjUqV5kq43/Ffu4C0riHz17BvkMHVkyJmHrB36xSm3gQFYW2sfZKSvpMaUCF06w2O7c+c2O3du1elcV69eZvjwgQDUqFEba+sc3Lhxlc2bf+Pq1UssW7ZG/YCfdIanjqTODvDwYcy8e1u3bsLFpTpubs3x8fHh77+Pc/nyRZYu/ZVixeySfF4/v6ecPRszivHLoATwv//1ZeDA3nTu3AaImVlg3rwlAGzZshFjY2Nat/5vMcbo6GgMDeX5eV1EvnpGZMDj750NgQSmROnSGa7y+fNn3N2nxrs0eHzmz59FdHQ0K1euo2RJe/W55syZyR9/7GHPnp20axczA7F0hqeOL2cHyJevAPfu/cu1a1eYO3cmc+YswsLCAkNDI3Llys24cZM0asN//XWQqVMn4O4+lfXrNyf5vLt370CpVNKhg/ZrVahQYTZu3Ia39ymio6OoUqUaOXPaEBQUyK5d2/j5535kzpyZAwf2sXr1CoKDgyhbtgJjx04k1/83UQmRVsgtVSKSMlX+lzZuXI+/vx8VKjgl+Ty+vo948uQx1arVUAcl1bm6d4+5cz9//qx6+/PnARp9IMWKKQgPD1Ov+yKd4cmjmh1g8uSZzJ69kAEDhrB06a8MHDiUW7duMGfODACGDx/Nrl1/xGmirV+/IY6O5bh//x5Pnz5O0jmjoqI4evQQ1tY5EpzHL1u27Li5taB589bkzBnTIb5p0wYsLCxp3rwVjx49ZPbs6dSuXZfZsxcSGPiG6dMnJu+NEOI7ksCUiKRMlR+bj88DNm3aQKdO3ShUKOFnWWIzMzOjb9+BGgFQRTUDcezp7qUzPOUlNjtAwYKF1bMDJEShiGnCe/Ysaf0Vt2/fJDg4mJo1a+v0rMmrVy/Zu3c3Xbr0wNjYmD//3EeWLFkZOHAYlStXpXfv/ly/fpVHj3ySfEwh9IEEpkQkZap8laioKGbNmoatbX66dOmh03ly5rShY8euWhfLO3XqBBDTnKNSqpQ9f/99HF/fR7x8+YKDBw9gZ1ccY2NjQkJC2Lr1d9q375zqs3r/SJIyO0B0dDQvXjzn33//4Z9/bmtN9+nTJ4A4A1Hic+7cGQBq1qyjU349PNZibZ1DXXv38/MjT5686tFX+fIVAGTiWpH2SB9TIpLaGQ4xHeH3799lxYq1KTZZYmDgW9at+xVAY8456QxPeUmdHSBrVitatmxM5symHDhwRGMYrlKp5PbtmxgZGSV58MM//9wiQ4YMGk24iQkI8OfPP/czZswE9eCbmCf+/+vXjIiICZBp6Yl/IUBqTIlK6lT5T58+Yf36NbRo0VpjvquvERoaysiRQwgMfEvr1u00frhUneGjRv3CiBFj2LhxO/b2ZdSd4Z06dVN3hru5NaBmzUoMHtyPFy8SXuAsPftydoDYVLMDODlVJlu27FStWo2QkPf8/ruHRrqtW3/n4UMf6tVzjXcewC89eHCfggULJ7mGBbB+/a/Y2uajfv2G6m358xfg6dPH6rWC/pu4tkCSjyuEPpAaUyKSMlX+1KnuzJo1DSsrK3r3HpAi5w0KCmL48IHcv3+XKlWqMWDAkDhpVJ3hsWnrDG/Vqg3OzlVYsWIx06dPZNmy1SmSxx9R7NkBXFyqkz9/AXx8HnD+vObsAIMHD+f27VusWbOS69evUqyYgrt3/+Xq1csUKlSYoUOHazzhH9/sAO/eBRMaGkKOHGUSnKw2Nl/fRxw5cpipU2diYvJfzbxpUzc8PXcwaFAfHB3LcuDAPipWdI63aVIIfSWBKQFJnSp/9+7t3Lx5nblzF6kHH3yNgAB/hg0bQECAPy4u1Zk6dZbWZ6W05Xfv3t0MGjQ8Tme4kZERUVGRjBkznEePfChcOOVmLPiRFCumYP36TWzevIGzZ89y9qw32bNnp23btgwYMICcOWNmlbayKsaePZ4sXryYU6dOcf36VXLmzEmPHj3o169fnNpSfLMDBAe/AiBbtqwJTlYb28SJq1EoFLRu3Vyjmc7ZuRwLFixg7ty57N27mypVqjB16jQMDQ2Ijpbn6EXaIYEpAUmdKn/jxg0AjBw5RGs61fIGO3fu1+iT0ubBg3sMGzaQoKBAGjZswujR45MUlEC3znAJTPHLnz8/c+bM4dXLl+qZmQEiPn3C389PI22/vn3p17evxrZ3wcG8+/9h+yrHjsZMXPvl6zNmyBDvvviMHjUKgAD/uIMa7EuV4jcPDyBm8EVOm5wEBYVJYBJpigSmBCS1M7xDh86EhcUdQnzhwjnu3LlNw4ZNyJUrN+bmCfc5+Pv7MXToAIKDg2jbtiMDBgxJcse1dIanvIiICI3AJIT4NiQwJSCpU+WrZmP4UmhoqDowJTZXXnR0NJMn/0JwcBA//dSegQOH6pTX+DrDb968RmhoKObm5tIZLoRIEyQwJSKpneHaqEZna5vq/svO8BMnTnD37h2MjY0xMzNlw4a4AxSyZ7emZcvWcbZLZ7gQ4kcigSkRSe0M10YVJCws4q7C+mVn+L//xtRmIiIi8PDQvlx38eLF6dkz7iJlunSGT5HOcCGEnpNlL7T4ctkL1bT4mw5/5GVQ4tPo6ysbK0M6N8ik83IA6WnZC/iv3P5+fmm6j8nY2BjbfPl0Lnd6XfbizaLxaXp28Qx5C2I9ZLose5HevAyKxv912g1MQgiRFsjMD0IIIfSKBCYhhBB6RQKTEEIIvSKBSQghhF5JM4Fpx44dNG7cGEdHRxo2bMjmzZuJPaDwyZMn9OnThwoVKuDs7MykSZPUsywLIYRIO9LEqLydO3cyYcIEOnfuTJ06dbh8+TLTpk3j06dP9OjRg/fv39O1a1esra2ZNWsWgYGBzJ07F39/f9at0/5MkBBCCP2UJgLT7t27KV++POPHjwegcuXK+Pr68vvvv9OjRw+2bt1KcHAwnp6eZMsWM7+djY0NvXr14sqVK5QvX/57Zl8IIYQO0kRT3qdPnzA311wiPGvWrAT//wzO3t7elC9fXh2UAFxcXDAzM+PUqVPfMqtCCCG+UpoITF26dMHb25t9+/YREhLC6dOn2bNnD82axSzv8PDhQwoVKqTxGiMjI2xtbfH19f0eWRZCCJFMaaIpr3Hjxly8eJFR/78ODcTUiMaNGwdASEgIZmZxF1kzMzNL1gCI9LAqRHooozZS7vRFyp0234M0EZj69evHlStXGDlyJGXKlOH+/fssXbqUwYMHs3z5chKa7i85aw9lz5625pXSVVJXSv3RSLnTFyl32qX3genq1aucPn2a6dOn89NPPwHg5OREvnz56NWrF3///Tfm5uZaF+oLDQ3FxsZG53O+fas5iauRkeEPcbFVgoLCiIpKfM4/KfePQcqdsB+93AYGae9mW+/7mJ49ewZAuXLlNLZXqBCz8N6DBw8oVKgQT59qrjIbFRWFv78/RYoU0fmcSqXmvx/Rl2XU9u9HJOWWcqfHcqc1eh+YChcuDMDly5c1tl+9ehWAfPnyUbVqVS5dukRgYKB6v7e3N+Hh4VStWvXbZVYIIcRX0/umvJIlS9KgQQNmzZrFu3fvcHBwwMfHh6VLl1KqVCnq1auHs7Mzv//+O927d2fAgAEEBwczd+5cqlevHqemJYQQQr/pfWACmDdvHitXrmTbtm0sWbKEPHny0LJlS/r370+GDBnIli0bGzduZObMmYwYMQIzMzNcXV01RvEJIYRIG9JEYDI2Nmbw4MEMHjw43jQKhQIPD49vlykhhBCpQu/7mIQQQqQvEpiEEELoFQlMQggh9IoEJiGEEHpFApMQQgi9IoFJCCGEXpHAJIQQQq9IYBJCCKFXJDAJIYTQKxKYhBBC6BUJTEIIIfSKBCYhhBB6RQKTEEIIvSKBSQghhF6RwCSEEEKvSGASQgihVyQwCSGE0CsSmIQQQugVCUxCCCH0igQmIYQQekUCkxBCCL0igUkIIYRekcAkhBBCr0hgEkIIoVckMAkhhNArEpiEEELoFQlMQggh9IoEJiGEEHpF58D06dOn1MiHEEIIASQjMFWtWpVJkyZx8+bN1MiPEEKIdE7nwNSjRw/Onz9P27ZtadSoEWvXruX169epkTchhBDpkM6BqV+/fhw+fJjNmzdTvnx5fv31V2rVqkWvXr04fPgwnz9/To18CiGESCeSPfihXLlyTJs2jTNnzrB48WI+fPjAkCFDcHFxYfbs2QQEBKRkPoUQQqQTXzUq7/nz56xfv54lS5Zw6dIlChYsSMuWLTl16hSNGjXCy8srpfIphBAincig6wtCQ0M5fPgwe/fu5cqVK2TKlAlXV1cmTZpEuXLlABg9ejS9e/dm5syZNGrUKMUzLYQQ4selc2CqWrUqnz59wtHRkalTp9KoUSNMTU3jpCtdujR37txJkUwKIYRIP3QOTB07dqR169YULlw4wXTdu3enb9++yc6YEEKI9EnnPqZRo0YRFBTE8uXL1dvu3LnD4MGDuX37tnqbmZkZRkZGKZNLIYQQ6YbOgenkyZN07doVb29v9TYDAwMeP35Mhw4duHz5copmUAghRPqic2BaunQpjRs3ZsuWLeptJUqUYN++fTRs2JAFCxakaAaFEEKkLzoHpocPH9K8eXMMDAzi7GvevDl3795NkYwJIYRIn3QOTBYWFvj6+mrd5+fnp3WEnhBCCJFUOgemevXqsXjxYk6cOKGx/fTp0yxevJh69eqlWOaEEEKkPzoPFx86dCi3bt2ib9++ZMyYkaxZsxIcHExkZCQODg4MHz48NfIphBAindA5MJmbm7Nt2zZOnjzJlStXePfuHRYWFlSoUIGaNWtiaChrDwohhEg+nQMTgKGhIbVq1aJWrVpx9imVSq0DI4QQQoikSFZg8vLy4uLFi0RERKBUKoGYgBQeHs7169c5depUimZSCCFE+qFzYFq2bBnLli3DwsKCyMhIMmbMSIYMGQgMDMTQ0JCffvopNfIphBAindC5Q2jPnj00b96cixcv0q1bN2rVqsXZs2fZtWsXWbNmpVixYqmRT65fv07nzp1xdHSkSpUqjB49mrdv36r3P3nyhD59+lChQgWcnZ2ZNGkSoaGhqZIXIYQQqUfnwPTy5UuaNm2KgYEBJUqU4Nq1awDY29vTp08fdu7cmeKZvH37Nl26dMHMzIxly5YxYsQIzpw5Q//+/QF4//49Xbt25c2bN8yaNYvhw4fj5eXF4MGDUzwvQgghUpfOTXmmpqbqwQ0FChTA39+fjx8/kilTJkqUKIG/v3+KZ3Lu3LmULFmSFStWqEf9mZubM2PGDPz8/PDy8iI4OBhPT0+yZcsGgI2NDb169eLKlSuUL18+xfMkhBAidehcYypdujR79+4FoFChQhgZGXHu3DkgZroiY2PjFM1gUFAQFy9epH379hpD0evXr8/JkyfJly8f3t7elC9fXh2UAFxcXDAzM5OBGEIIkcboXGPq06cP3bt35/3796xatQo3NzdGjx6Ns7Mz3t7e1K1bN0UzeO/ePaKjo8mWLRvDhw/n+PHjQMwMFOPHj8fS0pKHDx/GWSnXyMgIW1vbeKdPSkh6GO2eHsqojZQ7fZFyp833QOfAVLFiRXbt2sW9e/cAmDhxIoaGhly9ehVXV1fGjBmTohkMDAwEYNy4cVSvXp0VK1bw+PFjFixYgJ+fH1u2bCEkJAQzM7M4rzUzM0vWAIjs2S2+Ot/6zMoq7nuVHki50xcpd9qlc2BasWIFDRo0oFmzZgCYmJgwbdq0FM+YyufPnwEoVaoUM2bMAKBy5cpYWloybNgwzpw5o36WSpvkPOz79m0IsQ9pZGT4Q1xslaCgMKKiohNNJ+X+MUi5E/ajl9vAIO3dbOvcx/Trr7+mygCH+KhqQl/OMlGtWjUgZvVcc3NzwsLC4rw2NDQUCwvdL4hSqfnvR/RlGbX9+xFJuaXc6bHcaY3Ogalo0aLJ6rdJroIFCwIQERGhsT0yMhKATJkyUahQIZ4+faqxPyoqCn9/f4oUKfJN8imEECJl6NyUV6tWLRYsWMDp06exs7OLs/6SgYGB+vmilFCkSBHy5s3Ln3/+SadOndRNc8eOHQOgQoUKvH//nnXr1hEYGKgemeft7U14eDhVq1ZNsbwIIYRIfcmakgjgzJkznDlzJs7+lA5MBgYGjBo1iiFDhjB06FDatGmDj48PCxcupEGDBpQsWZJcuXLx+++/0717dwYMGEBwcDBz586levXqlCtXLsXyIoQQIvXpHJi+x9Lprq6urFy5kuXLl9O7d2+yZMlCu3btGDp0KADZsmVj48aNzJw5kxEjRmBmZoarqyujRo365nkVQgjxdZI1u/j3EN8yGyoKhQIPD49vlyEhhBCpQufANHbs2ETTuLu7JyszQgghhM6B6cKFC3G2hYeHExwcTNasWSldunSKZEwIIUT6pHNgUk0J9KWHDx8yYMAAmjdv/rV5EkIIkY7p/BxTfIoUKcLAgQPVo/aEEEKI5EixwAQxS1EEBASk5CGFEEKkMzo35T179izOtqioKF6+fMmSJUtkpgUhhBBfRefAVLt2ba0ToyqVSjJlyiRNeUIIIb6KzoFp5syZcQKTgYEB5ubmODs7J2vSVCGEEEJF58DUsmVLoqOjuX//PsWLFwfg9evX3Llzh8yZM6d4BoUQQqQvOg9+ePnyJc2aNWPAgAHqbXfu3KF379506tSJ4ODglMyfEEKIdEbnwDRnzhwiIiKYN2+eeluNGjXw9PQkODiY+fPnp2gGhRBCpC86B6azZ88yYsQIHB0dNbaXLFmSwYMHc+LEiZTKmxBCiHRI58AUERGBkZGR1n2ZM2fWupKsEEIIkVQ6ByYHBwc2bNjA58+fNbZHRkayceNGypQpk2KZE0IIkf7oPCpv0KBBdO7cmTp16lC9enWyZ89OYGAgZ86c4e3bt2zatCk18imEECKd0DkwOTo6sn37dlatWsXff/9NcHAwFhYWVKhQgX79+lGiRInUyKcQQoh0IlkLBZYsWZKFCxeq+5o+fPhAZGSkPFwrhBDiq+ncx/T582cmTZpEmzZt1NuuXbtG5cqVmT17NtHR0SmaQSGEEOmLzoFp6dKl7N+/n8aNG6u3lSxZkhEjRrBjxw7Wrl2bohkUQgiRvujclPfHH38wevRo2rVrp96WNWtWunXrRoYMGdi4cSO9evVK0UwKIYRIP3SuMQUFBZEvXz6t+woXLsyLFy++OlNCCCHSL50DU+HChTl8+LDWfcePH6dAgQJfnSkhhBDpl85NeV26dGHMmDEEBwdTt25d9XNMJ06c4ODBg7i7u6dGPoUQQqQTOgem5s2bExYWxooVK/jrr7/U262srJgwYQLNmzdPyfwJIYRIZ5L1HFPHjh3p0KEDvr6+BAcHY2lpSeHChTE0NESpVGpd4VYIIYRIimQFJohZtbZw4cLqv1+9esWOHTvYvXu3zDAuhBAi2ZIdmFROnz7Ntm3bOHnyJJGRkdja2qZEvoQQQqRTyQpMgYGB7Nq1ix07dhAQEIC5uTktWrSgWbNmVKhQIaXzKIQQIh3RKTCdP3+e7du3c/ToUaKioihfvjwBAQEsX74cJyen1MqjEEKIdCRJgcnDw4Pt27fj6+tLgQIF6NevHy1atMDU1BQnJycZ7CCEECLFJCkwzZo1Czs7OzZu3KhRMwoJCUm1jAkhhEifkjTzQ+PGjXny5Am9e/emX79+HDlyhMjIyNTOmxBCiHQoSTWm+fPnExoayh9//IGnpycDBw7EysqKunXrYmBgIE15QgghUkyS58ozNzenffv27Ny5kz/++INmzZpx/PhxlEol48aNY/Hixfj4+KRmXoUQQqQDOk/iClCsWDHGjBnDyZMnWbp0KYULF2bNmjU0bdoUNze3lM6jEEKIdOSrHrDNkCED9erVo169erx584Y9e/awZ8+elMqbEEKIdChZNSZtrK2t+fnnn/Hy8kqpQwohhEiHUiwwCSGEEClBApMQQgi9kqTANGnSJJ4+fQrAs2fP+Pz5c6pmSgghRPqVpMDk6enJq1evAKhTpw7//vtvqmZKCCFE+pWkUXk5cuRg3rx5uLi4oFQq2blzJ6dOndKa1sDAgP79+6doJoUQQqQfSQpMw4cPZ9q0aVy/fh0DAwN27twZb1oJTEIIIb5GkgJT48aNady4MQDFixdnx44dlClTJlUzJoQQIn3SeVTexo0bKVKkSGrkRQghhNB95gcnJyd8fX1ZsmQJFy9e5P3791hZWVGhQgX69etH0aJFUyOfQggh0gmdA5OPjw/t2rXDyMiI2rVrY21tzevXrzlx4gR///03O3fulBqVEEKIZNM5MM2bNw9bW1s2bdqEhYWFentISAhdu3Zl4cKFLFu2LEUz+aUBAwZw584djh8/rt725MkT3N3duXz5MkZGRri6ujJy5EjMzc1TNS9CCCFSls59TJcuXaJPnz4aQQnAwsKCXr16cenSpRTLnDb79u3jyJEjGtvev39P165defPmDbNmzWL48OF4eXkxePDgVM2LEEKIlKdzjSlDhgyYmJho3WdsbExERMRXZyo+L1++ZMaMGeTKlUtj+9atWwkODsbT05Ns2bIBYGNjQ69evbhy5Qrly5dPtTwJIYRIWTrXmEqXLs2WLVtQKpUa25VKJZs3b8be3j7FMvel8ePHU7VqVSpXrqyx3dvbm/Lly6uDEoCLiwtmZmbxPggshBBCP+lcYxo8eDDt27fHzc0NV1dXcuTIwevXrzl06BC+vr5s2LAhNfLJzp07+eeffzhw4ABz5szR2Pfw4UMaNWqksc3IyAhbW1t8fX11Pld6WCk+PZRRGyl3+iLlTpvvgc6BqXTp0qxdu5b58+ezbNkylEolBgYG2Nvbs2bNGipWrJjimQwICMDd3R13d3eNWpFKSEgIZmZmcbabmZkRGhqq8/myZ7dIPFEaZmUV971KD6Tc6YuUO+1K1gq2lSpVYufOnXz48IH3799jaWlJ5syZUzpvQEwT4bhx46hRowYNGjSIN018DJJxu/D2bQixD2lkZPhDXGyVoKAwoqKiE00n5f4xSLkT9qOX28Ag7d1sf9XS6pkzZ061gKSyefNm7t27xx9//EFkZCTwXyCKjIzE0NAQc3NzwsLC4rw2NDQUGxsbnc+pVEICse6H8KOXLz5S7vRFyp02fVVg+hYOHz5MUFAQLi4ucfaVKlWKAQMGUKhQIfV6USpRUVH4+/tTv379b5VVIYQQKUDvA9OUKVPi1IaWL1/O7du3WblyJTlz5sTAwIB169YRGBio7oPy9vYmPDycqlWrfo9sCyGESCa9D0yFCxeOsy1r1qwYGxtTunRpADp06MDvv/9O9+7dGTBgAMHBwcydO5fq1atTrly5b51lIYQQX0Hn55j0UbZs2di4cSNWVlaMGDGChQsX4urqysKFC7931oQQQujoq2pMJ0+e5PDhw7x584bs2bNTp04d6tatm1J5i9esWbPibFMoFHh4eKT6uYUQQqSuZNeYPDw8+OWXXzAxMaFEiRIYGBgwduxYFi1alILZE0IIkd4kqcYUFhYW5wHW3bt3s3r1akqWLKneVrNmTSZOnMiQIUNSNJNCCCHSjyTVmOrVq8fGjRv5/PmzeluOHDnUQ7mjo6N5+fIlR48eTdZzQ0IIIYRKkgLTunXrOHnyJA0aNGDfvn0ATJ48mdOnT1O5cmVKlSpFzZo1+ffff5k9e3aqZlgIIcSPLUlNeSVKlGDdunWcPXuWefPmsW7dOoYPH46npyd+fn7q54fy5cuX2vkVQgjxg9NpVF6VKlXw9PRk//79TJ06lTx58jBixAgcHBxSK39CCCHSGZ1G5X348IHQ0FDc3Nw4dOgQtWvXpnfv3gwYMIBHjx6lVh6FEEKkI0kKTE+ePKFdu3aUK1eOihUr0qxZMx4+fEj37t05cuQIBQsWpHXr1kyYMIGXL1+mdp6FEEL8wJIUmMaPH4+VlZW6Ga9KlSrqIeEWFhaMGDECLy8vIiMjcXV1Tc38CiGE+MElKTD9888/dOnShRIlSlCsWDH69evHkydP+PjxozpNrly5cHd3Z/v27amWWSGEED++JA1+cHBwYPHixYSFhWFsbMz+/ftRKBRkypQpTlqFQpHimRRCCJF+JKnGNHv2bHLmzMm4ceMYOXIkISEhLF26NLXzJoQQIh1KUo0pZ86cLFmyJLXzIoQQQvwYy14IIYT4cUhgEkIIoVckMAkhhNArEpiEEELoFQlMQggh9IoEJiGEEHpFApMQQgi9IoFJCCGEXpHAJIQQQq9IYBJCCKFXJDAJIYTQKxKYhBBC6BUJTEIIIfSKBCYhhBB6RQKTEEIIvSKBSQghhF6RwCSEEEKvSGASQgihVyQwCSGE0CsSmIQQQugVCUxCCCH0igQmIYQQekUCkxBCCL0igUkIIYRekcAkhBBCr0hgEkIIoVckMAkhhNArEpiEEELoFQlMQggh9IoEJiGEEHpFApMQQgi9IoFJCCGEXpHAJIQQQq9IYBJCCKFX0kRgio6OZuvWrTRt2pSyZctSp04dZs6cSWhoqDrNkydP6NOnDxUqVMDZ2ZlJkyZp7BdCCJE2ZPjeGUiKtWvXsmjRInr27EnlypXx9fVlyZIlPHjwgPXr1xMSEkLXrl2xtrZm1qxZBAYGMnfuXPz9/Vm3bt33zr4QQggd6H1gio6OZs2aNbRt25bhw4cDUKVKFaysrBg6dCi3b9/m7NmzBAcH4+npSbZs2QCwsbGhV69eXLlyhfLly3/PIgghhNCB3jflhYaG0qxZM5o0aaKxvXDhwgD4+fnh7e1N+fLl1UEJwMXFBTMzM06dOvVN8yuEEOLr6H2NydLSkvHjx8fZfvToUQCKFi3Kw4cPadSokcZ+IyMjbG1t8fX11fmcBgbJy2takh7KqI2UO32RcqfN90DvA5M2N27cYPXq1dSqVQuFQkFISAhmZmZx0pmZmSVrAET27BYpkU29ZWUV971KD6Tc6YuUO+1Kc4HpypUr9OnTB1tbW9zd3QFQKpXxpjdIxu3C27chxD6kkZHhD3GxVYKCwoiKik40nZT7xyDlTtiPXm4Dg7R3s633fUyxeXl50b17d3Lnzo2HhwdWVlYAmJubExYWFid9aGgoFha6XxClUvPfj+jLMmr79yOScku502O505o0E5jWrVvHsGHDcHR0ZPPmzeTMmVO9r1ChQjx9+lQjfVRUFP7+/hQpUuRbZ1UIIcRXSBOBadu2bcyZM4eGDRuydu3aOLWgqlWrcunSJQIDA9XbvL29CQ8Pp2rVqt86u0IIIb6C3vcxvX79Gnd3d/LmzUvHjh25c+eOxv78+fPToUMHfv/9d7p3786AAQMIDg5m7ty5VK9enXLlyn2nnAshhEgOvQ9MJ0+e5OPHjwQEBNCxY8c4+93d3WnZsiUbN25k5syZjBgxAjMzM1xdXRk1atR3yLEQQoivofeBqXXr1rRu3TrRdAqFAg8Pj9TPkBBCiFSVJvqYhBBCpB8SmIQQQugVCUxCCCH0igQmIYQQekUCkxBCCL0igUkIIYRekcAkhBBCr0hgEkIIoVckMAkhhNArEpiEEELoFQlMQggh9IoEJiGEEHpFApMQQgi9IoFJCCGEXpHAJIQQQq9IYBJCCKFXJDAJIYTQKxKYhBBC6BUJTEIIIfSKBCYhhBB6RQKTEEIIvSKBSQghhF6RwCSEEEKvSGASQgihVyQwCSGE0CsSmIQQQugVCUxCCCH0igQmIYQQekUCkxBCCL0igUkIIYRekcAkhBBCr0hgEkIIoVckMAkhhNArEpiEEELoFQlMQggh9IoEJiGEEHpFApMQQgi9IoFJCCGEXpHAJIQQQq9IYBJCCKFXJDAJIYTQKxKYhBBC6BUJTEIIIfSKBCYhhBB6RQKTEEIIvSKBSQghhF6RwCSEEEKv/HCBydvbm1atWuHg4EDt2rVZt24dSqXye2dLCCFEEv1Qgen69ev06dOHwoULs3TpUpo2bcrcuXNZs2bN986aEEKIJMrwvTOQkpYuXUqJEiWYO3cuANWrVycyMpJVq1bRpUsXMmXK9J1zKIQQIjE/TI0pIiKCCxcuUK9ePY3tDRo0ICwsjCtXrnynnAkhhNDFDxOY/Pz8+Pz5MwULFtTYXqBAAQB8fX2/Q66EEELo6odpygsJCQHA3NxcY7uZmRkAoaGhST6WoSFoGy9ha22IcRp+x3Jm/e8+xFCHWxIrMzBKw7cwlpn/+39dym1iYoKBgUHKZ+gbMTY2Vv+/LuU2sMwORmn3g25glkX9/7qUO0OeAhgYm6RCjr4NI+tc6v+PXe60+BFOu5++L0RHRye431CHT2i2bBZat7er+2P0UVlZmemUvkKRNByVYtG13Dly5kylnHxbupbb2L5aKuXk29K13Fnb/JxKOfm2dC23PvoxfnEAC4uYYBIWFqaxXVVT+rImJYQQQj/9MIEpf/78GBkZ8eTJE43tT58+BaBIkSLfI1tCCCF09MMEJhMTEypUqMCRI0c0Hqg9fPgwFhYWlClT5jvmTgghRFL9MIEJoG/fvty4cYPBgwdz8uRJFi1axLp16+jduzeZM2dO/ABCCCG+OwPlDzZfz5EjR1iyZAm+vr7Y2NjQsWNHevTo8b2zJYQQIol+uMAkhBAibfuhmvKEEEKkfRKYhBBC6BUJTEIIkUQ/cs+HPpXth5n5QZ917tyZixcvqv82MDAgc+bMFCpUiObNm9OhQwcyZMjAmDFj2LNnT4LHcnJyYtOmTamd5RTxZbkhpuympqYULFiQrl270qxZs3jTxrZr1y5Kly6dqvnVlSrPZcuWZdu2bVrTDB06FC8vL1q0aMGsWbOoXbs2Tk5OzJo1K97javscZMiQASsrKypXrsywYcPInTt3ipYlPtqui4WFBSVLlmTAgAE4OTkBMTP7L1u2LMFj3bx5ExMTk3jTZs6cGRsbG1xdXRk4cCAZMmRI9HMBMc8wqp5XjE9KfG927tzJw4cPGTNmDAAXLlygS5cucdJlzJiRbNmyUalSJUaOHEmOHDm+6rxfql27NsHBwRw4cIA8efLE2W9nZ8eAAQMYOHBgko955coVVq1apTdLBElg+kZKlizJpEmTAIiKiuLdu3ecOnUKd3d3Ll++zKJFi+jXrx/t2rVTv2bFihXcuXNH40uc1mawiF1uiCn7ixcv8PDwYNSoUWTNmpUaNWpoTRubvj4gbWhoyPXr13nx4gW5cuXS2BceHs6JEyeSddwcOXJoXPfIyEh8fX2ZN28e165d48CBA99sGZcvP7tBQUFs3bqVnj174unpSbFixdRpt2/fHu9xYs/dpy1tUFAQBw4cYNWqVURGRjJy5EgmTZqkMc/llClTADQ+Jx8/ftR4L1Lre7Ny5Up1II5t4sSJlCpVSv23ajWD1atX4+vry86dO7/63F8KCwtj/PjxrF+/PkWOpwq6+kIC0zdibm6Oo6OjxrbatWtTuHBhZsyYwYEDB3BzcyN//vzq/dmyZcPY2DjO69ISbeWGmLWyKleujKenpzowxZdWn5UsWRIfHx8OHTpEt27dNPadOHGCzJkzY2lpqfNxtV33ChUqkDFjRkaPHs2xY8do3LjxV+Q86bRdlypVqqiv3+jRo9Xbdbl+2tLWqlULf39/PD09GTlyJEWLFo2Tl8TO862/N0WLFo1zrqpVqxIREcGaNWvw8fGJU46vZWlpyZkzZ9ixYwdt2rRJ0WPrA+lj+s46deqEjY1NvE1BPyoTExOMjY3T9OzdAKamptSoUYNDhw7F2efl5UWDBg3IkCHl7v9UzZkBAQEpdszkyJw5c6rNvm5ubp7ix718+TKdOnXCwcEBJycnRo8eTWBgIBBTC2zdujXOzs7qbRDTpOro6MijR4+oXbs2AQEB7NmzBzs7O/z9/RM9p+qGJHZZ7t+/T+/evSlXrhzlypWjf//++Pn5abzut99+w9XVldKlS1OtWjUmT54cZ3UEVZPw7Nmzef78eYL5iI6OZvXq1dSrVw97e3saNGig0aypajoOCAjAzs4OT0/PRMuW2iQwfWeGhoZUrlyZmzdvEhkZ+b2zk+KUSiWRkZHqf58+feLRo0eMHTuWsLAwdR+TtrSqf/rUKatNo0aN1M15KqGhoZw6dYomTZqk6LlU64rFrlmnttjX5fPnz7x+/Zr58+cTERFBq1atNNJqu36RkZFaZ/+PvT8iIoKXL1+yZs0azpw5o/G5+FqXLl2iW7duZMqUiUWLFjFu3DguXrxIly5d+PjxI0ZGRsyaNYvw8HBmz54NwNGjR9mzZw+jRo2icOHCLFu2jBw5clCjRg22b99Ozlgzz0dHR2uUJTg4mL/++ot169ZRpkwZChUqBMRcu3bt2vH27Vtmz57NjBkz8PPzo3379rx9+xaAAwcOMHfuXDp27Mi6devo378/+/btY9q0aRplMjAwYObMmURHRzN+/PgEyz958mSWLFmCm5sbq1atwtXVlZkzZ7J8+XIA+vXrR40aNciRIwfbt2+nZs2aKfXWJ5s05ekBa2trPn/+THBwMNbW1t87Oynq0qVLGu3vEPOlUigULF68mFq1aiWYFmDBggXfrNkqOWrWrEnmzJk1mvOOHDlC9uzZKV++fLKPG/tGJTQ0lFu3buHu7o6tre03/fGI77oMGzYsTt+ftnQAHTt2ZOLEiYmmzZMnDwMHDqRXr15fkWNN8+fPp1ChQvz6668YGRkB4ODgQOPGjdm9ezcdO3akaNGiDBw4kPnz51O3bl0mT55MzZo16dChAxDTZGtsbEy2bNniNNt92YQLkCVLFurUqcPIkSPVS+4sW7aMzJkz4+HhoW6SrFy5MnXr1mXt2rWMHj2aixcvYmtrS8eOHTE0NMTJyQlTU1PevXsX5xz58uVj2LBhTJ8+nZ07d/LTTz/FSePr68uOHTsYNmyY+j11cXHBwMCAX3/9lQ4dOpA/f3696zaQwKQHVDWCtN6spU2pUqXUHdavXr1i0aJFfP78mUWLFlG4cOF408b2LWsHyZEpUyZq166tEZj+/PNPGjZsmOxrGhAQoPWH28HBgalTp36zgQ+geV2USiXv37/n1KlTLFy4kPDwcIYOHapOu2vXLq3HyJ49e5xtqrQfPnzAw8ODCxcuMH78eOrUqZNief/w4QM3btygZ8+e6pofxPyoFylShDNnztCxY0cAevbsydGjRxk0aBBWVlbMnDkzSeeYMmUKpUqVIjo6mmPHjrF27Vo6d+4cZ1Tc+fPncXJyIlOmTOp8mJubU6FCBc6ePQtApUqV2L59Oy1btqRu3brUqFGDpk2bxvs56tSpE4cPH2bWrFlUq1YtzgCc8+fPo1QqqV27tsaNTu3atVm5ciVXrlyhbt26SSrntySBSQ+8fPmSTJkykTVr1u+dlRRnZmamMczbwcEBNzc3evTogaenJ9myZYs3bVrSsGFDBgwYwIsXLzAxMeHcuXMMGTIk2cfLkSMHK1euVP9tbGxMrly5yJIlSwKvSh3arouLiwvh4eGsXbtWY8i0LtcvdtoKFSrQrVs3Bg8ejIeHBxUqVPj6jAPv378nOjqaNWvWaB0KbWLy34q1RkZGuLm5cePGDcqUKaM1mGpTqFAhdVkcHBzImDEjy5Ytw8TERKPmFxwcjJeXF15eXnGOofoeNGrUiOjoaLZs2cKKFStYunQpefPmZcSIETRq1CjO61RNem5ubowfP561a9dq7A8ODgaIt8Xh5cuXSSrjtyaB6TuLjIzkwoULlCtXTt3M8COztrZm4sSJDB48mBkzZjB//vzvnaUUUb16dczMzDh06BCmpqbY2tpib2+f7OMZGxvrfZC2t7dn586dSRoIkBhDQ0Pc3d1p3LgxY8aM4c8//9QIGsllZmaGgYEB3bp10/rjHHvVgdevX7N06VJKlCjBiRMnOHToEK6urjqfs2/fvhw9epQlS5ZQs2ZNFAoFEPP8V5UqVejevXuc18QeINOkSROaNGlCSEgI3t7erFmzhpEjR1K+fHlsbGzivDZ//vwMHTqUmTNnxqmxqgZg/Pbbb5iZxV3ZVttzUPpABj98Z9u3b+f169e0b9/+e2flm3F1daVatWocOHAg0Ycn0wpjY2Pq1q3L4cOHOXjwoF73iaWUmzdvYmRkRL58+VLkeHnz5qVfv374+fml2IOe5ubmlCxZkkePHlG6dGn1v2LFirF06VIuXLigTjtx4kSMjIzw8PCgTp06TJkyRWOUnqqvKDEZMmRg8uTJREZGMn36dPV2JycnfHx8KFGihDof9vb2eHh4cOTIEQCGDBlC//79gZhA1rBhQ/r160dkZCSvXr2K95ydO3emfPnycR7cVtU8g4KCNMofGBjI4sWL1TWqpJbtW5Ea0zcSGhrK9evXgZhRPEFBQXh7e7N9+3bc3NyoX7/+983gNzZu3Djc3NyYPn16orNdpBWNGjWid+/eGBoaJjhSysfHBw8Pjzjby5Urp5cLWsb+7AJERERw/Phxdu/eTdu2bTWaY2On+1KhQoUSbYrs1q0bu3btYs2aNbRo0YK8efN+bfbVHf/Dhw/Hzc2NqKgo1q9fz40bN+jXrx8Ae/fu5fjx48yfP5+sWbMyceJEGjVqpB7RBjG1jzt37nDx4sVEr1PZsmVxc3Nj3759HDx4UB1g2rVrR+/evWnfvj0mJiZs375dXbuCmD6mSZMmMXv2bKpXr8779+9ZtmwZBQsWpHjx4vGeT1XjdHNz09huZ2eHm5sbEyZMICAgAHt7e3x9fVm4cCG2trYULFhQXbY3b95w8uRJSpQooTHq8HuQwPSN3Llzh7Zt2wIx7cJmZmYoFAomT56sdTTNj65w4cJ07tyZ9evXs3Xr1u+dnRRRpUoVLC0tyZ07d4IzVdy6dYtbt27F2T548GC9DEyxP7sQ0y+jaj7q2bOnRtrY6b60fPnyRDvajY2NGTduHL1792b27NnqH+yv4eLiwrp161i2bBmDBg0iY8aMlCpVig0bNuDo6MjLly+ZMWMGNWrUUA/vz5UrF0OHDmX69OkcOHCAJk2a0KNHD2bOnEnPnj3ZsGFDoucdMWIER48eZc6cOdSsWZPixYuzefNmFi5cyKhRo1AqlSgUCpYvX64e8NGuXTs+f/7Mtm3b2LJlC5kyZaJy5cqMHDmSjBkzJni+AgUKMHToUNzd3TW2u7u78+uvv7Jt2zZevHhB9uzZadSoEUOGDFF3H7Rs2ZKTJ0/Sv39/Bg0alKKjIpND1mMSQgihV/SrYVEIIUS6J4FJCCGEXpHAJIQQQq9IYBJCCKFXJDAJIYTQKxKYhBBC6BUJTEIIIfSKBCYhhBB6RQKT0Bu3bt1i5MiR1KxZkzJlylC3bl0mTJgQZ4VPOzs7li5d+k3ztnTpUuzs7NR/h4aG0qdPHxwcHKhYsSKPHz9O1dU/O3fujJ2dHe3atYs3zdChQ7Gzs2PMmDFffb4LFy5gZ2enMZdcarxGCG1kSiKhFzZv3szMmTNxdnZm+PDh5MyZkydPnrBu3Tr++usvfvvttwTnCkttP/30E9WqVVP/vXfvXk6cOMHEiRMpVqwYefLkYfv27am6dpShoaF6pdwv190JDw/nxIkTqXZuIb4lqTGJ7+7KlSvMmDGDDh06sH79epo2bYqzszNt2rRh69atmJiYMG7cuO+ax1y5cmms7qmalblDhw44OTmpV/+MPaFpSitZsiQmJiYcOnQozr4TJ06QOXNmrcsiCJHWSGAS3926deuwsLBg2LBhcfZly5aNMWPGUKdOHcLDw7W+/u7duwwYMIBKlSpRqlQpqlWrxvTp0/n48aM6zZkzZ2jTpg1ly5alYsWK9O3bl4cPH6r3P336lD59+uDs7IyDgwNt27bl5MmT6v2xm/I6d+6sbkosXrw4Y8aMwd/fP05T3rNnzxg2bBhOTk44ODjQtWtX7ty5o96ves2GDRtwdXXFwcGB3bt3x/s+mZqaUqNGDa2BycvLiwYNGmis6wPw6dMnli9fjqurK6VLl6Z+/fqsXr2a6OhojXTbtm2jQYMGlClThk6dOvHs2bM450isPF/6+PEjkydPpnr16tjb2+Pq6sq6deviTS+EigQm8V0plUq8vb2pXLmyxqJtsTVq1Ij+/ftjamoaZ9+rV6/o2LEjHz58YNasWaxZs4bGjRuzadMmNm7cCICfnx/9+vXD3t6elStXMmPGDHx9fenVqxfR0dFER0fTu3dvPnz4wJw5c1ixYgVZs2alb9++PHnyJM45J02aROvWrYGY9bRUSyfEFhgYSLt27fjnn3+YMGEC8+fPJzo6mo4dO2oERIgJej///DNz5syhatWqCb5fjRo1UjfnqYSGhnLq1Cn1zNix39s+ffqwdu1afvrpJ1atWoWrqyuLFi1i0qRJ6nS///47kyZNokaNGqxYsQIHBwcmTJiQ7PKozJw5k1OnTjF69GjWrVtHnTp1mDNnToLBVwiQPibxnQUFBfHp0ydsbW2T9fr79+9TokQJFi9ejLm5ORCz/MSZM2e4cOECvXr14ubNm3z8+JHevXurm7py5crFsWPHCA8P58OHDzx69Ih+/fpRo0YNAMqUKcOyZcuIiIiIc86iRYuq+3hUzXtfruL622+/ERwczNatW9VrClWvXp1GjRqxePFijeUcGjZsSKtWrZJU3po1a5I5c2YOHTpEt27dADhy5AjZs2enfPnyGmlPnTrF2bNnWbBggXrhwqpVq5IpUyYWL15Mly5dKFq0KCtWrKBRo0bq5lIXFxdCQ0PZtm1bssqjcvHiRapWrao+t7OzM6ampkleslykXxKYxHelWg8mKioqWa93cXHBxcWFz58/4+Pjw5MnT7h//z6BgYFkzZoVAAcHB0xMTGjdujWurq5Ur14dZ2dn9dpHZmZmFC1alAkTJuDt7Y2LiwvVq1dn7NixyS7XuXPnKFGiBDY2NkRGRgIxgxeqV6/O/v37NdKWKFEiycfNlCkTtWvX1ghMf/75Jw0bNsTAwEAj7cWLF8mQIUOc5cHd3NxYvHgxFy9exNDQkLdv31KrVi2NNA0bNtQITLqUR8XZ2Vm9BlCNGjWoUaOGenVWIRIigUl8V1myZMHMzExrn4ZKeHg4nz9/1rr6aXR0NAsWLGDz5s2Eh4eTO3duypQpg4mJiTqNra0tv//+O6tXr2bXrl1s3LgRS0tLOnTowJAhQzAwMGD9+vWsXLmSI0eOsHfvXjJmzEjdunWZMmVKoquuahMcHMyTJ08oVaqU1v0fPnxQ/7+2JsqENGzYkAEDBvDixQtMTEw4d+4cQ4YMiZPu3bt3WFlZqYO/So4cOQAICQnh3bt3AFhZWWlNk5zyqPzyyy/kypWL/fv3M23aNKZNm0bZsmWZPHnydx1hKfSfBCbx3bm4uHDhwgU+ffqkEVBUduzYwezZs9m1a1ecH8bVq1fj4eHBlClTqF+/PhYWFgDqPiCV2E1zV65cYfv27axatYrixYvTsGFDbGxsmDx5MpMmTeLu3bscOnSINWvWYGVlpdEfk1QWFhY4OTkxatQorfuNjY11PqZK9erVMTMz49ChQ5iammJra4u9vX2cdFmyZCEoKIioqCiN4PTq1SsgJhipAtLbt281Xqsadfg15TE2NqZv37707duXZ8+eceLECVasWMHw4cP5888/dSqzSF9k8IP47nr06EFwcDCLFi2Ks+/169esX7+eokWLar1bv3LlCkWLFqVVq1bqoPTy5Uvu37+vHnnm4eFBrVq1iIiIwNjYmMqVKzNt2jQgZqTZtWvXqFKlCjdv3sTAwIASJUowdOhQFApFgjW5hDg5OeHr60uhQoUoXbq0+t++ffvYtWtXnFqMLoyNjalbty6HDx/m4MGD6j4cbXmIjIyMM4pP1fRWvnx5ChYsSO7cueOk+fKZKF3L8/HjRxo0aMD69esByJMnDx07dqRx48bJfk9F+iE1JvHdOTo6MnjwYBYtWsTDhw9p3rw5VlZWPHjwgHXr1vHp0yetQQtiakIrVqxg9erVODo68uTJE3799VciIiLUzUuVKlVi3rx59O/fn06dOmFkZMS2bdswNjamVq1a5M2bl0yZMjFq1CgGDhyItbU1Z8+e5d9//6VLly7JKlO3bt3Yt28f3bp1o0ePHlhZWeHl5cWOHTu+qu9KpVGjRvTu3RtDQ0PGjx+vNY2qL238+PG8fPmS4sWLc/HiRdasWUOLFi0oWrQoACNGjGD48OGMHz8eV1dXrl+/ztatW7+qPJkyZaJUqVIsW7aMjBkzYmdnh6+vL3v27KFBgwZfXX7xY5PAJPRC3759KVmypHoGiHfv3pE7d25q1qxJnz59yJ07t9bX9e7dm6CgIDZu3Mjy5cvJnTs3zZo1w8DAgF9//ZX3799TvHhxVq1axfLlyxk2bBhRUVHY29uzfv16ChcuDMD69euZP38+M2bM4P379xQsWJCpU6fSsmXLZJXHxsaGbdu2MX/+fCZPnsynT58oWLAgM2bMiNPMmBxVqlTB0tKS3LlzU6RIEa1pVO/BkiVL8PDwIDAwEFtbW4YNG0b37t3V6Zo0aYKhoSErVqxg3759KBQKpk6dqvFcWXLKM3XqVBYtWsT69et5/fo12bNnp3Xr1gwePPiryy9+bAZKpVL5vTMhhBBCqEgfkxBCCL0igUkIIYRekcAkhBBCr0hgEkIIoVckMAkhhNArEpiEEELoFQlMQggh9IoEJiGEEHpFApMQQgi9IoFJCCGEXpHAJIQQQq/8H7n9txOLuTvxAAAAAElFTkSuQmCC","text/plain":["<Figure size 400x400 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["def plot_acc():\n","    plt.rcParams['figure.figsize']=4,4\n","    sns.set_style('darkgrid')\n","    ax = sns.barplot(x=name_eval_list[\"name\"], y=name_eval_list[\"acc\"], palette=\"coolwarm\", saturation=2.0)\n","    plt.xlabel('Classifier Models', fontsize=12)\n","    plt.ylabel('% of Accuracy', fontsize=12)\n","    plt.title('Accuracy of different Classifier Models', fontsize=16)\n","    plt.xticks(fontsize=12, horizontalalignment='center')\n","    plt.yticks(fontsize=12)\n","    for i in ax.patches:\n","        width, height = i.get_width(), i.get_height()\n","        x, y = i.get_xy() \n","        ax.annotate(f'{round(height,2)}%', (x + width/2, y + height), ha='center', fontsize='x-large')\n","    plt.show()\n","    \n","plot_acc()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
